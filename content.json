{"meta":{"title":"logerJava","subtitle":"","description":"�����������ƾ�����","author":"loger","url":"http://logerJava.github.io","root":"/"},"pages":[{"title":"关于","date":"2022-11-09T06:13:45.000Z","updated":"2022-11-09T06:23:30.896Z","comments":false,"path":"about/index.html","permalink":"http://logerjava.github.io/about/index.html","excerpt":"","text":"互联网砖工"},{"title":"categories","date":"2018-09-30T09:25:30.000Z","updated":"2022-11-04T08:29:46.112Z","comments":true,"path":"categories/index.html","permalink":"http://logerjava.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-09-30T10:23:38.000Z","updated":"2022-11-04T08:31:26.145Z","comments":true,"path":"tags/index.html","permalink":"http://logerjava.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"一次导入引起的 MySQL 索引失效","slug":"一次导入引起的-MySQL-索引失效","date":"2023-02-17T09:35:40.000Z","updated":"2023-02-17T09:55:02.293Z","comments":true,"path":"2023/02/17/一次导入引起的-MySQL-索引失效/","link":"","permalink":"http://logerjava.github.io/2023/02/17/%E4%B8%80%E6%AC%A1%E5%AF%BC%E5%85%A5%E5%BC%95%E8%B5%B7%E7%9A%84-MySQL-%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88/","excerpt":"","text":"起因本次索引失效的起因场景如下： 在一张 300w 数据的表中存在一个二级索引，例如 age 字段，此时进行 SQL 操作可以走索引, 如下： 1SELECT XXX FROM table WHERE age &gt; 20 需求为向此表中导入 80w+ 的数据，导入后索引失效 分析根据上面的问题我分析导致索引失效的原因有如下几种可能： 索引的选择性下降：当索引的选择性下降时，查询优化器可能会选择全表扫描而不是使用索引来查找记录，因为全表扫描可能比使用索引更快。选择性是指索引中不同值的数量与表中记录总数的比率。如果一个索引的选择性很低，那么它将无法提供足够的性能优势 索引列上的数据类型转换：如果查询中的条件列与索引列上的数据类型不匹配，MySQL 将进行数据类型转换。这种转换可能会导致查询无法使用索引而选择全表扫描 内存不足：当表的大小增长时，需要更多的内存来维护索引。如果系统的内存不足，MySQL 将不得不从磁盘读取索引数据，这将降低查询的性能 索引损坏：索引损坏可能会导致查询无法使用索引而选择全表扫描。可以使用 MySQL 的 CHECK TABLE 命令来检查表和索引是否损坏 首先索引选择性下降，因为 age 20 只是举例说明，实际上字段的区分度还是很高的，所以这种可能 pass 掉 我司关于 MySQL 的使用还是很严格的，所以数据类型转换的问题也不存在 内存相关问题如果存在则会事先发出告警，然而并没有 后续进行了索引检查，发现并没有损坏 解决在多方寻求答案无果后，我终于找到了我们的 DBA，问题也得到了解决 SQL 是否走索引由优化器决定 ，而优化器是根据统计信息来判断的 首先我们来学习一个新概念，可能与后端无关，属于 DBA 范畴，名为统计信息 MySQL 会使用统计信息来优化查询，包括确定如何使用索引。统计信息通常包括表中不同值的数量，以及每个值出现的频率等信息。如果这些信息过时或不准确，MySQL 可能会做出错误的决策，选择全表扫描而不是使用索引 MySQL 会定期自动更新统计信息，也可以使用 ANALYZE TABLE 命令手动更新统计信息。如果在导入80万以上数据时，统计信息已经过时或不准确，可能会导致索引失效。此时，可以尝试手动更新统计信息来解决问题 另外，也有可能是因为在导入大量数据时，MySQL 无法及时更新统计信息，导致索引失效。这种情况下，可以尝试暂停数据导入，手动更新统计信息，然后再恢复数据导入","categories":[],"tags":[{"name":"工作中遇到问题的总结","slug":"工作中遇到问题的总结","permalink":"http://logerjava.github.io/tags/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/"}]},{"title":"Redis 持久化","slug":"Redis 持久化","date":"2023-02-14T09:00:59.000Z","updated":"2023-02-17T05:45:00.068Z","comments":true,"path":"2023/02/14/Redis 持久化/","link":"","permalink":"http://logerjava.github.io/2023/02/14/Redis%20%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"AOF 日志简介在 Redis 每执行一条写操作命令，就将该命令以追加的方式写入倒一个文件内，重启 Redis 时先读取文件内的命令，这种保存写操作命令到日志的持久化方式就是 Redis 中的 AOF（Append Only File） AOF 仅会记录写操作，不会记录读操作，记录都操作无实际意义 在 Redis 中 AOF 持久化功能不是默认开启的，需要修改 redis.conf 配置文件中的参数： 12appendonly yes // 表示是否开启 AOF 持久化appendfilename &quot;appendonly.aof&quot; // AOF 持久化文件名称 AOF 记录日志AOF 日志文件本质为普通的文本文件，在服务器中可以通过 cat 命令查看内容，其内容存在一定规则 下面以 set name loger 命令进行举例，如下： 在 AOF 日志中，*3 表示当前命令分为三个部分，每部分都以 $ + 数字 开始，后面是具体的命令、键、值，数字部分表示命令、键、值一共有多少字节，例如 $3 set 表示这部分有 3 个字节，也就是 set 命令这个字符串的长度 可以看到 Redis 先执行写操作命令后再执行 AOF 记录命令，其好处是： 避免额外检查开销，如果先将写操作命令记录到 AOF 日志再执行该命令的话，若命令语法出现问题在不进行命令语法检查情况下，错误的命令便会记录到 AOF 日志，Redis 在恢复数据时会出现错误 不会阻塞当前写操作命令的执行，因为当写操作命令执行成功后才会记录到 AOF 日志 当然也存在潜在风险： 执行写操作命令和记录日志是两个过程，当 Redis 还没来得及将命令写入硬盘时，服务器发生宕机数据会存在丢失风险 由于是在写操作成功后才记录到 AOF 日志，所以不会阻塞当前写操作命令，但是可能会对下一个命令带来阻塞风险，因为将命令写入到日志的操作是在主进程完成的，也就是说这两个操作是同步的，如果将日志写入到硬盘时，服务器的硬盘 I&#x2F;O 压力很大，就会导致写硬盘的速度很慢，进而阻塞使后续命令无法执行 分析一下，其实这两个风险都有一个共性，其都和 “AOF 日志写回硬盘的时机” 有关 三种写回策略Redis 写入 AOF 流程： 流程分析： Redis 执行写操作命令后，会将命令追加到 server.aof_buf 缓冲区 然后通过 write() 系统调用，将 aof_buf 缓冲区的数据写入到 AOF 文件，此时数据并没有写入到硬盘，而是拷贝到了内核缓冲区的 page cache，等待内核将数据写入硬盘 内核缓冲区的数据什么时候写入硬盘，由内核决定 Redis 提供了 3 种写回硬盘的策略，控制内核缓冲区写入硬盘的过程，在 redis.conf 配置文件中 appendfsync 配置项有以下 3 种参数填写： Always，表示每次写操作命令执行完成后，同步将 AOF 日志数据写回硬盘 Everysec，表示每次写操作命令执行完成后，先将命令写入 AOF 文件内核缓冲区，每隔一秒将缓冲区的内容写回硬盘 No，表示不由 Redis 控制写回硬盘的时机，转交给操作系统控制写回的时机，每次写操作命令执行完成后，先将命令写入到 AOF 文件内核缓冲区，再由操作系统决定何时将缓冲区内容写回硬盘 这三种写回策略都无法完美解决 “主进程阻塞” 和 “减少数据丢失” 的问题，因为两个问题是对立的，偏向于一边就要牺牲另一边，其原因如下： Always 策略可以最大成都保证数据不丢失，但是由于每执行一条写操作命令就同步将 AOF 内容写回硬盘，不可避免的会影响主进程性能 No 策略交由操作系统决定何时将 AOF 写回硬盘，相较于 Always 策略性能更好，但操作系统写回硬盘的时机不可预知，如果 AOF 日志没有写回硬盘时发生宕机，就会丢失不定数量的数据 Everysec 策略是一种折中策略，避免了 Always 策略的性能开销，相较于 No 策略更能避免数据丢失，但这不是绝对的，如果上一秒写操作日志的命令没有写回硬盘并发生宕机，这一秒内的数据依旧会丢失 其实际使用需根据业务场景自行选择 而在源码内，这三种策略的意图就是控制 fsync() 函数的调用时机，fsync() 的作用是将内核缓冲区的数据写入硬盘，直到硬盘写操作完成后，函数才会返回 当应用程序向文件写入数据时，内核通常先将数据复制到内核缓冲区，然后排入队列由内核决定何时写入硬盘 若想要应用程序向文件写入数据后，可以立即将数据同步到硬盘，就可以调用 fsync() 函数 Always 策略就是每次写入 AOF 文件数据后，就执行 fsync() 函数 Everysec 策略就会创建一个异步任务来执行 fsync() 函数 No 策略就是永不执行 fsync() 函数 AOF 重写机制AOF 日志作为一个文件，随着执行的写操作命令越多，文件大小也会越来越大 当 AOF 日志文件过大就会带来性能问题，例如重启 Redis 后读取 AOF 内容恢复数据，若文件过大恢复过程就会很慢 Redis 为了避免 AOF 文件越来越大提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件 AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到 “新 AOF 文件” 中，等到全部记录完成，将 “新 AOF 文件” 替换现有的 AOF 文件 例如，在没有使用重写机制前，前后执行 set name loger 和 set name logerjava 两个命令，这两条命令会记录到 AOF 文件中 在使用重写机制后，会读取 name 最新的 value，然后用一条 set name logerjava 命令记录到新的 AOF 文件，也就是将最新的内存数据记录到 AOF 中，历史记录无任何意义 重写工作完成后，就会将新的 AOF 文件覆盖现有的 AOF 文件，相当于压缩了 AOF 文件，减命令数量进而减小文件体积 而重写 AOF 文件不复用现有 AOF 的原因在于，如果采用复用方法 AOF 重写过程失败了，就会对现有 AOF 文件造成污染，可能使其无法用于恢复使用，所以 AOF 重写过程需要先写到新的 AOF 文件中，重写失败直接删除新的文件，不会对现有 AOF 造成污染 AOF 后台重写写入 AOF 日志是在主进程完成的，但因其写入内容不多，所以一般情况下不会太影响命令操作 但是 AOF 在重写时，需要读取所有缓存键值对数据并为每个键值对生成一条命令，再重写到新的 AOF 文件中，重写完成后还需要替换现有 AOF，这个过程十分耗时，所以重写操作不能放在主进程中 重写 AOF 过程是由后台子进程 bgrewriteaof 完成的，其好处在于： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，避免阻塞主进程 子进程带有主进程一样的 “数据副本” 使用子进程而非线程的原因如果使用线程，多线程之间会共享内存，那么在修改共享内存数据时需要通过加锁来保证数据安全，而这样就会降低性能 使用子进程在创建进程时，父子进程是共享内存数据的，不过共享的内存只能以只读方式共享，而当父子进程任意一方修改了共享内存，就会发生 Copy On Write（写时复制），于是父子进程就有了独立的数据副本，不用加锁来保证数据安全 子进程的数据副本主进程在通过 fork 系统调用生成 bgrewriteaof 子进程时，操作系统会把主进程的页表复制一份给子进程，页表中记录着虚拟地址和物理地址的映射关系，而不会复制物理内存，两者虚拟空间不同但对应的物理空间相同 如此子进程就共享了父进程的物理内存数据，节约物理内存资源，页表对应的页表项属性会标记该物理内存权限为只读 当父进程或子进程在向内存发起写操作时，由于违反权限会导致 CPU 会触发写保护中断，操作系统会在写保护中断处理函数中进行物理内存复制并重新设置内存映射关系，将父子进程的内存读写权限设置为可读写，最后对内存进行写操作，这个过程被称为 Copy On Write（写时复制） 写时复制，在发生写操作的时候，操作系统才会去复制物理内存，这样是为了防止 fork 创建子进程时由于物理内存数据复制时间过长而导致父进程长时间阻塞的问题 AOF 重写缓冲区思考一个问题，重写 AOF 日志过程中，如果主进程修改了已经存在的 key-value，此时这个 key-value 数据在子进程的内存数据就与主进程的内存数据不一致了，该如何处理 ？ 为了解决上述数据不一致问题，Redis 设置了 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用 在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 “AOF 缓冲区” 和 “AOF 重写缓冲区” 也就是说，在 bgrewriteaof 子进程执行 AOF 重写期间，主进程需要执行以下三个工作： 执行客户端发来的命令 将执行后的写命令追加到 “AOF 缓冲区” 将执行后的写命令追加到 “AOF 重写缓冲区” 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一将内存数据键值对转换为一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，信号是进程间通讯的一种方式，且是异步的 主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作： 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 文件中，使新旧两个 AOF 文件所保存的数据库状态一致 新 AOF 文件重命名，覆盖现有 AOF 文件 信号处理函数执行时会对主进程造成阻塞，信号函数执行完后，主进程就可以继续像往常一样处理命令了， 会导致父进程阻塞的原因当操作系统复制父进程页表的时候，父进程是阻塞的，页表的大小相比实际物理内存小很多所以通常复制页表的过程是比较快的，而当父进程的内存数据非常大时，页表也会很大，这时父进程在通过 fork 创建子进程时阻塞时间也会越久 所以，会有两个阶段导致阻塞父进程： 创建子进程途中，由于要复制父进程的页表等数据结构，阻塞的时间跟页表的大小有关，页表越大阻塞时间越长 创建完子进程后，如果子进程或者父进程修改了共享数据就会发生写时复制，此期间会拷贝物理内存，如果内存越大自然阻塞时间也越长 触发重写机制后，主进程会创建重写 AOF 的子进程（bgrewriteaof），此时父子进程共享物理内存，重写子进程只会对这个内存进行只读，重写 AOF 子进程会读取数据库里的所有数据，并逐一将内存数据的键值对转换成一条命令，再将命令记录到重写日志（新的 AOF），但子进程重写过程中，主进程依然可以正常处理命令 如果此时主进程修改了已经存在的 key-value，就会发生写时复制，这里只会复制主进程修改的物理内存数据，没修改物理内存还是与子进程共享的，当此阶段修改的是 bigkey 时，这时复制的物理内存数据的过程就会比较耗时，有阻塞主进程的风险 所有导致父进程阻塞的原因有以下两种： 发生写时复制时造成的阻塞 信号处理函数执行时造成的阻塞 其他情况下 AOF 后台重写不会阻塞主进程 RDB 快照RDB 快照实际上是记录某一瞬间的内存数据，为实际数据，而 AOF 文件记录的是命令操作日志，而不是实际数据 因此 Redis 恢复数据时，RDB 恢复数据的效率比 AOF 高，直接将 RDB 文件读入内存就可以不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据 快照怎么用Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，其区别在于是否在主线程执行： 执行 save 命令，会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件时间太长会阻塞主线程 执行 bgsave 命令，会创建一个子进程来生成 RDB 文件，可以避免主线程阻塞 RDB 文件的加载工作是在服务器启动时自动执行的，Redis 并没有提供专门用于加载 RDB 文件的命令 Redis 还可以通过配置文件的选项来实现每隔一段时间自动执行一次 bgsave 命令，默认会提供以下配置： 123save 900 1save 300 10save 60 10000 虽然选项名时 save，实际上执行的却是 bgsave 命令，也就是会创建子进程来生成 RDB 快照文件 只要满足上述条件中任意一个就会执行 bgsave，其分别代表： 900 秒之内对数据库进行了至少 1 次修改 300 秒之内对数据库进行了至少 10 次修改 60 秒之内对数据库进行了至少 10000 次修改 需要注意，Redis 的快照为全量快照，每次执行快照都会把内存中所有数据记录到磁盘中 可以得出执行快照是比较重量的操作，如果频率过于频繁会对 Redis 的性能造成影响，如果频率太低则宕机时丢失的数据会变多 这也是 RDB 的缺点，在服务器发生故障时丢失的数据会比 AOF 更多 执行快照时，数据可以被修改吗？执行 bgsave 过程中，Redis 依然可以继续处理操作命令，数据时可以被修改的，同样依赖于 Copy On Write（写时复制） 执行 bgsave 命令时，通过 fork() 创建子进程，此时子进程和父进程共享同一片内存数据，创建子进程时复制父进程的页表，所以页表指向的物理内存是同一个，只有在发生修改内存数据的情况时，物理内存才会被复制一份，这样的目的是为了减少创建子进程时的性能损耗，从而加快创建子进程的速度（创建子进程的过程会阻塞主进程） 创建 bgsave 子进程后，由于共享父进程的所有内存数据，就可以直接读取主进程（父进程）里的内存数据并写入到 RDB 文件中 当父进程对这些共享内存数据做只读操作时，主线程与 bgsave 子线程是互不影响的，但当父进程要修改共享数据中的某一块数据，就会发生写时复制，于是这块数据的物理内存会被复制一份，父进程子这个数据副本进行修改操作，与此同时 bgsave 子进程就可以继续将原来的数据写入到 RDB 文件 这其中有一个问题，当 bgsave 快照过程中，主线程修改了共享数据，发生写时复制后 RDB 快照保存的是原本的内存数据，而主线程刚修改的数据是没办法这一时间写入 RDB 文件的，只能交由下一次 bgsave 快照 所以 Redis 在使用 bgsave 快照过程中，如果主线程修改了内存数据，不管是否是共享内存数据，RDB 快照都无法写入主线程刚修改的数据，因为此时主线程和子进程的内存数据已经分离，子进程写入到 RDB 文件的内存数据只能是原本的内存数据 如果系统恰好在 RDB 快照创建完毕后宕机，那么 Redis 将丢失主线程快照期间修改的数据 极端情况下，在 Redis 执行 RDB 期间，刚刚 fork，主进程和子进程共享同一物理内存，但是途中主进程处理了写操作，修改了共享内存，于是当前被修改的数据的物理内存就会被复制一份，如果所有共享内存都被修改，则此时的内存占用是原先的 2 倍 所以针对写操作多的场景，需要主义快照过程中的内存变化，防止内存被占满 混合持久化尽管 RDB 相较于 AOF 数据恢复速度更快，但是快照的频率并不好把握： 如果频率太低，两次快照期间一旦服务器发生宕机，就可能会比较多的数据丢失 如果频率太高，频繁写入磁盘和创建子进程会带来额外的性能开销 混合持久化简介在 Redis 4.0 提出了一种将 RDB 和 AOF 合体使用的方法，也就是混合持久化 开启混合持久化需要在 Redis 配置文件中更改为 yes ： 1aof-use-rdb-preamble yes 混合持久化工作在 AOF 日志重写过程 当开启了混合持久化时，在 AOF 重写日志时，fork 出的重写子进程会先将与主线程共享的内存数据以 RDB 方式写入到 AOF 文件，主线程处理的操作命令会被记录在重写缓冲区中，重写缓冲区中的增量命令会以 AOF 方式写入到 AOF 文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的 AOF 文件 使用混合持久化就相当于，AOF 文件的前半部分时 RDB 格式的全量数据，后半部分时 AOF 格式的增量数据 此种方式的好处在于，重启 Redis 加载数据时，由于前半部分是 RDB 内容加载速度很快，后半部分为 AOF 内容，此部分内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使数据丢失的更少","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://logerjava.github.io/tags/Redis/"}]},{"title":"Redis 数据类型和应用场景","slug":"Redis 数据类型和应用场景","date":"2023-02-08T02:50:37.000Z","updated":"2023-02-14T09:01:48.568Z","comments":true,"path":"2023/02/08/Redis 数据类型和应用场景/","link":"","permalink":"http://logerjava.github.io/2023/02/08/Redis%20%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"概述Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射 键的类型只能为字符串，值有五种常见的数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合） 随着 Redis 版本的更新，后面又支持了四种数据类型：BitMap（2.2 版新增）、HyperLogLog（2.8 版新增）、GEO（3.2 版新增）、Stream（5.0 版新增） Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能 ps：Redis 命令菜单 数据类型与应用场景String简介String 是最基本的 key-value 结构，key 是唯一标识，value 是具体的值，value其实不仅是字符串， 也可以是数字（整数或浮点数），value 最多可以容纳的数据长度是 512M 内部实现String 类型的底层数据结构主要是 int 和 SDS （简单动态字符串） SDS 与 C 字符串不太一样，而 Redis 没有采用 C 语言的字符串表示原因如下： SDS 不仅可以保存文本数据，还可以保存二进制数据，因为 SDS 使用 len 属性的值而不是空字符串来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不仅仅能存放文本数据，还可以保存图片、音频、视频、压缩文件等二进制数据 **SDS 获取字符串长度的时间复杂度是 O(1)**，因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O（n）；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1) Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出，因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出问题 ps：Redis的设计与实现 - 简单动态字符串 应用场景缓存对象使用 String 来缓存对象有两种方式： 直接缓存整个对象的 JSON，例如：SET user:1 &#39;&#123;&quot;name&quot;:&quot;loger&quot;, &quot;age&quot;:24&#125;&#39; 采用将 key 进行分离为 user:ID:属性，采用 MSET 存储，MGET 获取各个属性值，例如：MSET user:1:name loger user:1:age 24 user:2:name xiaoming user:2:age 20 常规计数因为 Redis 处理命令是单线程，所以执行命令的过程是原子的，因此 String 数据类型适合计数场景，比如计算访问次数、点赞、转发、库存数量等 123456789101112131415# 初始化文章的阅读量&gt; SET aritcle:readcount:1001 0OK#阅读量+1&gt; INCR aritcle:readcount:1001(integer) 1#阅读量+1&gt; INCR aritcle:readcount:1001(integer) 2#阅读量+1&gt; INCR aritcle:readcount:1001(integer) 3# 获取对应文章的阅读量&gt; GET aritcle:readcount:1001&quot;3&quot; 分布式锁SET 命令 NX 参数可以实现 key 不存在才插入，再加上过期时间，可以实现分布式锁： 若 key 不存在，则显示插入成功，表示加锁成功 若 key 存在，则显示插入失败，表示加锁失败 例如： 1SET lock_key unique_value NX PX 10000 lock_key 就是 key 键 unique_value 是客户端生成的唯一标识 NX 代表只有在 lock_key 不存在时，才对 lock_key 进行设置操作 PX 10000 表示设置 lock_key 的过期时间为 10s，避免客户端发生异常无法释放锁 解锁时将 lock_key 删除，但需要保证执行操作的客户端为加锁客户端，所以需要先判断 unique_value 是否为加锁客户端，然后删除 lock_key 此时解锁存在两个操作（GET 和 DEL），为了保证原子性需要采用 Lua 脚本方式，因为 Redis 执行命令是单线程的，在执行 Lua 脚本时其它命令都会等待直到上一个命令处理完成，也就可以保证释放锁操作的原子性 123456// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end ps：Redis - 分布式锁 共享 Session 信息在开发后台管理系统时，会使用 Session 保存用户的会话（登录）状态，这些 Session 信息会被保存在服务器端，但这只适用于单系统应用，分布式系统下将不在适用 例如用户 1 的 Session 信息被存储在服务器 A，但第二次访问时用户 1 被分配到服务器 B，这个时候服务器 B 并没有用户 1 的 Session 信息，就会出现重复登陆的情况，其问题在于分布式系统每次会将请求随机分配到不同服务器 分布式系统单独存储 Session 流程图： 当我们使用 Redis 对 Session 进行统一存储和管理时，无论请求发送到哪台服务器，服务器都会去同一个 Redis 获取相关的 Session 信息，这样就解决了分布式系统下 Session 存储的问题 分布式系统使用同一个 Redis 存储 Session 流程图： List简介List 列表时简单的字符串列表，按照插入顺序排序，可以从头部或尾部向 List 列表添加元素 列表最大长度为 2^32 -1，每个列表支持超过 40 亿个元素 内部实现List 类型的底层数据结构是由双向链表或压缩列表实现的： 如果列表的元素个数小于 512 个（默认值，可通过 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可通过 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构 如果列表的元素不满足上面的条件，Redis 会使用双向链表作为 List 类型的底层数据结构 在 Redis 3.2 版本之后，List 数据类型底层数据结构只由 quicklist 实现，替代了双向链表和压缩列表 应用场景消息队列作为消息队列，要满足三个需求：消息的可靠性、消息的顺序性以及重复消息的处理 List 可以使用 LPUSH + RPOP 命令实现消息队列 生产者使用 LPUSH key value[value...] 将消息插入到队列的头部，如果 key 不存在则会创建一个空的队列再插入消息 消费者使用 PROP key 一次读取队列的消息，先进先出 1. 消息的可靠性当消费者程序从 List 中读取一条信息，List 就不会再保存这条信息了。因此，若消费者程序再处理消息的过程中出现了宕机等问题，消费者程序便无法再次从 List 中读取消息 List 类型 提供了 BRPOPLPUSH 命令，让消费者程序从另一个 List 中读取消息，同时 Redis 会把这个消息再插入到另一个 List（可以叫做备份 List）保存 这样当消费者程序进行消费却没能正常处理，等待重启后便可以从备份 List 中重新读取消息进行处理 2. 消息的顺序性List 自身按照先进先出的顺序对数据进行存取，所以使用 List 作为消息队列就可以满足消息的顺序性 但再消费者读取数据时，有一个潜在的性能风险点 在生产者向 List 中写入数据时，List 并不会主动的通知消费者有新消息写入，如果消费者想要及时处理消息，就需要在程序中不停的调用 RPOP 命令，如果有新消息写入，就会返回结果，否则返回空值继续循环 所以即使没有新消息写入 List，消费者也要不停的调用 RPOP 命令，这就会导致消费者程序的 CPU 一直消耗在执行 RPOP 命令上，带来不必要的性能损失 为了解决此问题，Redis 提供了 BRPOP 命令。BRPOP 命令也称为阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据，这种方式可以节省 CPU 开销 3. 重复消息消费者要实现重复消息的判断，需要满足两个条件： 每个消息都有一个全局的 ID 消费者要记录已经处理过的消息的 ID，当收到一条消息后，消费者程序就可以对比收到的消息 ID 和记录的已处理过的消息 ID，以此来判断当前收到的消息有没有经过处理 但是List 并不会为每个消息生成 ID 号，所以我们需要自行为每个消息生成一个全局唯一 ID，生成之后我们再用 LPUSH 命令把消息插入 List 时，需要在消息中包含这个全局唯一 ID 4. List 作为消息队列有什么缺陷？List 不支持多个消费者消费同一条消息，因为一旦消费者拉取一条消息后，这条消息就从 List 中删除了，无法被其他消费者再次消费 这也就是消费者组的概念，List 类型并不支持消费者组的实现 Hash简介Hash 是一个键值对（Key - value）集合，其中 value 的形式如：value=[&#123;field1，value1&#125;，...&#123;fieldN，valueN&#125;] 适合用于存储对象 内部实现Hash 类型的底层数据结构由压缩列表和哈希表实现： 若哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置），Redis 会使用压缩列表作为 Hash 类型的底层数据结构 若哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的底层数据结构 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现 应用场景缓存对象Hash 类型的（key，filed，value）的结构与对象的（对象id，属性，值）的结构类似，可以用来存储对象 以用户信息为例，在关系型数据库中结构如下： uid name age 1 小明 15 2 小华 14 将用户信息存储到 Hash 类型： 123456789101112# 存储一个哈希表uid:1的键值&gt; HMSET uid:1 name 小明 age 152# 存储一个哈希表uid:2的键值&gt; HMSET uid:2 name 小华 age 142# 获取哈希表用户id为1中所有的键值&gt; HGETALL uid:11) &quot;name&quot;2) &quot;小明&quot;3) &quot;age&quot;4) &quot;15&quot; String + JSON 与 Hash 都可以缓存对象，如何应用？ 一般对象用 String + Json 存储，对象中某些频繁变化的属性可以考虑抽出来用 Hash 类型存储 购物车以用户 id 为 key，商品 id 为 field，商品数量为 value，恰好构成了购物车的三要素 命令如下： 添加商品：HSET cart:&#123;用户id&#125; &#123;商品id&#125; 1 添加数量：HINCRBY cart:&#123;用户id&#125; &#123;商品id&#125; 1 商品总数：HLEN cart:&#123;用户id&#125; 删除商品：HDEL cart:&#123;用户id&#125; &#123;商品id&#125; 获取购物车所有商品：HGETALL cart:&#123;用户id&#125; 当前仅仅是将商品ID存储到了Redis 中，在回显商品具体信息的时候，还需要拿着商品 id 查询一次数据库，获取完整的商品的信息 Set简介Set 类型是一个无序并唯一的键值集合，它的存储顺序不会按照插入的先后顺序进行存储 一个集合最多可以存储 2^32-1 个元素。概念和数学中个的集合基本类似，可以交集，并集，差集等等，所以 Set 类型除了支持集合内的增删改查，同时还支持多个集合取交集、并集、差集 Set 类型和 List 类型的区别如下： List 可以重复存储元素，Set 只能存储非重复元素 List 是按照元素的先后顺序存储元素的，而 Set 则是无序方式存储元素的 内部实现Set 类型的底层数据结构是由哈希表或整数集合实现的： 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries 配置）个，Redis 会使用整数集合作为 Set 类型的底层数据结构 如果集合中的元素不满足上面条件，则 Redis 使用哈希表作为 Set 类型的底层数据结构。 应用场景因为 Set 类型的主要特性为无序、不可重复、并交差操作，所以 Set 类型比较适合数据去重和保障数据的唯一性，还可以用来统计多个集合的交集、错集、并集等，当我们存储的数据是无序的并且需要去重的情况下，比较适合使用集合类型进行存储 这里有一个潜在风险，Set 的差集，并集，交集的计算复杂度较高，在数据量较大时，若直接执行这些计算，会导致 Redis 实例阻塞 在主从集群中，为了避免主库因为 Set 做聚合计算时导致主库被阻塞，可以选择一个从库完成聚合统计，或者将数据返回客户端进行聚合统计 点赞Set 类型可以保证一个用户只能点一个赞，举例： key 代表文章 id，value 代表用户 id uid 1、2、3 三个用户分别对 article 1 文章点赞 123456789# uid:1 用户对文章 article:1 点赞&gt; SADD article:1 uid:1(integer) 1# uid:2 用户对文章 article:1 点赞&gt; SADD article:1 uid:2(integer) 1# uid:3 用户对文章 article:1 点赞&gt; SADD article:1 uid:3(integer) 1 uid 1 取消了对 article 1 的点赞 12&gt; SREM article:1 uid:1(integer) 1 获取 article 1 文章所有的点赞用户 123&gt; SMEMBERS article:11) &quot;uid:3&quot;2) &quot;uid:2&quot; 获取 article 1 文章的点赞用户数量 12&gt; SCARD article:1(integer) 2 判断用户 uid 1 是否对文章 article 1 点赞 12&gt; SISMEMBER article:1 uid:1(integer) 0 # 返回0说明没点赞，返回1则说明点赞了 共同关注Set 类型支持交集运算，所以可以用来计算共同关注的好友、公众号等，举例： key 代表用户 id，value 代表已关注的公众号 id uid 1 用户关注公众号为 5、6、7、8、9，uid 2 用户关注公众号为 7、8、9、10、11 123456# uid:1 用户关注公众号 id 为 5、6、7、8、9&gt; SADD uid:1 5 6 7 8 9(integer) 5# uid:2 用户关注公众号 id 为 7、8、9、10、11&gt; SADD uid:2 7 8 9 10 11(integer) 5 uid 1 和 2 的共同关注 12345# 获取共同关注&gt; SINTER uid:1 uid:21) &quot;7&quot;2) &quot;8&quot;3) &quot;9&quot; 给 uid 2 推荐 uid 1 关注的公众号 123&gt; SDIFF uid:1 uid:21) &quot;5&quot;2) &quot;6&quot; 验证某个公众号是否同时被 uid 1、2 关注 1234&gt; SISMEMBER uid:1 5(integer) 1 # 返回0，说明关注了&gt; SISMEMBER uid:2 5(integer) 0 # 返回0，说明没关注 抽奖活动存储某活动中中奖的用户名，Set 的去重功能可以确保一个用户不会中奖两次，举例： key 代表抽奖活动名，value 代表员工名 将所有员工放入抽奖活动 12&gt;SADD lucky Tom Jerry John Sean Marry Lindy Sary Mark(integer) 5 如果允许重复中奖，可以使用 SRANDMEMBER 命令 123456789101112# 抽取 1 个一等奖：&gt; SRANDMEMBER lucky 11) &quot;Tom&quot;# 抽取 2 个二等奖：&gt; SRANDMEMBER lucky 21) &quot;Mark&quot;2) &quot;Jerry&quot;# 抽取 3 个三等奖：&gt; SRANDMEMBER lucky 31) &quot;Sary&quot;2) &quot;Tom&quot;3) &quot;Jerry&quot; 如果不允许重复中奖，可以使用 SPOP 命令 123456789101112# 抽取一等奖1个&gt; SPOP lucky 11) &quot;Sary&quot;# 抽取二等奖2个&gt; SPOP lucky 21) &quot;Jerry&quot;2) &quot;Mark&quot;# 抽取三等奖3个&gt; SPOP lucky 31) &quot;John&quot;2) &quot;Sean&quot;3) &quot;Lindy&quot; Zset简介Zset 类型相较于 Set 类型增加了排序属性，对于有序集合 Zset，每个存储元素相当于由两个值组成，一个有序集合的元素值，一个是排序值 有序集合保留了集合不能有重复成员的特性（分值可以重复），不同的是有序集合中的元素可以排序 内部实现Zset 类型的底层数据结构是由压缩列表或跳表实现的： 如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构 如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了 应用场景Zset 类型可以根据元素的权重排序，权重值可以由自己决定，例如可以根据元素插入 Zset 的时间确定权重值，先插入的元素权重小，后插入的元素权重大 在面对需要展示最新列表、排行榜等，数据更新频繁或需要分页显示的情况下可优先考虑 Zset 排行榜以博客文章点赞排名为例，A 用户分别发表五篇文章，获得点赞数为 200、40、100、50、150 123456789101112131415# arcticle:1 文章获得了200个赞&gt; ZADD user:A:ranking 200 arcticle:1(integer) 1# arcticle:2 文章获得了40个赞&gt; ZADD user:A:ranking 40 arcticle:2(integer) 1# arcticle:3 文章获得了100个赞&gt; ZADD user:A:ranking 100 arcticle:3(integer) 1# arcticle:4 文章获得了50个赞&gt; ZADD user:A:ranking 50 arcticle:4(integer) 1# arcticle:5 文章获得了150个赞&gt; ZADD user:A:ranking 150 arcticle:5(integer) 1 文章 arcticle:4 新增一个赞，可以使用 ZINCRBY 命令（为有序集合key中元素member的分值加上increment） 12&gt; ZINCRBY user:A:ranking 1 arcticle:4&quot;51&quot; 查看某篇文章的赞数，可以使用 ZSCORE 命令（返回有序集合key中元素个数） 12&gt; ZSCORE user:A:ranking arcticle:4&quot;50&quot; 获取 A 文章赞数最多的 3 篇文章，可以使用 ZREVRANGE 命令（倒序获取有序集合 key 从start下标到stop下标的元素） 12345678# WITHSCORES 表示把 score 也显示出来&gt; ZREVRANGE user:A:ranking 0 2 WITHSCORES1) &quot;arcticle:1&quot;2) &quot;200&quot;3) &quot;arcticle:5&quot;4) &quot;150&quot;5) &quot;arcticle:3&quot;6) &quot;100&quot; 获取 A 用户 100 赞到 200 赞的文章，可以使用 ZRANGEBYSCORE 命令（返回有序集合中指定分数区间内的成员，分数由低到高排序） 1234567&gt; ZRANGEBYSCORE user:A:ranking 100 200 WITHSCORES1) &quot;arcticle:3&quot;2) &quot;100&quot;3) &quot;arcticle:5&quot;4) &quot;150&quot;5) &quot;arcticle:1&quot;6) &quot;200&quot; 电话、姓名排序使用 Zset 的 ZRANGEBYLEX 或 ZREVRANGEBYLEX 可以帮助我们实现电话号码或姓名的排序，我们以 ZRANGEBYLEX （返回指定成员区间内的成员，按 key 正序排列，分数必须相同）为例 ※ 不要在分数不一致的 SortSet 集合中去使用 ZRANGEBYLEX 和 ZREVRANGEBYLEX 指令，因为获取的结果会不准确 1. 电话排序将电话存储到 Zset 中，根据需要获取号段 123456&gt; ZADD phone 0 13100111100 0 13110114300 0 13132110901 (integer) 3&gt; ZADD phone 0 13200111100 0 13210414300 0 13252110901 (integer) 3&gt; ZADD phone 0 13300111100 0 13310414300 0 13352110901 (integer) 3 获取所有号码 12345678910&gt; ZRANGEBYLEX phone - +1) &quot;13100111100&quot;2) &quot;13110114300&quot;3) &quot;13132110901&quot;4) &quot;13200111100&quot;5) &quot;13210414300&quot;6) &quot;13252110901&quot;7) &quot;13300111100&quot;8) &quot;13310414300&quot;9) &quot;13352110901&quot; 获取 132 号段的号码 1234&gt; ZRANGEBYLEX phone [132 (1331) &quot;13200111100&quot;2) &quot;13210414300&quot;3) &quot;13252110901&quot; 获取132、133号段的号码 1234567&gt; ZRANGEBYLEX phone [132 (1341) &quot;13200111100&quot;2) &quot;13210414300&quot;3) &quot;13252110901&quot;4) &quot;13300111100&quot;5) &quot;13310414300&quot;6) &quot;13352110901&quot; 2. 姓名排序12&gt; zadd names 0 Toumas 0 Jake 0 Bluetuo 0 Gaodeng 0 Aimini 0 Aidehua (integer) 6 获取所有人的名字 1234567&gt; ZRANGEBYLEX names - +1) &quot;Aidehua&quot;2) &quot;Aimini&quot;3) &quot;Bluetuo&quot;4) &quot;Gaodeng&quot;5) &quot;Jake&quot;6) &quot;Toumas&quot; 获取名字中大写字母A开头的所有人 123&gt; ZRANGEBYLEX names [A (B1) &quot;Aidehua&quot;2) &quot;Aimini&quot; 获取名字中大写字母 C 到 Z 的所有人 1234&gt; ZRANGEBYLEX names [C [Z1) &quot;Gaodeng&quot;2) &quot;Jake&quot;3) &quot;Toumas&quot; BitMap简介BitMap（位图），是一串连续的二进制数组（0 和 1），可以通过偏移量（offset）定位元素，BitMap 通过最小的单位 bit 来进行 0 或 1 的设置，表示某个元素的值或状态，时间复杂度为 O(1) 由于 bit 是计算机中最小的单位，使用它进行存储会非常节省空间，特别适合一些数据量大且使用二进制统计的场景 内部实现BitMap 本身是由 String 类型作为底层数据结构实现的一种统计二值状态的数据类型 String 类型是会保存为二进制的字节数组，所以 Redis 就将字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态，可以将 BitMap 看作一个 bit 数组 应用场景由于 BitMap 的特性可知，BitMap 类型十分适合二值状态统计的场景 签到统计在签到打卡的场景中，我们只需要记录签到和不签到两种状态 每个用户一天的签到用 1 bit 位就可以表示，一个月（按 31 天）的情况 31 个 bit 位就可以 假设需要统计 id 100 的用户在 2023 年 2 月的签到情况，如下 因为 offset 从 0 开始的，所以我们需要将返回的 value + 1 第一步，执行下面的命令，记录该用户 2 月 1 号已签到 1SETBIT uid:sign:100:202302 0 1 第二步，检查该用户 2 月 1 号是否签到 1GETBIT uid:sign:100:202302 0 第三步，统计该用户在 2 月份的签到次数 1BITCOUNT uid:sign:100:202302 这样，我们就知道该用户在 2 月的签到情况了 统计这个月的首次打卡时间 Redis 提供了 BITPOS key bitValue [start] [end]指令，返回数据表示 Bitmap 中第一个值为 bitValue 的 offset 位置 在默认情况下， 命令将检测整个位图， 用户可以通过可选的 start 参数和 end 参数指定要检测的范围，所以我们可以通过执行这条命令来获取 userID &#x3D; 100 在 2023 年 2 月份首次打卡日期： 1BITPOS uid:sign:100:202302 1 判断用户登录状态Bitmap 提供了 GETBIT、SETBIT 操作，通过一个偏移值 offset 对 bit 数组的 offset 位置的 bit 位进行读写操作，需要注意的是 offset 从 0 开始 只需要一个 key &#x3D; login_status 表示存储用户登陆状态集合数据， 将用户 ID 作为 offset，在线就设置为 1，下线设置 0。通过 GETBIT 判断对应的用户是否在线，5000 万用户只需要 6 MB 的空间 假设需要判断 id &#x3D; 10086 的用户登录情况： 第一步，执行以下指令，表示用户已登录 1SETBIT login_status 10086 1 第二步，检查该用户是否登陆，返回值 1 表示已登录 1GETBIT login_status 10086 第三步，登出，将 offset 对应的 value 设置成 0 1SETBIT login_status 10086 0 连续签到用户总数如何统计 7 天连续打卡用户的总数 ？ 将每天的日期作为 BitMap 的 key，userId 作为 offset，若是打卡则将 offset 位置的 bit 设置为 1 key 对应集合的每个 bit 位的数据则是一个用户在该日期的打卡记录 一共由 7 个这样的 BitMap，如果我们能对这 7 个 BitMap 的对应 bit 位做 [与] 运算，同样的 UserId offset 都是一样的，当一个 userId 在 7 个 BitMap 对应的 offset 位置的 bit &#x3D; 1 就说明该用户 7 天连续打卡 结果保存到一个新的 BitMap 中，我们再通过 BITCOUNT 统计 bit &#x3D; 1 的个数，便得到了连续打卡 7 天用户的总数了 Redis 提供了 BITOP operation destkey key [key ...] 这个指令用于对一个或者多个 key 的 Bitmap 进行位元操作 operation 可以是 and、OR、NOT、XOR。当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0 。空的 key 也被看作是包含 0 的字符串序列 假设要统计 3 天连续打卡的用户数，则是将三个 BitMap 进行 AND 操作，并将结果保存到 destmap 中，接着对 destmap 执行 BITCOUNT 统计，如下： 1234# 与操作BITOP AND destmap bitmap:01 bitmap:02 bitmap:03# 统计 bit 位 = 1 的个数BITCOUNT destmap 即使一天产生一亿的数据，BitMap 占用的内存也不大，大约占 12 MB 的内存左右（10^8&#x2F;8&#x2F;1024&#x2F;1024），7 天的 BitMap 内存开销约为 84 MB，同时我们可以给 BitMap 设置过期时间，让 Redis 删除过期的打卡数据，节省内存 HyperLogLog简介Redis HyperLogLog 是 Redis 2.8.9 版本新增的数据类型，用于 [统计基数] 的数据集合类型，基数统计指统计一个集合中不重复的元素个数，要注意 HyperLogLog 的统计规则是基于概率完成的，不是非常准确，标准误算率为 0.81% HyperLogLog 的优点是，在输入元素的数量或者体积非常大时，计算基数所需的内存空间总是固定的、很小的 在 Redis 内，每个 HyperLogLog 键只需花费 12KB 内存就可以计算接近 2^64 个不同元素的基数，与元素越多越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 十分节省空间 内部实现内容比较多参考 Wikipedia 和我以前的文章： HyperLogLog - Wikipedia HyperLogLog 详解 应用场景百万级网页 UV 计数在统计 UV 时，可以用 PFADD 命令（向 HyperLogLog 添加新元素）把访问页面的每个用户都添加到 HyperLogLog 中 1PFADD page1:uv user1 user2 user3 user4 user5 接下来便可以用 PFCOUNT 命令直接获得 page1 的 UV 值了，这个命令的作用就是返回 HyperLogLog 的统计结果 1PFCOUNT page1:uv 要注意，因为 HyperLogLog 是基于概率完成的，所以给出的结果存在一定误差，如果想精确统计结果最好还是使用 Set 或 Hash 类型 GEORedis GEO 是 Redis 3.2 版本新增的数据类型，主要用于存储地理位置信息，并对存储的信息进行操作 在日常生活中，我们常见的打车软件、附近的XX等功能都离不开基于位置信息服务（Location Based Service，LBS），LBS 应用访问的数据是和人或物关联的一组经纬度信息，GEO 很适合 LBS 服务的场景 内部实现GEO 的底层数据结构直接使用了 Zset 集合类型 GEO 类型使用 GeoHash 编码方法实现了经纬度到 Zset 中元素权重分数的转换，这其中的关键机制为 [对二维地图做区间划分]、[对区间进行编码]，一组经纬度落在某个区间后就用区间的编码值来表示，并把编码值作为 Zset 元素的权重分数 这样一来我们就可以把经纬度保存到 Zset 中，利用 Zset 提供的按权重进行有序范围查找的特性，实现 LBS 服务中频繁使用的 “搜索附近” 功能 应用场景打车假设车辆 id 为 33，经纬度为（116.034579，39.030452），我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations 执行下面的命令，就可以把 id 号为 33 的车辆的当前经纬度位置存入 GEO 集合 1GEOADD cars:locations 116.034579 39.030452 33 当用户想要寻找自己附近的车辆时，LBS 应用就可以使用 GEORADIUS 命令 例如，LBS 应用执行下面的命令时，Redis 会根据输入用户的经纬度信息（116.054579，39.030452 ）查找以这个经纬度为中心的 5 公里内车辆信息并返回给 LBS 应用 1GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10 Stream简介Redis Stream 是 Redis 5.0 版本新增加的数据类型，Redis 专门为消息队列设计的数据类型 在 Redis 5.0 Stream 没出来之前，Redis 消息队列的实现方式都有着各自的缺陷，例如： 发布订阅模式，不能持久化也就无法可靠的保存消息，并且对于离线重连的客户端不能读取历史消息的缺陷 List 实现消息队列的方式不能重复消费，一个消息消费完就会被删除，而且生产者需要自行实现全局唯一 ID 基于以上问题，Redis 5.0 便推出了 Stream 类型也是此版本最重要的功能，用于完美地实现消息队列，它支持消息的持久化、支持自动生成全局唯一 ID、支持 ack 确认消息的模式、支持消费组模式等，让消息队列更加的稳定和可靠","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://logerjava.github.io/tags/Redis/"}]},{"title":"MySQL 知识点总结","slug":"MySQL 知识点总结","date":"2023-02-03T05:11:24.000Z","updated":"2023-02-14T09:01:36.373Z","comments":true,"path":"2023/02/03/MySQL 知识点总结/","link":"","permalink":"http://logerjava.github.io/2023/02/03/MySQL%20%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"","text":"逻辑架构 如上图, 我们可以分为三层来进行解读 : 第一层的服务其实并不是 MySQL 独有的, 大多数的基于网络的客户端 &#x2F; 服务器的工具都有类似架构, 例如连接处理, 授权认证等 第二层是 MySQL 的核心服务, 大多数的 MySQL 功能都在这一层, 包括查询解析, 分析, 优化, 内置函数等, 所有跨存储引擎的功能也都在这一层实现 : 存储过程, 触发器, 视图等 第三层包含了存储引擎, 负责 MySQL 中数据的存储和提取, 不同的存储引擎具有不同的优势和劣势, 服务器则通过 API 与存储引擎进行通信, 这些 API 屏蔽了不同存储引擎的差异 索引B+Tree 原理数据结构B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。 在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 ps ：旧金山大学数据结构模拟工具 Data Structure Visualizations 操作进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。 插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。 与红黑树的比较红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能 1. B+ 树有更低的树高平衡树的树高 O(h)&#x3D;O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多 2. 磁盘访问原理操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I&#x2F;O 就能完全载入一个节点 如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取 3. 磁盘预读特性为了减少磁盘 I&#x2F;O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入 MySQL 索引索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现 B+ Tree 索引B+ Tree 索引是大多数 MySQL 存储引擎的默认索引类型 因为不需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组 可以指定多个列作为索引列，多个索引列共同组成键 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找；如果不是按照索引列的顺序进行查找，则无法使用索引 InnoDB 的 B+ Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引 辅助索引（二级索引）的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找 1. 聚簇索引聚簇索引在表建立时就已经建立了，举一个例子： 1SELECT * FROM student WHERE studentId = 6 要查询学生表中 id 为 6 的学员全部信息，当没有聚簇索引的情况下，也就是无序情况下，需要进行全表查询才能找到，此处可能会有疑问，既然主键是自增的那么不可以用二分查找解决吗 ？ 答案是不可以的，试想如果数据在写入磁盘时是 1、2、3、4、5、6 的形式，此时我删除了 4，那么如何将 5 向前移动呢 ？如果没有维护一个有序的数组结构，那么数据在磁盘就认为它是无序的，如果维护了一个有序的数组那么也是建立了索引，只不过换了一种数据结果 当创建了聚簇索引，其情况如下： 数据库里面有 1, 2, 3, 4, 5, 6 个学生 : 从上到下查找, 在根节点进行二分查找, 找到一个 studentId 所在的指针, 这里指向右侧 在叶子节点上进行二分查找, 找到 studentId &#x3D; 6 对应的 data 聚簇索引是无论如何都会创建的, 如果设定了主键, 则主键是聚簇索引, 如果没有设定主键会寻找唯一键作为索引, 就算你连唯一键都没有也没关系, MySQL 会建立一个 rowid 字段, 来完成 B+ Tree 2. 二级索引在查询时我们并不能总是知道主键，所以也会用姓名、手机号等作为查询条件，而在聚簇索引中并没有这两项，这就需要建立辅助索引（二级索引） 1CREATE INDEX idx_studentName ON student(studentName) 建立学生姓名索引会新创建一个 B+ Tree，里面存储学生姓名，data 存储着主键 id 1SELECT * FROM student WHERE studentName = &#x27;F&#x27; 当在二级索引中查询姓名为 F 的学员时： 二分查找, 找到对应指针 会在 B+ Tree 中找到 key 是 F 的记录 二级索引的 data 域中存储的是主键id, 那么就拿到主键 id 因为是查询全部字段, 所以用主键 id 到聚簇索引中进行查找 3. 复合索引复合索引同样属于二级索引的范畴，而又不太相同 1CREATE INDEX idx_studentName_age ON student(studentName,age) 当建立复合索引时，会再次新创建一个 B+ Tree 此时索引的 key 中存在有姓名和年龄，其他查询步骤与二级索引相同，需要注意的是如上图中，姓名 E 的 14 岁学员，姓名 E 的 15 岁学员，同名不同年龄，那么在排序比较时，会先按照姓名比较，如果姓名相同再按照年龄比较 哈希索引哈希索引可以以 O(1) 时间进行查找，但是失去了有序性： 无法用于排序与分组 只支持精确查找，无法用于部分查找和范围查找 InnoDB 存储引擎有一个特殊的功能叫 “自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+ Tree 索引之上再创建一个哈希索引，这样就可以让 B+ Tree 索引具有哈希索引的一些优点，比如更快的哈希查找 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。 查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引 空间数据索引MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。 空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据 日志redo logMySQL 是按页为单位来读取数据的，个页里面有很多行记录，从内存刷数据到磁盘，也是以页为单位来刷 1. redo log 的概念1UPDATE student SET name = &#x27;loger&#x27; WHERE studentId = 3 在数据库执行此SQL，MySQL 的操作如下： 首先判断内存中有没有 studentId &#x3D; 3 的数据 如果没有就去磁盘找到这条数据所在的页, 将整页数据加载到内存 然后找到 studentId &#x3D; 3 的行数据, 将内存中的 name 修改为 loger 我们可以发现数据存在不一致的情况，内存中的数据为正确的修改过后的新数据，磁盘中的数据为未修改的旧数据，此时磁盘对应的页的数据称为脏页 内存修改而磁盘未修改时掉电 MySQL 是怎么解决的 ？ 基于这种场景 MySQL 的解决方案是将对页的操作、修改内容，记录下来保存到磁盘，也就是 redo log，在 redo log 写入成功后，MySQL 就认为十五已经提交成功且数据已经持久化，并会在空闲时间将内存数据刷入磁盘；如果此时掉电，只需在重启后将脏页数据加载到内存中，然后利用 redo log 脏页就会修正了 2. redo log 如何存储 根据 MySQL 的官方文档可以大致分析处如下几点： redo log 记录了 SQL 语句以及其他 API 对表产生的变化, 也就是物理变化 redo log 存储在磁盘, 用于 crash recovery 后修正数据, 也就是处理宕机, 掉电等问题 redo log 默认有两个文件 redo log 采取循环写方式 (circular) 实际上 redo log，默认是在 ib_logfile0 和 ib_logfile1 循环来回写的 redo log 在写入磁盘时并不是随机 I&#x2F;O 而是顺序 I&#x2F;O 所以写入速度很快, 并且 redo log 文件体积又很小, 所以恢复速度很快 binlog1. binlog 的概念1UPDATE student SET name = &#x27;loger&#x27; WHERE studentId = 3 在执行这条 SQL 语句时，不仅仅生成了 redo log，还生成了 binlog binlog 记录了数据库表结构和表数据变更, 比如 insert, delete, update, create 等, 不会记录查询 不同于 redo log，binlog 是 MySQL 在 Server 层的功能，而 redo log 则是 InnoDB 存储引擎的功能，也就是说 redo log 只要使用了 InnoDB 作为存储引擎就会有，而 binlog 只要是使用 MySQL 就会有 2. binlog 的作用MySQL 既然将 binlog 放在了 Server 层，就代表 binlog 提供了通用的能力，binlog 有两个作用： 数据恢复 找到前一时间点的 binlog 进行重放就可以恢复数据 主从复制 master 将 binlog 发送给 slave，slave 执行 binlog 那么就复制了 对比 redo log 与 binlogredo log 和 binlog 都可以作为恢复手段，但其实他们的细节部分还是不一样的 1. 存储内容不同binlog 记录的是 insert、delete、update、create 等 SQL 语句，而 redo log 记录的是物理修改内容，可以理解为，binlog 记录的是逻辑变化，redo log 记录的是物理变化 2. 功能不同redo log 写入内存，如果数据库宕机，可以通过 redo log 恢复内存还没有刷盘的数据，也就是可以恢复宕机之前的内存数据 binlog 可以保持主从一致性，如果整个数据库都被删除了，binlog 存储着所有数据的变更情况，可以通回放 binlog 进行恢复 需要注意，如果整个数据库都被删除了，redo log 是无法恢复的，因为 redo log 并不会记录历史的所有数据，文件内容会被覆盖 两段式提交redo log 和 binlog 都会在执行 update 的时候写入，那么是怎么写入的呢 ？ 写入：redo log（prepare） 写入：binlog 写入：redo log（commit） 为什么写入 redo log 需要两段式提交而不是一次性写入呢 ？下面分为两种情况分析 先写入 redo log 再写入 binlog： 如果 redo log 写入成功，写入 binlog 失败，此时出现宕机情况需要恢复数据，主从复制情况下，master 采用 redo log 恢复数据，从库采用 binlog，但是 binlog 写入失败不存在 binlog，那么从库就没有这些数据，导致主从不一致 先写入 binlog 再写入 redo log： 与上面情况实际是相同的，binlog 存在而 redo log 不存在，导致从库是最新数据，而主库出现问题 两段式提交： 写入 redo log，若失败则回滚，不再继续写入 binlog 若 redo log 写入成功，binlog 写入失败则回滚，删除无效的 binlog 只有当 redo log 和 binlog 都写入成功此次事务才会提交 可以看出 MySQL 需要保证 redo log 和 binlog 是一致的 undo logundo log 主要负责回滚与多版本控制（MVCC） 跳转MVCC undo log 实际上存储的也是逻辑日志，在执行 update 时不仅记录 redo log 与 binlog，undo log 也会进行记录，比如用户进行插入操作，那么 undo log 就会记录一条删除操作；若用户将 A 修改为了 B，则 undo log 就会记录一条 B 修改为 A 的记录 其目的就是为了保证回滚，这些 undo log 存储的记录就相当于之前 SQL 前的一个版本，回滚时直接返回上一个版本 error log错误日志, 记录着 MySQL 启动, 运行期间, 停止时的错误相关信息, 默认情况下是关闭的 可以指定 errorlog 的输出路径 : 编辑 my.cnf 写入 log-error &#x3D; [path] 通过命令参数错误日志 mysqld_safe –user&#x3D;mysql –log-error&#x3D;[path] &amp; general query log普通查询日志, 记录了 MySQL 接收到的所有查询或命令操作, 无论是正确还是错误的都会进行记录, 因为记录的比较频繁, 产生开销较大所以默认是关闭的 slow query log慢查询日志记录的是执行时间超过 long_query_time 和没有使用索引的查询语句, 只记录成功的语句 相关参数 : slow_query_log : 1. 开启; 0. 关闭 long_query_time : 慢查询阈值 log_output : 输出方式 通过如下方式配置慢查询 : 123show variables like &#x27;%slow_query_log%&#x27;;set global slow_query_log = 1;show variables like &#x27;%slow_query_log%&#x27;; 要注意的是, 此处修改只对当前数据库生效, 在 MySQL 重启后会失效, 如果需要配置永久生效需要修改 my.cnf 文件 锁MySQL 中锁的分类因为不同的存储引擎支持的锁机制是不同的，以下内容仅就 MyISAM 与 InnoDB 进行解析 需要注意 MyISAM 存储引擎仅支持表锁, InnoDB存储引擎既支持行级锁，也支持表级锁，但默认情况下是采用行级锁 1. MyISAM 中的表锁MyISAM 中的表锁可以分为两种： 表共享锁（Table Read Lock）：不会阻塞其他用户对同一张表的读请求，但会阻塞对同一张表的写请求 表独占锁（Table Write Lock）：会阻塞其他用户对同一张表的读写操作 MyISAM 表的读写是串行操作，当一个线程获得一个表的写锁后，只有持有锁的线程才可以对表进行更新操作，其他线程的读写操作都会阻塞，直到锁被释放 默认情况下，写锁优先级高于读锁，若存在锁争抢情况，在上一个锁被释放时会优先给写锁队列中等待的请求，然后再分配给读锁队列等待的请求 这也是 MyISAM 存储引擎的表不适合大量更新、查询操作的原因，在大量更新操作时，会导致查询操作难以获取读锁有可能出现一致阻塞的情况，并且时间较长的查询会导致写操作线程”饿死”，所以应用程序中应避免出现运行时间较长的查询操作 在 MyISAM 存储引擎中我们可以通过参数配置修改读写锁的优先级： low-priority-updates : 配置默认给读请求优先权 执行 set low-priority-updates &#x3D; 1 命令, 让该连接发出的更新请求优先级降低 指定 insert、update、delete 语句的 low_priority 属性, 降低执行语句的优先级 设置系统参数 max_write_lock_count 的值, 当表的读锁达到这个值后, MySQL 会暂时性的降低写请求的优先级, 从而使读进程获取锁 2. 查询表锁争用情况1234567mysql&gt; SHOW STATUS LIKE &#x27;Table%&#x27;;+-----------------------+---------+| Variable_name | Value |+-----------------------+---------+| Table_locks_immediate | 34800596 || Table_locks_waited | 0 |+-----------------------+---------+ 可以通过 table_locks_waited 和 table_locks_immediate 来分析表锁的争用情况, 如果 table_locks_immediate 数值较高, 则可以认为存在严重的锁竞争情况 3. InnoDB 中的锁在 InnoDB 中行锁和表锁是共存的, 它实现了如下两种行锁： 共享锁 (S) : 允许一个事务去读一行, 阻止其他事务获得相同数据集的排他锁 排他锁 (X) : 允许获得排他锁的事务更新数据, 阻止其他事务取得相同数据集的共享读锁和排他写锁 而为了兼容表锁和行锁, 实现多粒度的锁, InnoDB 中还存在两种意向锁, 这两种都是表锁： 意向共享锁 (IS) : 事务打算给数据行加行共享锁, 事务在给一个数据行加共享锁前必须先取得该表的意向共享锁 意向排他锁 (IX) : 事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的意向排他锁 锁兼容情况表： 共享锁 排他锁 意向共享锁 意向排他锁 共享锁 兼容 冲突 兼容 冲突 排他锁 冲突 冲突 冲突 冲突 意向共享锁 兼容 冲突 兼容 兼容 意向排他锁 冲突 冲突 兼容 兼容 若一个事务请求的锁与当前的锁是兼容的, InnoDB 就会把请求的锁给到该事务; 相反, 若两者不兼容, 那么该事务就要等待锁的释放 4. InnoDB 加锁方法加锁情况： 对于意向锁是 InnoDB 自动加的, 不需用户干预 对于 insert, update, delete 语句, InnoDB 会自动给涉及数据加上排他锁 对于 select 语句, InnoDB 不会进行加锁操作 也可以通过如下语句显示的加排他锁和共享锁： select … from 表名 where … lock in share mode ：其他会话仍然可以查询本条记录, 并且可以加 share mode 的共享锁, 但是如果需要对该记录进行更新, 那么就会有可能造成死锁 select … from 表名 where … for update ：其他会话可以查询该记录, 但是不能对此记录加排他锁, 共享锁, 而是阻塞等待获取锁 5. 隐式锁定与显示锁定1. 隐式锁定InnoDB 在事务执行时, 采用两阶段锁协议 : 在任何时间都可以执行锁定, InnoDB 会根据隔离级别自动加锁 锁只有在提交和回滚的时候才会在同一时间释放 2. 显示锁定1234-- 共享锁select ... lock in share mode-- 排他锁select ... for update select … for update : 我们一般在确保查询的是最新数据时会用到 for update, 在执行加了 for update 的查询语句时, 会将对查询行加排他锁, 也就是说只允许自己进行修改 select … lock in share mode : 在使用 lock in share mode 时, 会对查询数据加共享锁, 同样是为了确保最新数据, 不允许其他用户进行修改, 但同样的自己也不一定能修改这条数据, 因为有可能其他事务也存在对相同数据添加 lock in share mode 的情况 上面两种锁的区别： for update 为排他锁, 事务一旦获取此锁, 其他数据无法再在同样数据上添加 for update lock in share mode 是共享锁, 多个事务可以同时对同样数据添加 lock in share mode 3. 性能影响select … for update 语句实际上相当于一个 update 语句, 若事务没有及时提交或回滚的情况下, 有可能造成其他事务长时间等待的问题, 影响数据库的并发效率 select … lock in share mode 允许同时对数据上共享锁, 但是不能对数据进行更新操作, 同理如果不及时提交和回滚也可能造成大量事务等待问题 InnoDB 的间隙锁在用户使用范围查询而非等值查询并请求锁时，InnoDB 会将符合的已存在数据记录的索引项加锁，对于键值在条件范围内但并不存在的记录，叫做间隙（GAP），InnoDB 同样会对这个”间隙”加锁，这种锁机制就是所谓的间隙锁，间隙锁只会在 Repeatableread (可重复读) 隔离级别下使用 用学生表举例，如果 student 表存在 101 条记录，其中 studentId 分别是 1 - 101： 1SELECT * FROM student WHERE studentId &gt; 100 for update; 在上面的范围查询中，InnoDB 不仅会对复合条件的 101 进行加锁，还会对 studentId 大于 101 的”间隙”加锁，即便这些记录不存在 InnoDB 使用间隙锁的目的InnoDB 使用间隙锁的目的有两个： 防止幻读, 在可重复读的隔离级别下, 通过 GAP 锁是可以避免幻读的 满足恢复和复制的需求 MySQL 是通过 binlog 回放执行成功的增删改 SQL 语句来进行主从复制和数据恢复的，根据其恢复特点分析恢复和复制的需求： binlog 恢复是重放 SQL 语句 binlog 按照事务提交的先后顺序记录, 回放也是根据这个顺序回放 可以看出, 在一个事务没有提交之前, 其他并发事务不能插入满足其锁定条件的任何记录, 通俗来讲就是不允许出现幻读 获取 InnoDB 行锁的争用情况通过 innodb_row_lock 变量来分析行锁的争用情况： 1234567891011mysql&gt; show status like &#x27;innodb_row_lock%&#x27;;+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| InnoDB_row_lock_current_waits | 0 | -- 当前正在等待锁定的数量；| InnoDB_row_lock_time | 6345955 | -- 从系统启动到现在锁定总时间长度；| InnoDB_row_lock_time_avg | 287 | -- 每次等待所花平均时间；| InnoDB_row_lock_time_max | 51094 | -- 从系统启动到现在等待最长的一次所花的时间；| InnoDB_row_lock_waits | 22069 | -- 系统启动后到现在总共等待的次数；+-------------------------------+-------+5 rows in set (0.01 sec) 事务什么是事务概念 : 事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚 ACID： 原子性 : 事务被视为不可分割的最小单元, 事务的所有操作要么全部提交成功, 要么全部失败回滚 一致性 : 数据库在事务执行前后都保持一致性状态, 在一致性状态下, 所有事务对同一个数据的读取结果都是相同的 隔离性 : 一个事务所做的修改在最终提交以前, 对其它事务是不可见的 持久性 : 一旦事务提交, 则其所做的修改将会永远保存到数据库中, 即使系统发生崩溃, 事务执行的结果也不能丢失 ACID 并非平级关系： 只有满足一致性的前提下, 事物的执行结果才认为是正确的 无并发情况下, 事务是串行执行的, 隔离性一定能够满足, 此时只要还满足原子性, 那么就一定能满足一致性 并发情况下, 多个事务并行执行, 事务不仅要满足原子性, 还要满足隔离性, 才可以满足一致性 事务满足持久化是为了能应对系统崩溃的情况 MySQL 默认采用自动提交模式, 这表示, 如果不在 SQL 中显示的使用 START TRANSACTION 语句开始一个事务, 那么每个操作都会被当做一个事务并自动提交 并发下一致性问题1. 脏读脏读问题指事务读取了未提交的数据, 例如 : A 事务修改 id &#x3D; 1 的 name 从 ‘铁蛋’ 修改为 ‘loger’, 但未提交 随后 B 事务读取 id &#x3D; 1 的数据, 获取数据为 ‘loger’ 如果 A 事务进行回滚操作, 撤销了修改, 那么 B 事务读取的就是脏数据 2. 不可重复度不可重复读指在一个事务内多次读取同一数据集合, 在一个事务未结束, 另一个事务也访问了同一数据进行修改, 由于第二次的修改就会导致, 第一次事务的两次读取可能不一致, 例如 : A 事务读取一个数据, 并未结束 B 事务访问同一数据, 并做修改 如果 A 事务再次读取这个数据, 此时读取结果会与第一次读取结果不同 3. 幻读幻读本质上也属于不可重复读的情况, 一个事务在前后两次查询同一范围的时候，后一次查询看到了前一次查询没有看到的行, 例如 : A 事务按照一定条件读取范围数据 B 事务在这个范围内插入了新的数据 当 A 事务再次按照条件读取范围数据时, 发现 B 事务插入的数据, 第一次读取结果与第二次不同 4. 丢失更新丢失更新指一个事务的更新操作会被另一个事务的更新操作所覆盖, 例如 : A, B 两个事务都对同一个数据进行修改 A 先执行完毕, 提交后生效 B 随后执行完毕, 提交后覆盖了 A 的修改 事务隔离级别并发下一致性问题其原因在于破坏了事务的隔离性，通过确保隔离性就可以解决上述问题，其方案有： 通过锁 通过事物的隔离级别 事务的隔离级别分为以下四种： 读未提交 (READ UNCOMMITTED)：事务可以读取未提交数据 读已提交 (READ COMMITTED)：事务只可以读取已经提交的事务所做的修改 可重复读 (REPEATABLE READ)：同一个事务多次读取同样记录结果一致 可串行化 (SERIALIZABLE)：读取每一行数据上都加锁, 强制事务串行执行 事务隔离级别能处理的一致性问题表格如下： 脏读 不可重复读 幻读 读未提交 × × × 读已提交 √ × × 可重复读 √ √ × 可串行化 √ √ √ 多版本并发控制(MVCC)多版本并发控制 (Multi-Version Concurrency Control, 以下简称 MVCC), 是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式, 用于实现提交读和可重复读这两种隔离级别, 而未提交读隔离级别总是读取最新的数据行, 要求很低, 无需使用 MVCC, 可串行化隔离级别需要对所有读取的行都加锁, 单纯使用 MVCC 是无法实现的 1. 名词解释版本号： 系统版本号 SYS_ID : 是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增 事务版本号 TRX_ID : 事务开始时的系统版本号 回滚指针 ROLL_POINTER : 一个指针, 指向上一个版本位置 当前读与快照读： 当前读：像 SELECT … LOCK IN SHARE MODE (共享锁), SELECT … FOR UPDATE, UPDATE, INSERT, DELETE, 这些操作都是一种当前读, 它们读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁 快照读：不加锁的 SELECT 操作就是快照读, 快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读, 快照读基于 MVCC，为了提高并发性能的考虑 ReadView：MVCC 在内部维护了一个 ReadView 结构, 内部包含当前系统未提交的事务列表 (TRX_IDs), 该列表的最小值 (TRX_ID_MIN) 和最大值 (TRX_ID_MAX), 在进行 SELECT 操作时, 根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系, 从而判断数据行快照是否可以使用 2. 工作原理当一个事务开始时，数据库系统为其创建一个数据的快照版本，并且当事务结束时，该版本的修改将被提交。其他事务在读取该数据时，也看到的是其对应的快照版本，而不会受到当前事务的影响 细节： 获取事务自己的版本号, 也就是 TRX_ID 获取 ReadView 查询得到的数据，然后与 ReadView 中的事务版本号进行比较 TRX_ID &lt; TRX_ID_MIN, 表示该数据行快照是在当前所有未提交事务之前进行更改的, 因此可以使用 TRX_ID &gt; TRX_ID_MAX, 表示该数据行快照是在事务启动之后被更改的, 因此不可使用 TRX_ID_MIN &lt;&#x3D; TRX_ID &lt;&#x3D; TRX_ID_MAX, 需要根据隔离级别再进行判断 读未提交 : 直接读取最新版本 ReadView 读已提交 : 每次查询的开始都会生成一个独立的 ReadView 可重复读 : 可重复读隔离级别则在第一次读的时候生成一个 ReadView，之后的读都复用之前的 ReadView 如果不符合 ReadView 规则， 那么就需要 undo log 中回滚指针 ROLL_POINTER 找到下一个快照，再进行上面的判断","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"集合知识点总结(附源码解析)","slug":"集合知识点总结-附源码解析","date":"2023-02-01T02:30:03.000Z","updated":"2023-02-14T08:43:47.495Z","comments":true,"path":"2023/02/01/集合知识点总结-附源码解析/","link":"","permalink":"http://logerjava.github.io/2023/02/01/%E9%9B%86%E5%90%88%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93-%E9%99%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"概览ListArrayList ： 底层为数组实现，查找访问效率高，增删效率较低 每次扩容为原来的 1.5 倍，默认初始化容量为 10 增删效率低的原因是因为，需要进行 navite 方法的数组拷贝复制 线程不安全 LinkedList ： 底层双向链表实现，随机访问效率低，增删效率高 Vector ： 底层数组实现，现在比较少用 Vector 的所有方法均为synchronized修饰，性能损耗较大 Vector 初始 length 是 10 超过 length 时以 100% 比率增长（2 倍），相比于 ArrayList 消耗更多的内存 SetTreeSet ： 基于红黑树实现，支持有序性操作 查找效率不如 HashSet HashSet ： 基于哈希表实现，支持快速查找，不支持有序性操作 LinkedHashSet ： 具有 HashSet 的查找效率，并且内部使用双向链表维护元素的插入顺序 MapHashMap ： 基于哈希表实现 TreeMap ： 基于红黑树实现 HashTable ： 与 HashMap 类似，但是其线程安全，同一时刻多线程同时写入 HashTable 不会导致数据不一致，但是不建议使用遗留类，若想使用线程安全的 Map 建议使用 ConcurrentHashMap LinkedHashMap ： 使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序 源码分析及拓展ArrayList扩容12345678910111213141516171819202122232425262728293031public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 进入add方法分别分为四部分： 首先进入 add() 方法，为了得到最小的容量，避免资源浪费，这里用了 size + 1 确认一下 +1 后是否满足我们的需求 在 ensureExplicitCapacity(int minCapacity) 方法中判断 minCapacity - elementData.length &gt; 0 如果添加容量大于最小容量则调用 grow() 进行扩容 在 grow()方法中将，新容量 &#x3D; 旧容量 + 旧容量右移1位 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 此处相当于1.5倍扩容 然后调用 copyOf() 方法，复制指定数组 可以看到，需要进行扩容就需要调用 Arrays.copyOf() 将原数组整个复制到新数组中，此操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少需要的扩容次数 删除元素12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; remove方法的实现步骤： 先检查角标是否越界 删除掉指定元素 因为 ArrayList 是数组的数据结构，所以需要计算出要移动的个数，将后面的向前移动 将 elementData[--size] 设为 null，来让 GC 回收 需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，我查了一下该操作的时间复杂度为 O(N)，可以看到 ArrayList 删除元素的代价是非常高的 elementData[–size] 为什么要设为 null ？ 这个问题首先要了解 GC 回收的原理，GC 回收采用可达性分析算法，通过 GC Roots 到节点是否可达来进行对象的回收。回到问题，删除一个元素后，numMoved 长度的数组元素，要向前移动，很明显移动过后 list 中的数组长度减少了 1。所以此处将多出的这个引用，也就是 elementData[–size] 设为 null ， 让 GC 可以正确回收内存，并且 size 自身减少1后可以保证 size 属性可以返回 list 所需要的正确数组长度。 我们可以举一个具体的例子，比如此处为 0、1、2、3、4、5 现在我想 remove 掉下标为 1 的元素，此处变为 0、2、3、4、5、5 最后的 5 依旧存在，所以将它设为 null 进行释放。 Vector同步12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125; Vector 的实现与 ArrayList 类似，但是使用了 synchronized 进行同步 扩容12345678public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125; 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; Vector 的构造函数可以传入 capacityIncrement 参数，它的作用是在扩容时使容量 capacity 增长 capacityIncrement，如果这个参数的值小于等于 0，扩容时每次都令 capacity 为原来的两倍 1234567public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 而在调用没有 capacityIncrement 参数的构造函数时，capacityIncrement 值默认为 0，也就是说默认情况下 Vector 每次扩容的容量都会翻倍 同 ArrayList 的比较 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢，最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己控制 Vector 每次扩容请求其大小的 2 倍，而 ArrayList 是 1.5 倍 替代方案可以使用 Collections.synchronizedList() 得到一个线程安全的 ArrayList 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteArrayList读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响 写操作需要加锁，防止并发写入时导致写入数据丢失 写操作结束之后需要把原始数组指向新的复制数组 123456789101112131415161718public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125; 1234@SuppressWarnings(&quot;unchecked&quot;)private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 适用场景CopyOnWriteArrayList 在写操作的同时允许读操作的进行，这样大大提高了读的性能，因此很适合读多写少的应用场景 但是 CopyOnWriteArrayList 也存在缺陷 ： 内存占用 ：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右 数据不一致 ：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景 LinkedList同 ArrayList 的比较ArrayList 基于动态数组实现，LinkedList 基于双向链表实现。ArrayList 和 LinkedList 的区别可以归结为数组和链表的区别 ： 数组支持随机访问，但插入删除代价很高，需要移动大量元素 链表不支持随机访问，但插入删除只需要改变指针 HashMap存储结构JDK 1.8 以前 ：HashMap 的数据结构为数组+链表实现的，数组中存储了 key-value 的键值对（Entry），在进行插入操作时会根据 key 的 hash 计算出 index 值表示在数组中插入的位置，而 hash 存在概率性，不同的 key 计算 hash 出的 index 可能是一样的，这样就形成了链表 JDK 1.8 及之后 ：HashMap 的数据结构为数组+链表+红黑树，当链表长度超过 8 时会自动转为红黑树，小于 6 时重新变为链表 头插法在 JDK 1.8 以前，采用头插法处理链表插入问题 例如实例化如下 HashMap 1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;K1&quot;, &quot;V1&quot;);map.put(&quot;K2&quot;, &quot;V2&quot;);map.put(&quot;K3&quot;, &quot;V3&quot;); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16&#x3D;3 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6，插在 &lt;K2,V2&gt; 前面 链表指针顺序为 k3 -&gt; k2 环形链表问题环形链表问题是 JDK 1.8 HashMap 将链表插入从头插法变为尾插法的主要原因 假设有一个容量大小为 2 的 HashMap，负载因子 0.75，现将插入 α、β、γ 三个数据 key 为 1、2、3 因为扩容机制的原因，上述图片其实并不可能发生，事实上在插入第二个元素时就会进行扩容，因为 JDK 1.7 采用头插法的原因，新加入的元素会放在链表的头部，又因为经过 rehash 后同一链表上的元素可能被放到数组的其他位置，所以可能会变成下面的样子 可以发现指针位置是可能发生改变的，此时我们带入多线程的场景，用不同的线程操作分别插入 1、2、3 因为头插法更换指针的原因，可能就会变成下面的样子 因为头插法会改变链表的顺序，在多线程场景下，就有可能会出现环形链表的问题，陷入无限循环 尾插法在 JDK 1.8 及之后，采用尾插法处理链表插入问题 例如实例化如下 HashMap 1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;K1&quot;, &quot;V1&quot;);map.put(&quot;K2&quot;, &quot;V2&quot;);map.put(&quot;K3&quot;, &quot;V3&quot;); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16&#x3D;3 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6，插在 &lt;K2,V2&gt; 后面 链表指针顺序为 k2 -&gt; k3，并且在扩容后不会改变链表顺序，所以尾插法并不会出现环形链表问题 扩容 - 基本原理123456789101112131415static final int DEFAULT_INITIAL_CAPACITY = 16; // table 的容量大小，默认为 16, 需要注意的是 capacity 必须保证为 2 的 n 次方static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient Entry[] table;transient int size; // 键值对数量int threshold; // size 的临界值,当 size 大于等于 threshold 就必须进行扩容操作final float loadFactor; // 装载因子, table 能够使用的比例, threshold = (int)(capacity* loadFactor)transient int modCount; 123456void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length);&#125; 当需要扩容时，令 capacity 为原来的两倍 123456789101112131415161718192021222324252627282930void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把 oldTable 的所有键值对重新插入 newTable 中，因此这一步很费时 扩容 - 重新计算桶下标在进行扩容时，需要将键值对重新计算桶下标，从而放到对应桶上，HashMap 计算桶下标方法如下，HashMap capacity 为 2^n 这一特点能够极大降低重新计算桶下标的复杂度 123456int hash = hash(key);int i = indexFor(hash, table.length);static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 假设原数组长度为 16，扩容之后 new capacity 为 32 12capacity : 00010000new capacity : 00100000 对于一个 key 值，它的哈希值在第 5 位 ： 为 0 时，hash % 00010000 &#x3D; hash % 00100000，桶位置和原来一致 为 1 时，hash % 00010000 &#x3D; hash % 00100000 + 16，桶位置是原位置加16 扩容 - 计算数组容量HashMap 构造函数允许用户传入容量为非 2^n 的情况，因为它可以自动将传入容量转换为 2^n 先考虑如何求一个数的掩码，对于 10000000 其掩码为 11111111，可以使用如下方法 ： 12345int num = 1000 0000int mask = nummask |= mask &gt;&gt; 1 11000000 // 将num右移一位得到 0100 0000，然后与原始的num(1000 0000)进行或运算，得到mask（1100 000）mask |= mask &gt;&gt; 2 11110000 // 将刚得到的mask再右移一位得到 0011 0000，然后与上一步得到的mask(1100 0000)进行或运算，得到新mask（1110 0000）mask |= mask &gt;&gt; 4 11111111 // 我们再次将刚得到的mask右移4位，得到0000 1111,然后与上一步得到的mask进行与运算，得到1111 1111 mask + 1 是大于原始数字的最小的 2^n 12num 10000000mask + 1 100000000 以下是 HashMap 中计算数组容量的代码 ： 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 同 Hashtable 的对比 Hashtable 使用 synchronized 来进行同步 HashMap 可以插入键为 null 的 Entry HashMap 的迭代器是 fail-fast 迭代器 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的 ConcurrentHashMap存储结构 在 JDK 1.8 以前 ConcurrentHashMap 和 HashMap 实现上类似，主要差别在于 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），也就是说 ConcurrentHashMap 的底层实现为 Segments + HashEntry 数组 123456static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;&#125; Segment 继承自 ReentrantLock，查看源码的话还可以发现对每个 Segment 都有进行单独加锁的操作, 我们也可以这样认为 Segment 的个数为锁的并发度 1234567891011121314151617static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;&#125; 1final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; HashEntry 虽然用来存储键值对但是和 HashMap 并不是相同的, 因为 HashEntry 采用 volatile 字段修饰了 value 和 指向下一节点的 next, 确保了可见性, 这也是为什么 ConcurrentHashMap 高效的原因之一, 因为 volatile 原因导致它的 get 方法根本不用加锁 JDK 1.7 的 ConcurrentHashMap 采用 CAS 方式更新 baseCount 来确保线程安全, 如果失败则必定存在线程竞争关系, 此时会调用 scanAndLockForPut() 方法自旋获取锁, 在其内部存在 MAX_SCAN_RETRIES 可以理解为最大重试次数, 如果达到了则改为阻塞锁获取, 确保修改成功 在 JDK 1.8 时, ConcurrentHashMap 放弃了分段锁, 取而代之的类似于 HashMap 的数组 + 链表 + 红黑树结构，采取 CAS + Synchronized 的方式来保证线程安全，put 操作流程如下 ： 首先根据 key 计算出 hashCode 判断是否需要进行初始化操作 根据 key 定位到 Node (1.7 中的 HashEntry), 如果是 null 则表示当前位置可以写入数据, CAS 操作写入, 若失败则自旋确保成功 如果当前的 hashCode &#x3D;&#x3D; MOVED &#x3D;&#x3D; -1, 则表示需要扩容 若都不满足, 就利用 Synchronized 锁写入数据 若数据大于 TREEIFY_THRESHOLD, 也就是大于 8 就转为红黑树","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"LiteFlow 复杂业务逻辑解耦","slug":"LiteFlow-复杂业务逻辑解耦","date":"2022-11-11T02:31:01.000Z","updated":"2023-02-14T08:42:55.639Z","comments":true,"path":"2022/11/11/LiteFlow-复杂业务逻辑解耦/","link":"","permalink":"http://logerjava.github.io/2022/11/11/LiteFlow-%E5%A4%8D%E6%9D%82%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91%E8%A7%A3%E8%80%A6/","excerpt":"","text":"应用场景参考官网: https://liteflow.yomahub.com/ 使用示例pom12345&lt;dependency&gt; &lt;groupId&gt;com.yomahub&lt;/groupId&gt; &lt;artifactId&gt;liteflow-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; demoyml 配置: 12liteflow: rule-source: config/flow.el.xml resource 下 config&#x2F;flow.el.xml 定义规则: 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;flow&gt; &lt;chain name=&quot;chain1&quot;&gt; THEN(a, b, c); &lt;/chain&gt;&lt;/flow&gt; 测试类: 12345678910@Component(&quot;a&quot;)public class ACmp extends NodeComponent &#123; @Override public void process() throws Exception &#123; Thread.sleep(3000); System.out.println(&quot;a : &quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;))); &#125;&#125; 12345678@Component(&quot;b&quot;)public class BCmp extends NodeComponent &#123; @Override public void process() throws Exception &#123; Thread.sleep(3000); System.out.println(&quot;b : &quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;))); &#125;&#125; 12345678@Component(&quot;c&quot;)public class CCmp extends NodeComponent &#123; @Override public void process() throws Exception &#123; Thread.sleep(3000); System.out.println(&quot;c : &quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;))); &#125;&#125; 1234567@SpringBootApplication@ComponentScan(&#123;&quot;com.loger.java.test&quot;&#125;)public class ApplicationRun &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationRun.class, args); &#125;&#125; 测试: 1234567891011121314@SpringBootTest@RunWith(SpringRunner.class)public class TestAll &#123; @Resource private FlowExecutor flowExecutor; @Test public void test1()&#123; LiteflowResponse liteflowResponse = flowExecutor.execute2Resp(&quot;chain1&quot;, &quot;arg&quot;); System.out.println(JSON.toJSONString(liteflowResponse)); &#125;&#125; LiteFlow 可以将复杂的业务代码拆分为一个个小组件, 根据定义的规则流程进行运行","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"GroovyClassLoader 从字符串中加载解析代码","slug":"GroovyClassLoader-从字符串中加载解析代码","date":"2022-11-11T02:28:39.000Z","updated":"2023-02-14T08:42:20.596Z","comments":true,"path":"2022/11/11/GroovyClassLoader-从字符串中加载解析代码/","link":"","permalink":"http://logerjava.github.io/2022/11/11/GroovyClassLoader-%E4%BB%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%8A%A0%E8%BD%BD%E8%A7%A3%E6%9E%90%E4%BB%A3%E7%A0%81/","excerpt":"","text":"应用场景个性化较强, 复杂多变的业务场景, 可将代码持久化到数据库, 在执行时直接执行数据库脚本达到动态更新效果 使用示例pom12345&lt;dependency&gt; &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt; &lt;artifactId&gt;groovy&lt;/artifactId&gt; &lt;version&gt;$&#123;groovy.version&#125;&lt;/version&gt;&lt;/dependency&gt; 示例代码123456/** * 抽象类 */public abstract class TestHandler &#123; public abstract String testHandler(String str) throws Exception;&#125; 1234567891011121314151617181920@Testpublic void testAll() throws Exception &#123; GroovyClassLoader groovyClassLoader = new GroovyClassLoader(); Class&lt;?&gt; clazz = groovyClassLoader.parseClass( &quot;package com.loger.java.test;\\n&quot; + &quot;\\n&quot; + &quot;import com.loger.java.test.TestHandler;\\n&quot; + &quot;\\n&quot; + &quot;public class DemoTestHandler extends TestHandler &#123;\\n&quot; + &quot;\\n&quot; + &quot;\\t@Override\\n&quot; + &quot;\\tpublic String testHandler(String str) throws Exception &#123;\\n&quot; + &quot; return str;\\n&quot; + &quot;\\t&#125;\\n&quot; + &quot; \\n&quot; + &quot;\\n&quot; + &quot;&#125;\\n&quot;); TestHandler testHandler = (TestHandler) clazz.newInstance(); System.out.println(testHandler.testHandler(&quot;有参测试 !!!&quot;));&#125; 通过 GroovyClassLoader 中的 parseClass 方法, 编译加载 Groovy 脚本(可支持纯 Java 代码), 脱离 Java 的双亲委派模型 通过反射构建对象, 执行方法","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"KeyTool 工具生成密钥对","slug":"KeyTool-工具生成密钥对","date":"2022-11-09T05:56:50.000Z","updated":"2023-02-14T08:42:31.986Z","comments":true,"path":"2022/11/09/KeyTool-工具生成密钥对/","link":"","permalink":"http://logerjava.github.io/2022/11/09/KeyTool-%E5%B7%A5%E5%85%B7%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5%E5%AF%B9/","excerpt":"","text":"生成 JKS1keytool -genkeypair -alias *** -keyalg RSA -keypass *** -keystore xxx.jks -storepass *** -keystore xxx.jks 查看 JKS 生成的证书详细信息1keytool -list -v -keystore xxx.jks 导出 cer 证书1keytool -alias *** -exportcert -keystore xxx.jks -file xxx.cer 导出公钥, 此处命令需要 OpenSSL , 并配置环境变量1keytool -list -rfc --keystore xxx.jks | openssl x509 -inform pem -pubkey 拷贝出公钥","categories":[],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://logerjava.github.io/tags/Tools/"}]},{"title":"Linux 安装 JDK","slug":"Linux-安装-JDK","date":"2022-11-09T05:56:00.000Z","updated":"2023-02-14T08:42:37.307Z","comments":true,"path":"2022/11/09/Linux-安装-JDK/","link":"","permalink":"http://logerjava.github.io/2022/11/09/Linux-%E5%AE%89%E8%A3%85-JDK/","excerpt":"","text":"下载 tar.gz 包 12解压到指定位置tar -zxvf jdk.tar.gz -C /目录 12配置环境变量vim /etc/profile 12345export JAVA_HOME=/jdk/jdk1.8.0_311export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$CLASSPATHexport JAVA_PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/binexport PATH=$PATH:$&#123;JAVA_PATH&#125; 12让配置文件立即生效, 不行就重启source /etc/profile javac, java -verison 测试是否成功 echo $PATH 查看环境变量是否正确","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"}]},{"title":"Linux 集群部署 Nacos","slug":"Linux-集群部署-Nacos","date":"2022-11-09T05:55:13.000Z","updated":"2023-02-14T08:42:50.088Z","comments":true,"path":"2022/11/09/Linux-集群部署-Nacos/","link":"","permalink":"http://logerjava.github.io/2022/11/09/Linux-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-Nacos/","excerpt":"","text":"在 nacos.io 下载 tar.gz 包, 导入 linux 解压 因为 nacos 集群部署, 各个节点配置信息需要一致, 所以采取 MySQL 持久化 找到 &#x2F;conf 下的 nacos-mysql.sql 文件, 在 MySQL 执行 修改 application.properties 文件, 添加如下配置 : 1234567spring.datasource.platform=mysqldb.num=1db.url.0=jdbc:mysql://10.1.30.114:3306/nacos_config?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user.0=rootdb.password.0=1qaz@WSX 在 &#x2F;conf 下 的 cluster.conf 文件, 添加节点 ip 和 port, 例如: 12310.1.30.111:884810.1.30.112:884810.1.30.113:8848 启动 nacos 1sh startup.sh 如果出现 oom 问题, 编辑 startup.sh, 调整 jvm 内存 : 1-server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=64m -XX:MaxMetaspaceSize=128m 如果出现 12Nacos Server did not start because dumpservice bean construction failure :No DataSource set 原因可能是因为 MySQL 没有给当前 ip 开放 1grant all privileges on *.* to root@&quot;xxx.xxx.xxx.xxx&quot; identified by &quot;1qaz@WSX&quot;; 如果在浏览器无法访问, 可能原因是端口未在防火墙开放 1iptables -I INPUT -p tcp --dport 8848 -j ACCEPT","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"},{"name":"Nacos","slug":"Nacos","permalink":"http://logerjava.github.io/tags/Nacos/"}]},{"title":"Linux 安装 MySQL","slug":"Linux-安装-MySQL","date":"2022-11-09T05:54:14.000Z","updated":"2023-02-14T08:42:44.018Z","comments":true,"path":"2022/11/09/Linux-安装-MySQL/","link":"","permalink":"http://logerjava.github.io/2022/11/09/Linux-%E5%AE%89%E8%A3%85-MySQL/","excerpt":"","text":"安装 Mysql执行脚本12345678910安装 MySQLcurl https://webfile.newbanker.cn/mysql/install_centos7.sh -q | bash -s记录临时密码登录mysql修改mysql密码 mysql -u root -p输入数据安装后显示的密码set password for root@&#x27;localhost&#x27;=password(&#x27;1qaz@WSX&#x27;); 可能会遇到得问题SELinux linux服务器的安全策略问题1Can&#x27;t create test file /data/mysql/test-mysql.lower-test 安全策略问题, 可临时关闭 1setenforce 0 永久关闭需要修改配置文件，重启机器： 修改&#x2F;etc&#x2F;selinux&#x2F;config 文件 将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled MySQL 不允许远程连接可能导致的原因 : 网络不通 服务未启动 1service mysqld start; 防火墙端口未开放 12345查看网络端口信息netstat -ntp查看防火墙状态,查看3306端口iptables -vnL 如果3306如下，是drop状态，或者根本无3306端口，说明3306端口设置问题 12添加需要监听的端口/sbin/iptables -I INPUT -p tcp --dport 3306 -j ACCEPT MySQL 没有允许远程登陆 12345678root 权限登录mysql -u root -p输入use mysql;查看是否只有 localhost 主机select user,host from user; 如果只有 localhost 主机, 那么把需要远程连接的添加到这里 1grant all privileges on *.* to root@&quot;xxx.xxx.xxx.xxx&quot; identified by &quot;1qaz@WSX&quot;; 1flush privileges;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"},{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"ShardingSphere-JDBC 读写分离","slug":"ShardingSphere-JDBC-读写分离","date":"2022-11-09T05:42:40.000Z","updated":"2023-02-14T08:43:36.787Z","comments":true,"path":"2022/11/09/ShardingSphere-JDBC-读写分离/","link":"","permalink":"http://logerjava.github.io/2022/11/09/ShardingSphere-JDBC-%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/","excerpt":"","text":"为什么要读写分离 ?随着我们系统的业务量扩展, 原有的单机 MySQL 肯定会发生 I&#x2F;O 频率过高等问题, 导致损失性能, 采用主从复制, 读写分离可以提高数据库的可用性, 以及利用率 实现方式读写分离有很多种实现方式, 比如 AOP 的方式通过方法名判断是读操作还是写操作, 进而使用 master 或 slave , 但是本着不重复造轮子的原则, 以及现有框架成熟度很高我们采取 Apache 的 ShardingSphere-JDBC 框架, 该框架不仅可以实现读写分离, 还有很多其他便利功能, 这里仅对读写分离进行简单讲解 ShardingSphere-JDBC 官方文档 - https://shardingsphere.apache.org/document/current/cn/overview/ 示例项目项目配置pom 文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.12.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.76&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.velocity&lt;/groupId&gt; &lt;artifactId&gt;velocity-engine-core&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; application.yml这里采用一主一从 12345678910111213141516171819202122232425spring: shardingsphere: datasource: names: master,slave master: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://10.1.30.114:3306/test-db?useUnicode=true&amp;characterEncoding=utf8&amp;tinyInt1isBit=false&amp;useSSL=false&amp;serverTimezone=GMT username: root password: 1qaz@WSX slave: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://10.1.30.113:3306/test-db?useUnicode=true&amp;characterEncoding=utf8&amp;tinyInt1isBit=false&amp;useSSL=false&amp;serverTimezone=GMT username: root password: 1qaz@WSX props: sql.show: true masterslave: load-balance-algorithm-type: round_robin sharding: master-slave-rules: master: master-data-source-name: master slave-data-source-names: slave 启动启动看到如下提示则代表配置成功 读写接口测试我们编写两个简单的读写接口 使用 postman 请求访问, 可以看到 insert 走的是 master, select 走的是 slave","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"MySQL Explain","slug":"MySQL-Explain","date":"2022-11-09T05:37:44.000Z","updated":"2023-02-14T08:43:03.791Z","comments":true,"path":"2022/11/09/MySQL-Explain/","link":"","permalink":"http://logerjava.github.io/2022/11/09/MySQL-Explain/","excerpt":"","text":"关于 ExplainExplain 查询字段的含义 字段 含义 id 该语句的唯一标识 select_type 查询类型 table 表名 type 联接类型 possible_keys 可能的索引选择 key 实际选择的索引 key_len 索引的长度 ref 索引的哪一列被引用了 rows 估计要扫描的行 Extra 附加信息 id该语句的唯一标识, 如果 explain 的结果包括多个 id 的值, 则数字越大越先执行; 对于相同 id 的行, 则表示从上向下依次执行 select_type查询类型, 具体如下表 : 查询类型 作用 SIMPLE 简单查询(未使用 UNION 或子查询) PRIMARY 最外层查询 UNION 在 UNION 中的第二个和随后的 SELECT 被标记为 UNION DEPENDENT UNION UNION 中的第二个或后面的查询, 依赖了外面的查询 UNION RESULT UNION 的结果 SUBQUERY 子查询中的第一个 SELECT DEPENDENT SUBQUERY 子查询中的第一个 SELECT , 依赖了外面的查询 DERIVED 用来表示包含在 FROM 子句的子查询中的 SELECT , MySQL 会递归执行并将结果放到一个临时表中 (MySQL 内部将其称为 Derived table 派生表, 因为该表是从子查询中派生出来的) DEPENDENT DERIVED 派生表, 依赖了其他的表 MATERIALIZED 物化子查询 UNCACHEABLE SUBQUERY 子查询, 结果无法缓存, 必须针对外部查询的每一行重新评估 UNCACHEABLE UNION UNION 属于 UNCACHEABLE SUBQUERY 的第二个或后面的查询 table表示当前这一行正在访问哪张表, 如果 SQL 定义了别名, 则展示表的别名 type联接类型, 取值如下 (性能由好到坏排序) : system : 该表只有一行(相当于系统表), system 是 const 类型的特例 const : 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据; const 查询速度非常快, 因为仅仅读取一次即可 eq_ref : 当使用了索引的全部组成部分, 并且索引是 PRIMARY KEY 或 UNIQUE NOT NULL 才会使用该类型, 性能仅次于 system 和 const ref : 当满足索引的最左前缀规则, 或者索引不是主键也不是唯一索引时才会发生, 如果使用的索引只会匹配到少量的行, 性能也是不错的 tips : 最左前缀原则, 指索引按最左优先的方式匹配索引 fulltext : 全文索引 ref_or_null : 该类型类似 ref , 但是 MySQL 会额外搜索哪些行包含了 null, 常见于解析子查询 index_merge : 表示使用索引合并优化, 表示一个查询里面用到了多个索引 unique_subquery : 类似 eq_ref , 但是使用了 IN 查询, 且子查询是主键或者唯一索引 index_subquery : 和 unique_subquery 类似, 只是子查询使用的是唯一索引 range : 范围扫描, 表示检索了指定范围的行, 主要用于有限制的索引扫描 index : 全索引扫描, 和 ALL 类似, 只不过 index 是全盘扫描了索引的数据. 当查询仅使用索引中的一部分时, 可使用此类型, 有两种情况会触发 : 如果索引是查询的覆盖索引, 并且索引查询的数据就可以满足查询中所需的所有数据,则只扫描索引树. 此时, explain 的 Extra 列的结果是 Using index. index 通常比 ALL 快, 因为索引的大小通常小于表数据 按索引的顺序来查找数据行, 执行了全表扫描. 此时, explain 的 Extra 列的结果不会出现 Uses index ALL : 全表扫描, 性能最差 possible_keys展示当前查询可以使用那些索引, 这一列的数据是在优化过程的早期创建的, 因此有些索引可能对于后续优化过程是没用的 key表示 MySQL 实际选择的索引 key_len索引使用的字节数, 由于存储格式, 当字段允许为 NULL 时, key_len 比不允许为空时大 1 字节关于 key_len 的计算 : key_len 计算 ref表示将哪个字段或常量和 key 列所使用的字段进行比较 如果 ref 是一个函数, 则使用的值是函数的结果, 如果想查看是哪个函数, 可以在 EXPLAIN 语句后添加 SHOW WARNING 语句 rowsMySQL 估算会扫描的行数, 数值越小越好 Extra主要包括 Using filesort 、Using temporary 、Using index、Using where、Using join buffer、impossible where、select tables optimized away、distinct Using filesort : 说明 MySQL 会对数据使用一个外部的索引排序, 而不是按照表内的索引顺序进行读取; MySQL 中无法利用索引完成的排序操作称为 “文件排序” Using temporary : 使用了临时表保存中间结果, MySQL 在对查询结果排序时使用临时表; 常见于排序 order by 和分组 group by Using index : 表示相应 select 操作中使用了覆盖索引, 避免回表; 如果同时出现 Using where, 表明索引被用来执行索引键值的查找; 如果没有出现 Using where, 表明索引只是用来读取数据而非利用索引执行查找 Using where : 表明使用 where 过滤 Using join buffer : 表明使用了连接缓存 impossible where : where 的子句值总是 false select tables optimized away : 在没有 group by 子句的情况下, 基于索引优化 min&#x2F;max 操作或者对于 MyIsam 引擎, 优化 count(*) 操作, 不必等到执行阶段进行计算, 直接查询执行计划生成的阶段完成优化 distinct : 优化 distinct 操作, 在找到第一匹配的元组后即停止找同样值的动作 ……","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"MySQL 关于时间的一些思考","slug":"MySQL-时间的一些思考","date":"2022-11-09T05:36:42.000Z","updated":"2023-02-14T08:43:15.029Z","comments":true,"path":"2022/11/09/MySQL-时间的一些思考/","link":"","permalink":"http://logerjava.github.io/2022/11/09/MySQL-%E6%97%B6%E9%97%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/","excerpt":"","text":"关于 DateTime 和 Timestamp从时区方面考虑DateTime 没有时区信息, DateTime 在保存时保存的是当前会话所设置的时区对应的时间, 当时区更换会导致数据库读取时间出错Timestamp 存在时区信息, Timestamp 会跟随服务器的时区变化而变化, 自动换算成对应时间, 不同时区查询的时间是不同的 从占用空间, 时间范围方面考虑DateTime 耗费的空间更大, Timestamp 占用 4 个字节的存储空间, DateTime 占用 8 个字节的存储空间, 因此 Timestamp 表示的时间范围更小 DateTime : 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 Timestamp : 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 不要使用字符串存储时间使用字符串存储时间占用的空间更大, 效率较低(需要逐个字符对比), 无法使用相关函数进行计算和比较 不建议使用 int 和 bigint 表示时间此种存储方式拥有 Timestamp 类型具有的优点, 并且使用 int 和 bigint 进行日期排序和对比操作会更有效率, 跨系统也没有什么问题, 但是可读性很差, 无法看到具体时间 总结综上所述, 是关于 MySQL 中时间的一些思考, 可以看出关于 MySQL 的时间选择实际没有一种特定的最优解, 根据不同的业务场景应选择最适合的存储方法, 下面是各种类型的对比 : 日期类型 存储空间 日期格式 日期范围 是否存在时区问题 DateTime 8 字节 YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 是 Timestamp 4 字节 YYYY-MM-DD HH:MM:SS 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 否 时间戳 4 字节 全数字 1970-01-01 00:00:01 之后的时间 否","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"MySQL 高性能优化","slug":"MySQL-高性能优化","date":"2022-11-09T05:35:31.000Z","updated":"2023-02-14T08:43:09.498Z","comments":true,"path":"2022/11/09/MySQL-高性能优化/","link":"","permalink":"http://logerjava.github.io/2022/11/09/MySQL-%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","excerpt":"","text":"数据库命令规范 数据库对象名称使用小写字母, 下划线分割 数据库对象名称禁止使用 MySQL 关键字 数据库对象名称做到见名识意, 不要超过 32 字符 临时库表以 tmp_ 前缀, 日期为后缀; 备份表以 bak_ 为前缀, 日期为后缀 存储相同数据的列名和列类型必须一致 数据库基本设计规范1. 所有表均使用 Innodb 存储引擎在没有特殊需求的情况下(即 Innodb 无法满足的功能), 所有表必须使用 Innodb 存储引擎 2. 数据库和表的字符集统一使用 UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储emoji表情的需要，字符集需要采用utf8mb4字符集 3. 所有表和字段都需要添加注释4. 尽量控制单表数据量大小在 500 万以内单表数据量过大, 在修改表结构, 进行表备份, 恢复等操作时会出现问题, 可以通过分库分表手段控制表大小 5. 谨慎使用分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表, 谨慎选择分区键，跨分区查询效率可能更低, 建议采用物理分表的方式管理大数据 6. 条件允许尽量冷热数据分离, 减小表宽度MySQL 限制单表最多存储 4096 列, 并且每一行数据的大小不能超过 65535 字节减少磁盘 IO, 表越宽, 将表加载到内存缓冲池时所占用的内存也就越大, 会消耗更多的 IO , 保证热数据的内存缓存命中率, 更有效的利用缓存, 避免读入无用的冷数据, 经常一起使用的列可以放到一个表中(避免过多的关联操作) 7. 禁止在表中建立预留字段预留字段的命名很难做到见名识义, 预留字段无法确认存储的数据类型，所以无法选择合适的类型, 对预留字段类型的修改，会对表进行锁定 8. 禁止在数据库中存储图片, 文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时,通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在生产库做压力测试10. 禁止在开发, 测试环境直接连接生产环境数据库数据库字段设计规范1. 优先选择符合存储需要的最小的数据类型列的字段越大, 建立索引时所需要的空间也就越大, 这样一页中所能存储的索引节点的数量也就越少, 在遍历时需要的 IO 次数也就越多, 索引性能也就越差 example : 将字符串转换为数字类型存储(ip 地址转换为整型数据) MySQL 提供了两个方法来处理 ip 地址 inet_aton 把ip转为无符号整型(4-8位) inet_ntoa 把整型的ip转为地址 插入数据前，先用inet_aton把ip地址转为整型，可以节省空间, 显示数据时，使用inet_ntoa把整型的ip地址转为地址显示即可 对于非负型的数据（如自增 id、整型 ip）来说，要优先使用无符号整型来存储 因为, 无符号相对于有符号可以多出一倍的存储空间 SIGNED INT -21474836482147483647 UNSIGNED INT 04294967295 VARCHAR(N)中的N代表的是字符数，而不是字节数, 使用 UTF8 存储 255 个汉字, Varchar(255)&#x3D;765个字节, 过大的长度会消耗更多的内存 2. 避免使用 TEXT, BLOB 数据类型, 最常见的 TEXT 类型可以存储 64k 的数据建议将 BLOB 或 TEXT 列分离到单独的扩展表中 :MySQL 内存临时表不支持 TEXT, BLOB 这样的大数据类型, 如果查询中包含这样的数据, 在排序等操作时, 就不能使用内存临时表, 必须使用磁盘临时表进行, 而且对于这种数据, MySQL 还需要二次查询, 会使 SQL 性能变的很差, 如果一定要使用, 建议将 TEXT, BLOB 放到单独的扩展表, 查询时必要使用 select * , 而是查询指定列, 不需要 TEXT 时不要查询 TEXT 或 BLOB 类型只能使用前缀索引 :因为MySQL对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的 3. 避免使用 ENUM 类型修改 ENUM 类型需要使用 ALTER 语句, 并且 ENUM 类型的 ORDER BY 操作效率低, 需要额外操作 4. 尽可能将所有列定义为 NOT NULL索引 NULL 列需要额外的空间来保存, 所以要占用更多的空间, 进行比较和计算时都要对 NULL 值进行特别处理 5. 使用 Timestamp 或 DateTime 存储时间MySQL - 关于时间问题的一些思考 6. 同财务相关的金额类数据必须使用 decimal 类型float, double 为非精准浮点, decimal 是精准浮点, decimal 在计算时不会丢失精度, 占用空间由定义宽度决定, 每 4 个字节可以存储 9 位数字(小数点要占用 1 字节), 可存储比 bigint 更大的整型数据 索引设计规范1. 限制每张表的索引数量, 建议的单张表不超过 5 个索引并不是越多越好, 我们知道索引可以增加查询效率, 但是如果使用存在问题索引会降低写入的效率, 有些情况也会降低查询效率 MySQL 优化器在选择如何优化查询时, 会根据统一信息, 对每一个可以用到的索引进行评估,生成一个最好的执行计划, 如果同时有很多个索引都可以用于查询, 就会增加 MySQL 优化器生成执行计划的时间, 降低查询性能 2. 禁止给表中的每一列都建立单独的索引5.6 版本之前，一个 SQL 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好 3. Innodb 表必须存在主键Innodb是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的每个表都可以有多个索引，但是表的存储顺序只能有一种Innodb是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引）不要使用 UUID,MD5,HASH, 字符串列作为主键（无法保证数据的顺序增长）主键建议使用自增ID值 索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 多表 JOIN 的关联列 条件合适的情况下建立联合索引, 避免每个单独列都建立索引 索引顺序问题索引建立的目的是 : 通过索引进行数据查找, 减少随机 IO, 增加查询性能, 索引能过滤出越少的数据则从磁盘中读取的数据也就越少 区分度最高的放在联合索引的最左侧（区分度&#x3D;列中不同值的数量&#x2F;列的总行数） 尽量将字段长度小的列放在联合索引的最左侧（字段长度越小，一页能存储的数据量越大，IO性能也就越好） 使用最频繁的列放到联合索引最左侧（较少的建立一些索引） 避免建立冗余索引和重复索引原因 : 增加查询优化器生成执行计划的时间 重复索引示例：primary key(id)、index(id)、unique index(id)冗余索引示例：index(a,b,c)、index(a,b)、index(a) 对于频繁的查询优先考虑使用覆盖索引原因 : 避免 Innodb 表进行索引的二次查询 可以将随机 IO 变为顺序 IO 加快查询速度 详情 : MySQL - 索引机制 数据库 SQL 开发规范1. 建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题, 只传参数，比传递 SQL 语句更高效, 相同语句可以一次解析，多次使用，提高处理效率 2. 避免数据类型隐式转换隐式转换会导致索引失效, 在单次查询数据很多的情况下, 若查询列隐式转换将会降低效率 3. 充份利用已建立的索引example : 避免使用双 % 的查询条件 如 name like %loger% , 若无前置 % 只有后置 % , 是可以用到列上的索引的 一个 SQL 只能利用到复合索引中的一列进行查询 如有 a, b, c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b, c 列上的索引将不会被用到，在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧 使用 LEFT JOIN 或 NOT EXISTS 来优化 NOT IN 操作 NOT INT 会导致索引失效 4. 数据库设计时, 需考虑后续扩展情况5. 程序连接不同数据库使用不同账号, 进行跨库查询为数据库迁移和分库分表留出余地, 降低业务耦合度, 避免权限过大而产生的安全风险 6. 禁止使用 SELECT *消耗更多的 CPU 和 IO 以网络带宽资源, 无法使用覆盖索引, 可以减少表结构变更带来的影响 7. 禁止使用不含字段列表的 INSERT 语句如： insert into values (‘a’,’b’,’c’);应使用 insert into t(c1,c2,c3) values (‘a’,’b’,’c’); 8. 避免使用子查询, 可以将子查询优化为 JOIN 操作通常子查询在 IN 子句中, 且子查询为简单 SQL (不包含 union、group by、order by、limit 从句) 时, 才可以将子查询转化为关联查询进行优化 子查询性能差的原因 :子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响, 特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大, 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的CPU和IO资源，产生大量的慢查询 9. 避免使用 JOIN 关联太多表对于Mysql来说，是存在关联缓存的，缓存的大小可以由join_buffer_size参数进行设置在Mysql中，对于同一个SQL多关联（join）一个表，就会多分配一个关联缓存，如果在一个SQL中关联的表越多，所占用的内存也就越大 如果程序中大量的使用了多表关联的操作，同时join_buffer_size设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性 同时对于关联操作来说，会产生临时表操作，影响查询效率Mysql最多允许关联61个表，建议不超过5个 10. 减少同数据库交互次数数据库更适合处理批量操作, 合并多个相同的操作在一起, 可以提高处理效率 11. 对应同一列进行 OR 判断时, 使用 IN 替代 ORIN 的值不要超过 500 个IN 操作可以更有效的利用索引，OR 大多数情况下很少能利用到索引 12. 禁止使用 ORDER BY RAND() 进行随机排序会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的CPU和IO及内存资源 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式 13. WHERE 从句中禁止对列进行函数转换和计算会导致索引失效 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNIONUNION 会把两个结果集的所有数据放到临时表中后再进行去重操作UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大 SQL 为多个小 SQL大SQL:逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL, MySQL 一个 SQL 只能使用一个 CPU 进行计算, SQL 拆分后可以通过并行执行来提高处理效率 数据库操作行为规范超 100 万的批量写操作, 要分批次进行操作 大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量日志 大批量写操作会产生大量日志，特别是对于row格式二进制数据而言，由于在row格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对MySQL的性能产生非常大的影响, 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 对于大表使用 pt-online-schema-change 修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的 pt-online-schema-change它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器, 把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉, 把原来一个 DDL 操作，分解成多个小的批次进行","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"Java 泛型详解","slug":"Java-泛型详解","date":"2022-11-08T08:23:12.000Z","updated":"2023-02-14T08:42:26.474Z","comments":true,"path":"2022/11/08/Java-泛型详解/","link":"","permalink":"http://logerjava.github.io/2022/11/08/Java-%E6%B3%9B%E5%9E%8B%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"概述泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用&#x2F;调用时传入具体的类型（类型实参）。 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 举例12345678List list = new ArrayList();list.add(&quot;loger&quot;);list.add(100);for(int i = 0; i&lt; list.size();i++)&#123; String str = (String)list.get(i); log.info(&quot;泛型测试&quot;,&quot;str = &quot; + str);&#125; 运行结果 1java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String ArrayList 中可以存放任意类型, 当上述情况, 其中即存在 Integer 又存在 String, 且都以 String 的方式使用时, 程序就会报错, 泛型可以解决此类问题 1List&lt;String&gt; list = new ArrayList&lt;String&gt;(); 声明带泛型的集合, 在集合内类型不匹配时, 会直接报错 特性泛型只在编辑阶段有效 123456789List&lt;String&gt; stringArrayList = new ArrayList&lt;String&gt;();List&lt;Integer&gt; integerArrayList = new ArrayList&lt;Integer&gt;();Class classStringArrayList = stringArrayList.getClass();Class classIntegerArrayList = integerArrayList.getClass();if(classStringArrayList.equals(classIntegerArrayList))&#123; log.info(&quot;类型相同&quot;);&#125; 输出结果：类型相同 通过上面的例子可以证明，在编译之后程序会采取去泛型化的措施。也就是说 Java 中的泛型，只在编译阶段有效。在编译过程中，正确检验泛型结果后，会将泛型的相关信息擦出，并且在对象进入和离开方法的边界处添加类型检查和类型转换的方法。也就是说，泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 泛型的使用泛型类1234567891011121314// 此处 T 可以使用任意表示, T、E、K、V 均可// 实例化时必须指定 T 的具体类型public class Test&lt;T&gt; &#123; // T 的类型为外部指定 private T key; public Test(T key) &#123; this.key = key; &#125; public T getKey() &#123; return key; &#125;&#125; 传入实参的类型与泛型相同, 如不做泛型限制, 则会根据传入实参做相应限制 123456// 泛型限制Test&lt;String&gt; stringTest = new Test&lt;&gt;(&quot;loger&quot;);Test&lt;Integer&gt; integerTest = new Test&lt;&gt;(333);// 非限制Test test = new Test&lt;&gt;(33.33); 泛型接口1234// 定义一个泛型接口public interface Test&lt;T&gt; &#123; public T next(); &#125; 当实现泛型接口未传入实参时 1234567891011/** * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中 * 即：class TestImpl&lt;T&gt; implements Test&lt;T&gt;&#123; * 如果不声明泛型，如：class TestImpl implements Test&lt;T&gt;，编译器会报错：&quot;Unknown class&quot; */class TestImpl&lt;T&gt; implements Test&lt;T&gt;&#123; @Override public T next() &#123; return null; &#125; &#125; 当实现泛型接口的类，传入泛型实参时 1234567891011121314151617/** * 传入泛型实参时： * 定义一个生产器实现这个接口,虽然我们只创建了一个泛型接口 Test&lt;T&gt; * 但是我们可以为 T 传入无数个实参，形成无数种类型的 Test 接口 * 在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型 * 即：TestImpl&lt;T&gt;，public T next();中的的T都要替换成传入的 String 类型 */public class TestImpl implements Test&lt;String&gt; &#123; private String[] fruits = new String[]&#123;&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;&#125;; @Override public String next() &#123; Random rand = new Random(); return fruits[rand.nextInt(3)]; &#125;&#125; 泛型通配符试想一个问题 Integer 是 Number 的一个子类, 而 Test&lt;Integer&gt; 和 Test&lt;Number&gt; 实际上是相同的基本类型, 那么 Test&lt;Number&gt; 作为形参的方法中, 能否使用 Test&lt;Ingeter&gt; 实例传入呢 ? 123public void showValue(Test&lt;Number&gt; arg) &#123; log.info(arg.getKey()); &#125; 1234Test&lt;Integer&gt; integerTest = new Test&lt;&gt;(123);Test&lt;Number&gt; numberTest = new Test&lt;&gt;(456);showValue(integerTest); 很明显编译的时候就报错了 Test&lt;java.lang.Integer&gt; cannot be applied to Test&lt;java.lang.Number&gt; 由此可以看出: 同一种泛型可以对应多个版本（因为参数类型是不确定的），不同版本的泛型类实例是不兼容的 在处理上述问题时, 可以将泛型替换为 ? 123public void showValue(Test&lt;?&gt; arg) &#123; log.info(arg.getKey()); &#125; 注意, 此处 ? 代表的是类型实参, 而非形参, 换一种说话就是可以把 ? 看成所有类的父类, 它可以解决当具体类型不确定时, ? 即是通配符 泛型方法12345678910111213/** * 泛型方法的基本介绍 * @param clazz 传入的泛型实参 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间 &lt;T&gt; 非常重要, 可以理解为声明此方法为泛型方法 * 2）只有声明了 &lt;T&gt; 的方法才是泛型方法, 泛型类中的使用了泛型的成员方法并不是泛型方法 * 3）&lt;T&gt; 表明该方法将使用泛型类型 T , 此时才可以在方法中使用泛型类型 T * 4）与泛型类的定义一样, 此处 T 可以随便写为任意标识, 常见的如 T、E、K、V 等形式的参数常用于表示泛型 */public &lt;T&gt; T test(Class&lt;T&gt; clazz) throws Exception&#123; return clazz.newInstance();&#125; 泛型上下边界在使用泛型的时候，我们还可以为传入的泛型类型实参进行上下边界的限制，如：类型实参只准传入某种类型的父类或某种类型的子类 123public void showVale(Test&lt;? extends Number&gt; test)&#123; log.info(obj.getKey());&#125; 这样就规定了泛型的上边界, 传入的类型实参必须是指定类型的子类型 1234Test&lt;String&gt; test1 = new Test&lt;String&gt;(&quot;11111&quot;);Test&lt;Integer&gt; test2 = new Test&lt;Integer&gt;(2222);Test&lt;Float&gt; test3 = new Test&lt;Float&gt;(2.4f);Test&lt;Double&gt; test4 = new Test&lt;Double&gt;(2.56); 在编译时, test1 会报错, 因为 String 并不是 Number 的子类型","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"MySQL 主从复制","slug":"MySQL 主从复制","date":"2022-11-08T05:32:44.000Z","updated":"2023-02-14T08:43:27.286Z","comments":true,"path":"2022/11/08/MySQL 主从复制/","link":"","permalink":"http://logerjava.github.io/2022/11/08/MySQL%20%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","excerpt":"","text":"主从复制原理 当 Master 节点进行 insert、update、delete 操作时，会按顺序写入到 binlog 中 salve 从库连接 master 主库，Master 有多少个 slave 就会创建多少个 binlog dump 线程 当 Master 节点的 binlog 发生变化时，binlog dump 线程会通知所有的 salve 节点，并将相应的 binlog 内容推送给 slave 节点 I&#x2F;O 线程接收到 binlog 内容后，将内容写入到本地的 relay-log SQL 线程读取 I&#x2F;O 线程写入的 relay-log，并且根据 relay-log 的内容对从数据库做对应的操作 这里有很重要的两个问题 : 从库同步主库数据的过程是串行化的, 也就是说主库上并行的操作, 在从库上会串行化执行, 由于从库从主库拷贝日志以及串行执行 SQL 的特点, 在高并发场景下, 从库的数据势必会比主库慢, 存在延迟, 所以经常出现刚写入主库的数据读不到的情况 如果主库突然宕机, 此时数据还没有同步到从库, 那么有些数据从库上是没有的, 会出现数据丢失情况 MySQL 存在两个机制解决上面的问题 : 半同步复制 : 主要解决主库数据丢失问题, 也叫做 semi-sync 复制, 指主库写入 binlog 日志之后, 就会强制将此时的数据立即同步到从库, 从库将日志写入自己本地的 relay log 之后, 返回一个 ack 给主库, 主库接收到至少一个从库的 ack 才会认为写操作完成了 并行复制 : 主要解决同步延时问题, 指从库开启多线程, 并行读取 relay log 中不同库的日志, 然后并行重放不同库的日志, 这是库级别的并行 如何实现主从复制这里举例是一主一从 Master登录 MySQL 1mysql -u root -p 创建用户 1234# 10.1.30.113 是 slave 从机的 IPGRANT REPLICATION SLAVE ON *.* to &#x27;root&#x27;@&#x27;10.1.30.113&#x27; identified by &#x27;1qaz@WSX&#x27;;# 刷新系统权限表的配置FLUSH PRIVILEGES; 在 etc&#x2F;my.cnf 增加以下配置 12345678# 开启binloglog-bin=mysql-binserver-id=114# 需要同步的数据库，如果不配置则同步全部数据库binlog-do-db=test_db# binlog日志保留的天数，清除超过10天的日志# 防止日志文件过大，导致磁盘空间不足expire-logs-days=10 重启 MySQL 1service mysql restart 通过下方命令查看当前 binlog 日志信息 1show master status\\G; Slave在 etc&#x2F;my.cnf 增加以下配置 12# 不要和其他mysql服务id重复即可server-id=114 登录 MySQL 1mysql -u root -p 输入以下命令 : MASTER_HOST : 主机 IP MASTER_USER: 之前创建的用户账号 MASTER_PASSWORD : 之前创建的用户密码 MASTER_LOG_FILE : master 主机的 binlog 日志名称 MASTER_LOG_POS : binlog 日志偏移量 master_port : 端口 1CHANGE MASTER TO MASTER_HOST=&#x27;10.1.30.114&#x27;,MASTER_USER=&#x27;root&#x27;,MASTER_PASSWORD=&#x27;1qaz@WSX&#x27;,MASTER_LOG_FILE=&#x27;mysql-bin.000007&#x27;,MASTER_LOG_POS=862,master_port=3306; 重新启动 1start slave; 启动下方命令校验 1show slave status\\G; 判断同步成功方式 : 首先 Master_Log_File 和 Relay_Master_Log_File 所指向的文件必须一致 其次 Relay_Log_Pos 和 Exec_Master_Log_Pos 的为止也要一致才行 测试12345678910CREATE TABLE `tb_role` ( `role_id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;角色id&#x27;, `role_name` varchar(30) DEFAULT NULL COMMENT &#x27;角色名称&#x27;, `state` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;1启用0停用,默认0&#x27;, `create_by` varchar(40) DEFAULT NULL COMMENT &#x27;创建人&#x27;, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, `remark` varchar(100) DEFAULT NULL COMMENT &#x27;备注&#x27;, PRIMARY KEY (`role_id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; 在 master 执行上述 SQL 同步到 slave , 表示配置成功 常见问题配置失败出现的问题常见于 Slave_IO_Running: No 或 Slave_SQL_Running: No , 通常是配置读取文件出现问题或事务回滚造成的主从问题, 由于问题很多不做赘述, 列举几个类似问题的博客 : https://blog.csdn.net/zzddada/article/details/113352717 https://blog.csdn.net/weixin_30657999/article/details/99613614 https://blog.csdn.net/lihuarongaini/article/details/101299375","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"为什么要使用消息队列, 优缺点, 各消息队列对比","slug":"whyMQ","date":"2022-11-07T07:53:14.000Z","updated":"2023-02-14T08:43:42.017Z","comments":true,"path":"2022/11/07/whyMQ/","link":"","permalink":"http://logerjava.github.io/2022/11/07/whyMQ/","excerpt":"","text":"为什么使用消息队列解耦 如上方场景, A 系统通过接口调用方式发送数据到 B, C, D 系统, 此时新增 E 系统也需要此数据该如何解决 ? 此时又新增了其他系统呢 ? B 系统在某个时间节点不需要 A 系统的数据了该如何解决 ? 在上方场景中 A 系统不仅和 B, C, D 等系统严重耦合在一起, 并且要时刻考虑其他系统的状态, 如果宕机是否要重新发送, 是否需要存储消息等, 负责人的讲 A 系统负责人会很痛苦 如果改用 MQ 方式处理, A 系统产生数据, 直接发送到 MQ 中, 其余需要数据的系统到 MQ 中消费, B 系统不需要则取消消费, 这种情况下 A 系统就摆脱了束缚, 无需考虑调用是否成功, 是否超时等问题, 如下图 异步 如上场景, 假设 A 系统接收到用户请求需要本地持久化数据, 过程为 3ms, 后 B, C, D 写库总计 3 + 300 + 450 + 200 &#x3D; 953ms, 总体接近 1s, 在一般的项目中我们要求基本上是请求响应基本上是对用户无感知的, 大概 200ms 以内完成, 以上情况很难接受 此时添加 MQ, A 系统发送三条消息到 MQ 中耗时 5ms, 总计 3 + 5 &#x3D; 8ms, 直接返回后续操作在后台完成 削峰考虑如下场景, 从早晨 0:00 开始到下午 13:00, 系统 A 每秒并发请求基本维持在 30 左右, 在 13:00 到 14:00 每秒请求激增到 5k+, 系统基于 MySQL 直连, 这时会有每秒 5k+ 的请求打入数据库 一般的 MySQL 很明显无法抗住这种请求级别, 2k 左右大概是极限, 很可能直接宕机, 用户也就无法继续操作系统, 但是经过高峰期后又再度恢复为每秒 30 的请求量 这个时候我们考虑接入 MQ 处理, 每秒 5k+ 的请求写入 MQ, 系统 A 每秒至多处理 2k 的请求, 那么就仅拉取 2k 的请求, 只要不超过处理极限就可以, 这样在最高峰值期间服务并不会挂掉, 每秒 5k 左右的请求进入 MQ, 2k 左右的请求被消费, 这样可能会导致几十万甚至百万的请求积压在 MQ 中, 但是短暂的积压是没有关系的, 经历过高峰期后只有每秒 30 的请求量, 但是系统还是在按照每秒 2k 左右的速度消费, 高峰期过后用不了多久就可以处理结束 消息队列的优缺点优点 : 解耦, 异步, 削峰 缺点 : 可用性降低 : 系统引入的外部依赖越多则可用性越低, 根据上面的场景, 本身是 A, B, C, D 四个系统的问题, 接入 MQ 后需要考虑 MQ 的维护问题, 如果 MQ 宕机则整套系统都将崩溃 复杂度提高 : 新增 MQ 后需要考虑消息幂等问题(是否重复), 消息丢失问题, 顺序等 一致性问题 : 在将消息发送到 MQ 后返回成功, 但是不一定真的全部成功, 有可能 B, C 写入成功而 D 却失败等问题, 会导致数据不一致 综上所属消息队列实际上并没有想象的那么简单, 引入消息队列确实可以带来好处, 但是也会衍生出另一些问题, 针对某些必要使用 MQ 的场景我们需要提前准备问题的解决方案, 难度系统直线上升, 但是关键时刻消息队列是起决定性作用的技术, 该用还是要用 ActiveMQ、RabbitMQ、RocketMQ、Kafka 对比 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百&#x2F;几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 以前很多人用 ActiveMQ , 但是现在用的很少, 并且社区不活跃, 不建议使用 RabbitMQ 社区很活跃, 但是 erlang 语言导致 RabbitMQ 处于基本不可控的状态, 也无法做到自定义 RocketMQ 来自阿里, 质量有保证, 毕竟有双 11 检验, 但是目前 RocketMQ 已经捐献给 Apache, 并且活跃度不是很高, 不过毕竟是 Java 写的可控性还是有的, 如果对公司技术自信的可以考虑 Kafka 一般适用于大数据领域, 日志采集, 实时计算","categories":[],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://logerjava.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]}],"categories":[],"tags":[{"name":"工作中遇到问题的总结","slug":"工作中遇到问题的总结","permalink":"http://logerjava.github.io/tags/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%BB%E7%BB%93/"},{"name":"Redis","slug":"Redis","permalink":"http://logerjava.github.io/tags/Redis/"},{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"},{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"Tools","slug":"Tools","permalink":"http://logerjava.github.io/tags/Tools/"},{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"},{"name":"Nacos","slug":"Nacos","permalink":"http://logerjava.github.io/tags/Nacos/"},{"name":"消息队列","slug":"消息队列","permalink":"http://logerjava.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]}