{"meta":{"title":"logerJava","subtitle":"","description":"","author":"loger","url":"http://logerJava.github.io","root":"/"},"pages":[{"title":"categories","date":"2018-09-30T09:25:30.000Z","updated":"2022-11-04T08:29:46.112Z","comments":true,"path":"categories/index.html","permalink":"http://logerjava.github.io/categories/index.html","excerpt":"","text":""},{"title":"关于","date":"2022-11-09T06:13:45.000Z","updated":"2022-11-09T06:23:30.896Z","comments":false,"path":"about/index.html","permalink":"http://logerjava.github.io/about/index.html","excerpt":"","text":"互联网砖工"},{"title":"tags","date":"2018-09-30T10:23:38.000Z","updated":"2022-11-04T08:31:26.145Z","comments":true,"path":"tags/index.html","permalink":"http://logerjava.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MySQL知识点总结","slug":"MySQL知识点总结","date":"2023-02-03T05:11:24.000Z","updated":"2023-02-07T09:07:58.651Z","comments":true,"path":"2023/02/03/MySQL知识点总结/","link":"","permalink":"http://logerjava.github.io/2023/02/03/MySQL%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"","text":"逻辑架构 如上图, 我们可以分为三层来进行解读 : 第一层的服务其实并不是 MySQL 独有的, 大多数的基于网络的客户端 &#x2F; 服务器的工具都有类似架构, 例如连接处理, 授权认证等 第二层是 MySQL 的核心服务, 大多数的 MySQL 功能都在这一层, 包括查询解析, 分析, 优化, 内置函数等, 所有跨存储引擎的功能也都在这一层实现 : 存储过程, 触发器, 视图等 第三层包含了存储引擎, 负责 MySQL 中数据的存储和提取, 不同的存储引擎具有不同的优势和劣势, 服务器则通过 API 与存储引擎进行通信, 这些 API 屏蔽了不同存储引擎的差异 索引B+Tree 原理数据结构B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。 在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 ps ：旧金山大学数据结构模拟工具 Data Structure Visualizations 操作进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。 插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。 与红黑树的比较红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能 1. B+ 树有更低的树高平衡树的树高 O(h)&#x3D;O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多 2. 磁盘访问原理操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I&#x2F;O 就能完全载入一个节点 如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取 3. 磁盘预读特性为了减少磁盘 I&#x2F;O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入 MySQL 索引索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现 B+ Tree 索引B+ Tree 索引是大多数 MySQL 存储引擎的默认索引类型 因为不需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组 可以指定多个列作为索引列，多个索引列共同组成键 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找；如果不是按照索引列的顺序进行查找，则无法使用索引 InnoDB 的 B+ Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引 辅助索引（二级索引）的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找 1. 聚簇索引聚簇索引在表建立时就已经建立了，举一个例子： 1SELECT * FROM student WHERE studentId = 6 要查询学生表中 id 为 6 的学员全部信息，当没有聚簇索引的情况下，也就是无序情况下，需要进行全表查询才能找到，此处可能会有疑问，既然主键是自增的那么不可以用二分查找解决吗 ？ 答案是不可以的，试想如果数据在写入磁盘时是 1、2、3、4、5、6 的形式，此时我删除了 4，那么如何将 5 向前移动呢 ？如果没有维护一个有序的数组结构，那么数据在磁盘就认为它是无序的，如果维护了一个有序的数组那么也是建立了索引，只不过换了一种数据结果 当创建了聚簇索引，其情况如下： 数据库里面有 1, 2, 3, 4, 5, 6 个学生 : 从上到下查找, 在根节点进行二分查找, 找到一个 studentId 所在的指针, 这里指向右侧 在叶子节点上进行二分查找, 找到 studentId &#x3D; 6 对应的 data 聚簇索引是无论如何都会创建的, 如果设定了主键, 则主键是聚簇索引, 如果没有设定主键会寻找唯一键作为索引, 就算你连唯一键都没有也没关系, MySQL 会建立一个 rowid 字段, 来完成 B+ Tree 2. 二级索引在查询时我们并不能总是知道主键，所以也会用姓名、手机号等作为查询条件，而在聚簇索引中并没有这两项，这就需要建立辅助索引（二级索引） 1CREATE INDEX idx_studentName ON student(studentName) 建立学生姓名索引会新创建一个 B+ Tree，里面存储学生姓名，data 存储着主键 id 1SELECT * FROM student WHERE studentName = &#x27;F&#x27; 当在二级索引中查询姓名为 F 的学员时： 二分查找, 找到对应指针 会在 B+ Tree 中找到 key 是 F 的记录 二级索引的 data 域中存储的是主键id, 那么就拿到主键 id 因为是查询全部字段, 所以用主键 id 到聚簇索引中进行查找 3. 复合索引复合索引同样属于二级索引的范畴，而又不太相同 1CREATE INDEX idx_studentName_age ON student(studentName,age) 当建立复合索引时，会再次新创建一个 B+ Tree 此时索引的 key 中存在有姓名和年龄，其他查询步骤与二级索引相同，需要注意的是如上图中，姓名 E 的 14 岁学员，姓名 E 的 15 岁学员，同名不同年龄，那么在排序比较时，会先按照姓名比较，如果姓名相同再按照年龄比较 哈希索引哈希索引可以以 O(1) 时间进行查找，但是失去了有序性： 无法用于排序与分组 只支持精确查找，无法用于部分查找和范围查找 InnoDB 存储引擎有一个特殊的功能叫 “自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+ Tree 索引之上再创建一个哈希索引，这样就可以让 B+ Tree 索引具有哈希索引的一些优点，比如更快的哈希查找 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。 查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引 空间数据索引MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。 空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据 日志redo logMySQL 是按页为单位来读取数据的，个页里面有很多行记录，从内存刷数据到磁盘，也是以页为单位来刷 1. redo log 的概念1UPDATE student SET name = &#x27;loger&#x27; WHERE studentId = 3 在数据库执行此SQL，MySQL 的操作如下： 首先判断内存中有没有 studentId &#x3D; 3 的数据 如果没有就去磁盘找到这条数据所在的页, 将整页数据加载到内存 然后找到 studentId &#x3D; 3 的行数据, 将内存中的 name 修改为 loger 我们可以发现数据存在不一致的情况，内存中的数据为正确的修改过后的新数据，磁盘中的数据为未修改的旧数据，此时磁盘对应的页的数据称为脏页 内存修改而磁盘未修改时掉电 MySQL 是怎么解决的 ？ 基于这种场景 MySQL 的解决方案是将对页的操作、修改内容，记录下来保存到磁盘，也就是 redo log，在 redo log 写入成功后，MySQL 就认为十五已经提交成功且数据已经持久化，并会在空闲时间将内存数据刷入磁盘；如果此时掉电，只需在重启后将脏页数据加载到内存中，然后利用 redo log 脏页就会修正了 2. redo log 如何存储 根据 MySQL 的官方文档可以大致分析处如下几点： redo log 记录了 SQL 语句以及其他 API 对表产生的变化, 也就是物理变化 redo log 存储在磁盘, 用于 crash recovery 后修正数据, 也就是处理宕机, 掉电等问题 redo log 默认有两个文件 redo log 采取循环写方式 (circular) 实际上 redo log，默认是在 ib_logfile0 和 ib_logfile1 循环来回写的 redo log 在写入磁盘时并不是随机 I&#x2F;O 而是顺序 I&#x2F;O 所以写入速度很快, 并且 redo log 文件体积又很小, 所以恢复速度很快 binlog1. binlog 的概念1UPDATE student SET name = &#x27;loger&#x27; WHERE studentId = 3 在执行这条 SQL 语句时，不仅仅生成了 redo log，还生成了 binlog binlog 记录了数据库表结构和表数据变更, 比如 insert, delete, update, create 等, 不会记录查询 不同于 redo log，binlog 是 MySQL 在 Server 层的功能，而 redo log 则是 InnoDB 存储引擎的功能，也就是说 redo log 只要使用了 InnoDB 作为存储引擎就会有，而 binlog 只要是使用 MySQL 就会有 2. binlog 的作用MySQL 既然将 binlog 放在了 Server 层，就代表 binlog 提供了通用的能力，binlog 有两个作用： 数据恢复 找到前一时间点的 binlog 进行重放就可以恢复数据 主从复制 master 将 binlog 发送给 slave，slave 执行 binlog 那么就复制了 对比 redo log 与 binlogredo log 和 binlog 都可以作为恢复手段，但其实他们的细节部分还是不一样的 1. 存储内容不同binlog 记录的是 insert、delete、update、create 等 SQL 语句，而 redo log 记录的是物理修改内容，可以理解为，binlog 记录的是逻辑变化，redo log 记录的是物理变化 2. 功能不同redo log 写入内存，如果数据库宕机，可以通过 redo log 恢复内存还没有刷盘的数据，也就是可以恢复宕机之前的内存数据 binlog 可以保持主从一致性，如果整个数据库都被删除了，binlog 存储着所有数据的变更情况，可以通回放 binlog 进行恢复 需要注意，如果整个数据库都被删除了，redo log 是无法恢复的，因为 redo log 并不会记录历史的所有数据，文件内容会被覆盖 两段式提交redo log 和 binlog 都会在执行 update 的时候写入，那么是怎么写入的呢 ？ 写入：redo log（prepare） 写入：binlog 写入：redo log（commit） 为什么写入 redo log 需要两段式提交而不是一次性写入呢 ？下面分为两种情况分析 先写入 redo log 再写入 binlog： 如果 redo log 写入成功，写入 binlog 失败，此时出现宕机情况需要恢复数据，主从复制情况下，master 采用 redo log 恢复数据，从库采用 binlog，但是 binlog 写入失败不存在 binlog，那么从库就没有这些数据，导致主从不一致 先写入 binlog 再写入 redo log： 与上面情况实际是相同的，binlog 存在而 redo log 不存在，导致从库是最新数据，而主库出现问题 两段式提交： 写入 redo log，若失败则回滚，不再继续写入 binlog 若 redo log 写入成功，binlog 写入失败则回滚，删除无效的 binlog 只有当 redo log 和 binlog 都写入成功此次事务才会提交 可以看出 MySQL 需要保证 redo log 和 binlog 是一致的 undo logundo log 主要负责回滚与多版本控制（MVCC） 跳转MVCC undo log 实际上存储的也是逻辑日志，在执行 update 时不仅记录 redo log 与 binlog，undo log 也会进行记录，比如用户进行插入操作，那么 undo log 就会记录一条删除操作；若用户将 A 修改为了 B，则 undo log 就会记录一条 B 修改为 A 的记录 其目的就是为了保证回滚，这些 undo log 存储的记录就相当于之前 SQL 前的一个版本，回滚时直接返回上一个版本 error log错误日志, 记录着 MySQL 启动, 运行期间, 停止时的错误相关信息, 默认情况下是关闭的 可以指定 errorlog 的输出路径 : 编辑 my.cnf 写入 log-error &#x3D; [path] 通过命令参数错误日志 mysqld_safe –user&#x3D;mysql –log-error&#x3D;[path] &amp; general query log普通查询日志, 记录了 MySQL 接收到的所有查询或命令操作, 无论是正确还是错误的都会进行记录, 因为记录的比较频繁, 产生开销较大所以默认是关闭的 slow query log慢查询日志记录的是执行时间超过 long_query_time 和没有使用索引的查询语句, 只记录成功的语句 相关参数 : slow_query_log : 1. 开启; 0. 关闭 long_query_time : 慢查询阈值 log_output : 输出方式 通过如下方式配置慢查询 : 123show variables like &#x27;%slow_query_log%&#x27;;set global slow_query_log = 1;show variables like &#x27;%slow_query_log%&#x27;; 要注意的是, 此处修改只对当前数据库生效, 在 MySQL 重启后会失效, 如果需要配置永久生效需要修改 my.cnf 文件 锁MySQL 中锁的分类因为不同的存储引擎支持的锁机制是不同的，以下内容仅就 MyISAM 与 InnoDB 进行解析 需要注意 MyISAM 存储引擎仅支持表锁, InnoDB存储引擎既支持行级锁，也支持表级锁，但默认情况下是采用行级锁 1. MyISAM 中的表锁MyISAM 中的表锁可以分为两种： 表共享锁（Table Read Lock）：不会阻塞其他用户对同一张表的读请求，但会阻塞对同一张表的写请求 表独占锁（Table Write Lock）：会阻塞其他用户对同一张表的读写操作 MyISAM 表的读写是串行操作，当一个线程获得一个表的写锁后，只有持有锁的线程才可以对表进行更新操作，其他线程的读写操作都会阻塞，直到锁被释放 默认情况下，写锁优先级高于读锁，若存在锁争抢情况，在上一个锁被释放时会优先给写锁队列中等待的请求，然后再分配给读锁队列等待的请求 这也是 MyISAM 存储引擎的表不适合大量更新、查询操作的原因，在大量更新操作时，会导致查询操作难以获取读锁有可能出现一致阻塞的情况，并且时间较长的查询会导致写操作线程”饿死”，所以应用程序中应避免出现运行时间较长的查询操作 在 MyISAM 存储引擎中我们可以通过参数配置修改读写锁的优先级： low-priority-updates : 配置默认给读请求优先权 执行 set low-priority-updates &#x3D; 1 命令, 让该连接发出的更新请求优先级降低 指定 insert、update、delete 语句的 low_priority 属性, 降低执行语句的优先级 设置系统参数 max_write_lock_count 的值, 当表的读锁达到这个值后, MySQL 会暂时性的降低写请求的优先级, 从而使读进程获取锁 2. 查询表锁争用情况1234567mysql&gt; SHOW STATUS LIKE &#x27;Table%&#x27;;+-----------------------+---------+| Variable_name | Value |+-----------------------+---------+| Table_locks_immediate | 34800596 || Table_locks_waited | 0 |+-----------------------+---------+ 可以通过 table_locks_waited 和 table_locks_immediate 来分析表锁的争用情况, 如果 table_locks_immediate 数值较高, 则可以认为存在严重的锁竞争情况 3. InnoDB 中的锁在 InnoDB 中行锁和表锁是共存的, 它实现了如下两种行锁： 共享锁 (S) : 允许一个事务去读一行, 阻止其他事务获得相同数据集的排他锁 排他锁 (X) : 允许获得排他锁的事务更新数据, 阻止其他事务取得相同数据集的共享读锁和排他写锁 而为了兼容表锁和行锁, 实现多粒度的锁, InnoDB 中还存在两种意向锁, 这两种都是表锁： 意向共享锁 (IS) : 事务打算给数据行加行共享锁, 事务在给一个数据行加共享锁前必须先取得该表的意向共享锁 意向排他锁 (IX) : 事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的意向排他锁 锁兼容情况表： 共享锁 排他锁 意向共享锁 意向排他锁 共享锁 兼容 冲突 兼容 冲突 排他锁 冲突 冲突 冲突 冲突 意向共享锁 兼容 冲突 兼容 兼容 意向排他锁 冲突 冲突 兼容 兼容 若一个事务请求的锁与当前的锁是兼容的, InnoDB 就会把请求的锁给到该事务; 相反, 若两者不兼容, 那么该事务就要等待锁的释放 4. InnoDB 加锁方法加锁情况： 对于意向锁是 InnoDB 自动加的, 不需用户干预 对于 insert, update, delete 语句, InnoDB 会自动给涉及数据加上排他锁 对于 select 语句, InnoDB 不会进行加锁操作 也可以通过如下语句显示的加排他锁和共享锁： select … from 表名 where … lock in share mode ：其他会话仍然可以查询本条记录, 并且可以加 share mode 的共享锁, 但是如果需要对该记录进行更新, 那么就会有可能造成死锁 select … from 表名 where … for update ：其他会话可以查询该记录, 但是不能对此记录加排他锁, 共享锁, 而是阻塞等待获取锁 5. 隐式锁定与显示锁定1. 隐式锁定InnoDB 在事务执行时, 采用两阶段锁协议 : 在任何时间都可以执行锁定, InnoDB 会根据隔离级别自动加锁 锁只有在提交和回滚的时候才会在同一时间释放 2. 显示锁定1234-- 共享锁select ... lock in share mode-- 排他锁select ... for update select … for update : 我们一般在确保查询的是最新数据时会用到 for update, 在执行加了 for update 的查询语句时, 会将对查询行加排他锁, 也就是说只允许自己进行修改 select … lock in share mode : 在使用 lock in share mode 时, 会对查询数据加共享锁, 同样是为了确保最新数据, 不允许其他用户进行修改, 但同样的自己也不一定能修改这条数据, 因为有可能其他事务也存在对相同数据添加 lock in share mode 的情况 上面两种锁的区别： for update 为排他锁, 事务一旦获取此锁, 其他数据无法再在同样数据上添加 for update lock in share mode 是共享锁, 多个事务可以同时对同样数据添加 lock in share mode 3. 性能影响select … for update 语句实际上相当于一个 update 语句, 若事务没有及时提交或回滚的情况下, 有可能造成其他事务长时间等待的问题, 影响数据库的并发效率 select … lock in share mode 允许同时对数据上共享锁, 但是不能对数据进行更新操作, 同理如果不及时提交和回滚也可能造成大量事务等待问题 InnoDB 的间隙锁在用户使用范围查询而非等值查询并请求锁时，InnoDB 会将符合的已存在数据记录的索引项加锁，对于键值在条件范围内但并不存在的记录，叫做间隙（GAP），InnoDB 同样会对这个”间隙”加锁，这种锁机制就是所谓的间隙锁，间隙锁只会在 Repeatableread (可重复读) 隔离级别下使用 用学生表举例，如果 student 表存在 101 条记录，其中 studentId 分别是 1 - 101： 1SELECT * FROM student WHERE studentId &gt; 100 for update; 在上面的范围查询中，InnoDB 不仅会对复合条件的 101 进行加锁，还会对 studentId 大于 101 的”间隙”加锁，即便这些记录不存在 InnoDB 使用间隙锁的目的InnoDB 使用间隙锁的目的有两个： 防止幻读, 在可重复读的隔离级别下, 通过 GAP 锁是可以避免幻读的 满足恢复和复制的需求 MySQL 是通过 binlog 回放执行成功的增删改 SQL 语句来进行主从复制和数据恢复的，根据其恢复特点分析恢复和复制的需求： binlog 恢复是重放 SQL 语句 binlog 按照事务提交的先后顺序记录, 回放也是根据这个顺序回放 可以看出, 在一个事务没有提交之前, 其他并发事务不能插入满足其锁定条件的任何记录, 通俗来讲就是不允许出现幻读 获取 InnoDB 行锁的争用情况通过 innodb_row_lock 变量来分析行锁的争用情况： 1234567891011mysql&gt; show status like &#x27;innodb_row_lock%&#x27;;+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| InnoDB_row_lock_current_waits | 0 | -- 当前正在等待锁定的数量；| InnoDB_row_lock_time | 6345955 | -- 从系统启动到现在锁定总时间长度；| InnoDB_row_lock_time_avg | 287 | -- 每次等待所花平均时间；| InnoDB_row_lock_time_max | 51094 | -- 从系统启动到现在等待最长的一次所花的时间；| InnoDB_row_lock_waits | 22069 | -- 系统启动后到现在总共等待的次数；+-------------------------------+-------+5 rows in set (0.01 sec) 事务什么是事务概念 : 事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚 ACID： 原子性 : 事务被视为不可分割的最小单元, 事务的所有操作要么全部提交成功, 要么全部失败回滚 一致性 : 数据库在事务执行前后都保持一致性状态, 在一致性状态下, 所有事务对同一个数据的读取结果都是相同的 隔离性 : 一个事务所做的修改在最终提交以前, 对其它事务是不可见的 持久性 : 一旦事务提交, 则其所做的修改将会永远保存到数据库中, 即使系统发生崩溃, 事务执行的结果也不能丢失 ACID 并非平级关系： 只有满足一致性的前提下, 事物的执行结果才认为是正确的 无并发情况下, 事务是串行执行的, 隔离性一定能够满足, 此时只要还满足原子性, 那么就一定能满足一致性 并发情况下, 多个事务并行执行, 事务不仅要满足原子性, 还要满足隔离性, 才可以满足一致性 事务满足持久化是为了能应对系统崩溃的情况 MySQL 默认采用自动提交模式, 这表示, 如果不在 SQL 中显示的使用 START TRANSACTION 语句开始一个事务, 那么每个操作都会被当做一个事务并自动提交 并发下一致性问题1. 脏读脏读问题指事务读取了未提交的数据, 例如 : A 事务修改 id &#x3D; 1 的 name 从 ‘铁蛋’ 修改为 ‘loger’, 但未提交 随后 B 事务读取 id &#x3D; 1 的数据, 获取数据为 ‘loger’ 如果 A 事务进行回滚操作, 撤销了修改, 那么 B 事务读取的就是脏数据 2. 不可重复度不可重复读指在一个事务内多次读取同一数据集合, 在一个事务未结束, 另一个事务也访问了同一数据进行修改, 由于第二次的修改就会导致, 第一次事务的两次读取可能不一致, 例如 : A 事务读取一个数据, 并未结束 B 事务访问同一数据, 并做修改 如果 A 事务再次读取这个数据, 此时读取结果会与第一次读取结果不同 3. 幻读幻读本质上也属于不可重复读的情况, 一个事务在前后两次查询同一范围的时候，后一次查询看到了前一次查询没有看到的行, 例如 : A 事务按照一定条件读取范围数据 B 事务在这个范围内插入了新的数据 当 A 事务再次按照条件读取范围数据时, 发现 B 事务插入的数据, 第一次读取结果与第二次不同 4. 丢失更新丢失更新指一个事务的更新操作会被另一个事务的更新操作所覆盖, 例如 : A, B 两个事务都对同一个数据进行修改 A 先执行完毕, 提交后生效 B 随后执行完毕, 提交后覆盖了 A 的修改 事务隔离级别并发下一致性问题其原因在于破坏了事务的隔离性，通过确保隔离性就可以解决上述问题，其方案有： 通过锁 通过事物的隔离级别 事务的隔离级别分为以下四种： 读未提交 (READ UNCOMMITTED)：事务可以读取未提交数据 读已提交 (READ COMMITTED)：事务只可以读取已经提交的事务所做的修改 可重复读 (REPEATABLE READ)：同一个事务多次读取同样记录结果一致 可串行化 (SERIALIZABLE)：读取每一行数据上都加锁, 强制事务串行执行 事务隔离级别能处理的一致性问题表格如下： 脏读 不可重复读 幻读 读未提交 × × × 读已提交 √ × × 可重复读 √ √ × 可串行化 √ √ √ 多版本并发控制(MVCC)多版本并发控制 (Multi-Version Concurrency Control, 以下简称 MVCC), 是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式, 用于实现提交读和可重复读这两种隔离级别, 而未提交读隔离级别总是读取最新的数据行, 要求很低, 无需使用 MVCC, 可串行化隔离级别需要对所有读取的行都加锁, 单纯使用 MVCC 是无法实现的 1. 名词解释版本号： 系统版本号 SYS_ID : 是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增 事务版本号 TRX_ID : 事务开始时的系统版本号 回滚指针 ROLL_POINTER : 一个指针, 指向上一个版本位置 当前读与快照读： 当前读：像 SELECT … LOCK IN SHARE MODE (共享锁), SELECT … FOR UPDATE, UPDATE, INSERT, DELETE, 这些操作都是一种当前读, 它们读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁 快照读：不加锁的 SELECT 操作就是快照读, 快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读, 快照读基于 MVCC，为了提高并发性能的考虑 ReadView：MVCC 在内部维护了一个 ReadView 结构, 内部包含当前系统未提交的事务列表 (TRX_IDs), 该列表的最小值 (TRX_ID_MIN) 和最大值 (TRX_ID_MAX), 在进行 SELECT 操作时, 根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系, 从而判断数据行快照是否可以使用 2. 工作原理当一个事务开始时，数据库系统为其创建一个数据的快照版本，并且当事务结束时，该版本的修改将被提交。其他事务在读取该数据时，也看到的是其对应的快照版本，而不会受到当前事务的影响 细节： 获取事务自己的版本号, 也就是 TRX_ID 获取 ReadView 查询得到的数据，然后与 ReadView 中的事务版本号进行比较 TRX_ID &lt; TRX_ID_MIN, 表示该数据行快照是在当前所有未提交事务之前进行更改的, 因此可以使用 TRX_ID &gt; TRX_ID_MAX, 表示该数据行快照是在事务启动之后被更改的, 因此不可使用 TRX_ID_MIN &lt;&#x3D; TRX_ID &lt;&#x3D; TRX_ID_MAX, 需要根据隔离级别再进行判断 读未提交 : 直接读取最新版本 ReadView 读已提交 : 每次查询的开始都会生成一个独立的 ReadView 可重复读 : 可重复读隔离级别则在第一次读的时候生成一个 ReadView，之后的读都复用之前的 ReadView 如果不符合 ReadView 规则， 那么就需要 undo log 中回滚指针 ROLL_POINTER 找到下一个快照，再进行上面的判断","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"集合知识点总结(附源码解析)","slug":"集合知识点总结-附源码解析","date":"2023-02-01T02:30:03.000Z","updated":"2023-02-06T07:38:28.302Z","comments":true,"path":"2023/02/01/集合知识点总结-附源码解析/","link":"","permalink":"http://logerjava.github.io/2023/02/01/%E9%9B%86%E5%90%88%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93-%E9%99%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"概览ListArrayList ： 底层为数组实现，查找访问效率高，增删效率较低 每次扩容为原来的 1.5 倍，默认初始化容量为 10 增删效率低的原因是因为，需要进行 navite 方法的数组拷贝复制 线程不安全 LinkedList ： 底层双向链表实现，随机访问效率低，增删效率高 Vector ： 底层数组实现，现在比较少用 Vector 的所有方法均为synchronized修饰，性能损耗较大 Vector 初始 length 是 10 超过 length 时以 100% 比率增长（2 倍），相比于 ArrayList 消耗更多的内存 SetTreeSet ： 基于红黑树实现，支持有序性操作 查找效率不如 HashSet HashSet ： 基于哈希表实现，支持快速查找，不支持有序性操作 LinkedHashSet ： 具有 HashSet 的查找效率，并且内部使用双向链表维护元素的插入顺序 MapHashMap ： 基于哈希表实现 TreeMap ： 基于红黑树实现 HashTable ： 与 HashMap 类似，但是其线程安全，同一时刻多线程同时写入 HashTable 不会导致数据不一致，但是不建议使用遗留类，若想使用线程安全的 Map 建议使用 ConcurrentHashMap LinkedHashMap ： 使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序 源码分析及拓展ArrayList扩容12345678910111213141516171819202122232425262728293031public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 进入add方法分别分为四部分： 首先进入 add() 方法，为了得到最小的容量，避免资源浪费，这里用了 size + 1 确认一下 +1 后是否满足我们的需求 在 ensureExplicitCapacity(int minCapacity) 方法中判断 minCapacity - elementData.length &gt; 0 如果添加容量大于最小容量则调用 grow() 进行扩容 在 grow()方法中将，新容量 &#x3D; 旧容量 + 旧容量右移1位 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 此处相当于1.5倍扩容 然后调用 copyOf() 方法，复制指定数组 可以看到，需要进行扩容就需要调用 Arrays.copyOf() 将原数组整个复制到新数组中，此操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少需要的扩容次数 删除元素12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; remove方法的实现步骤： 先检查角标是否越界 删除掉指定元素 因为 ArrayList 是数组的数据结构，所以需要计算出要移动的个数，将后面的向前移动 将 elementData[--size] 设为 null，来让 GC 回收 需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，我查了一下该操作的时间复杂度为 O(N)，可以看到 ArrayList 删除元素的代价是非常高的 elementData[–size] 为什么要设为 null ？ 这个问题首先要了解 GC 回收的原理，GC 回收采用可达性分析算法，通过 GC Roots 到节点是否可达来进行对象的回收。回到问题，删除一个元素后，numMoved 长度的数组元素，要向前移动，很明显移动过后 list 中的数组长度减少了 1。所以此处将多出的这个引用，也就是 elementData[–size] 设为 null ， 让 GC 可以正确回收内存，并且 size 自身减少1后可以保证 size 属性可以返回 list 所需要的正确数组长度。 我们可以举一个具体的例子，比如此处为 0、1、2、3、4、5 现在我想 remove 掉下标为 1 的元素，此处变为 0、2、3、4、5、5 最后的 5 依旧存在，所以将它设为 null 进行释放。 Vector同步12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125; Vector 的实现与 ArrayList 类似，但是使用了 synchronized 进行同步 扩容12345678public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125; 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; Vector 的构造函数可以传入 capacityIncrement 参数，它的作用是在扩容时使容量 capacity 增长 capacityIncrement，如果这个参数的值小于等于 0，扩容时每次都令 capacity 为原来的两倍 1234567public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 而在调用没有 capacityIncrement 参数的构造函数时，capacityIncrement 值默认为 0，也就是说默认情况下 Vector 每次扩容的容量都会翻倍 同 ArrayList 的比较 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢，最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己控制 Vector 每次扩容请求其大小的 2 倍，而 ArrayList 是 1.5 倍 替代方案可以使用 Collections.synchronizedList() 得到一个线程安全的 ArrayList 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteArrayList读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响 写操作需要加锁，防止并发写入时导致写入数据丢失 写操作结束之后需要把原始数组指向新的复制数组 123456789101112131415161718public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125; 1234@SuppressWarnings(&quot;unchecked&quot;)private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 适用场景CopyOnWriteArrayList 在写操作的同时允许读操作的进行，这样大大提高了读的性能，因此很适合读多写少的应用场景 但是 CopyOnWriteArrayList 也存在缺陷 ： 内存占用 ：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右 数据不一致 ：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景 LinkedList同 ArrayList 的比较ArrayList 基于动态数组实现，LinkedList 基于双向链表实现。ArrayList 和 LinkedList 的区别可以归结为数组和链表的区别 ： 数组支持随机访问，但插入删除代价很高，需要移动大量元素 链表不支持随机访问，但插入删除只需要改变指针 HashMap存储结构JDK 1.8 以前 ：HashMap 的数据结构为数组+链表实现的，数组中存储了 key-value 的键值对（Entry），在进行插入操作时会根据 key 的 hash 计算出 index 值表示在数组中插入的位置，而 hash 存在概率性，不同的 key 计算 hash 出的 index 可能是一样的，这样就形成了链表 JDK 1.8 及之后 ：HashMap 的数据结构为数组+链表+红黑树，当链表长度超过 8 时会自动转为红黑树，小于 6 时重新变为链表 头插法在 JDK 1.8 以前，采用头插法处理链表插入问题 例如实例化如下 HashMap 1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;K1&quot;, &quot;V1&quot;);map.put(&quot;K2&quot;, &quot;V2&quot;);map.put(&quot;K3&quot;, &quot;V3&quot;); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16&#x3D;3 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6，插在 &lt;K2,V2&gt; 前面 链表指针顺序为 k3 -&gt; k2 环形链表问题环形链表问题是 JDK 1.8 HashMap 将链表插入从头插法变为尾插法的主要原因 假设有一个容量大小为 2 的 HashMap，负载因子 0.75，现将插入 α、β、γ 三个数据 key 为 1、2、3 因为扩容机制的原因，上述图片其实并不可能发生，事实上在插入第二个元素时就会进行扩容，因为 JDK 1.7 采用头插法的原因，新加入的元素会放在链表的头部，又因为经过 rehash 后同一链表上的元素可能被放到数组的其他位置，所以可能会变成下面的样子 可以发现指针位置是可能发生改变的，此时我们带入多线程的场景，用不同的线程操作分别插入 1、2、3 因为头插法更换指针的原因，可能就会变成下面的样子 因为头插法会改变链表的顺序，在多线程场景下，就有可能会出现环形链表的问题，陷入无限循环 尾插法在 JDK 1.8 及之后，采用尾插法处理链表插入问题 例如实例化如下 HashMap 1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;K1&quot;, &quot;V1&quot;);map.put(&quot;K2&quot;, &quot;V2&quot;);map.put(&quot;K3&quot;, &quot;V3&quot;); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16&#x3D;3 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6，插在 &lt;K2,V2&gt; 后面 链表指针顺序为 k2 -&gt; k3，并且在扩容后不会改变链表顺序，所以尾插法并不会出现环形链表问题 扩容 - 基本原理123456789101112131415static final int DEFAULT_INITIAL_CAPACITY = 16; // table 的容量大小，默认为 16, 需要注意的是 capacity 必须保证为 2 的 n 次方static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient Entry[] table;transient int size; // 键值对数量int threshold; // size 的临界值,当 size 大于等于 threshold 就必须进行扩容操作final float loadFactor; // 装载因子, table 能够使用的比例, threshold = (int)(capacity* loadFactor)transient int modCount; 123456void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length);&#125; 当需要扩容时，令 capacity 为原来的两倍 123456789101112131415161718192021222324252627282930void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把 oldTable 的所有键值对重新插入 newTable 中，因此这一步很费时 扩容 - 重新计算桶下标在进行扩容时，需要将键值对重新计算桶下标，从而放到对应桶上，HashMap 计算桶下标方法如下，HashMap capacity 为 2^n 这一特点能够极大降低重新计算桶下标的复杂度 123456int hash = hash(key);int i = indexFor(hash, table.length);static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 假设原数组长度为 16，扩容之后 new capacity 为 32 12capacity : 00010000new capacity : 00100000 对于一个 key 值，它的哈希值在第 5 位 ： 为 0 时，hash % 00010000 &#x3D; hash % 00100000，桶位置和原来一致 为 1 时，hash % 00010000 &#x3D; hash % 00100000 + 16，桶位置是原位置加16 扩容 - 计算数组容量HashMap 构造函数允许用户传入容量为非 2^n 的情况，因为它可以自动将传入容量转换为 2^n 先考虑如何求一个数的掩码，对于 10000000 其掩码为 11111111，可以使用如下方法 ： 12345int num = 1000 0000int mask = nummask |= mask &gt;&gt; 1 11000000 // 将num右移一位得到 0100 0000，然后与原始的num(1000 0000)进行或运算，得到mask（1100 000）mask |= mask &gt;&gt; 2 11110000 // 将刚得到的mask再右移一位得到 0011 0000，然后与上一步得到的mask(1100 0000)进行或运算，得到新mask（1110 0000）mask |= mask &gt;&gt; 4 11111111 // 我们再次将刚得到的mask右移4位，得到0000 1111,然后与上一步得到的mask进行与运算，得到1111 1111 mask + 1 是大于原始数字的最小的 2^n 12num 10000000mask + 1 100000000 以下是 HashMap 中计算数组容量的代码 ： 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 同 Hashtable 的对比 Hashtable 使用 synchronized 来进行同步 HashMap 可以插入键为 null 的 Entry HashMap 的迭代器是 fail-fast 迭代器 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的 ConcurrentHashMap存储结构 在 JDK 1.8 以前 ConcurrentHashMap 和 HashMap 实现上类似，主要差别在于 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），也就是说 ConcurrentHashMap 的底层实现为 Segments + HashEntry 数组 123456static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;&#125; Segment 继承自 ReentrantLock，查看源码的话还可以发现对每个 Segment 都有进行单独加锁的操作, 我们也可以这样认为 Segment 的个数为锁的并发度 1234567891011121314151617static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;&#125; 1final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; HashEntry 虽然用来存储键值对但是和 HashMap 并不是相同的, 因为 HashEntry 采用 volatile 字段修饰了 value 和 指向下一节点的 next, 确保了可见性, 这也是为什么 ConcurrentHashMap 高效的原因之一, 因为 volatile 原因导致它的 get 方法根本不用加锁 JDK 1.7 的 ConcurrentHashMap 采用 CAS 方式更新 baseCount 来确保线程安全, 如果失败则必定存在线程竞争关系, 此时会调用 scanAndLockForPut() 方法自旋获取锁, 在其内部存在 MAX_SCAN_RETRIES 可以理解为最大重试次数, 如果达到了则改为阻塞锁获取, 确保修改成功 在 JDK 1.8 时, ConcurrentHashMap 放弃了分段锁, 取而代之的类似于 HashMap 的数组 + 链表 + 红黑树结构，采取 CAS + Synchronized 的方式来保证线程安全，put 操作流程如下 ： 首先根据 key 计算出 hashCode 判断是否需要进行初始化操作 根据 key 定位到 Node (1.7 中的 HashEntry), 如果是 null 则表示当前位置可以写入数据, CAS 操作写入, 若失败则自旋确保成功 如果当前的 hashCode &#x3D;&#x3D; MOVED &#x3D;&#x3D; -1, 则表示需要扩容 若都不满足, 就利用 Synchronized 锁写入数据 若数据大于 TREEIFY_THRESHOLD, 也就是大于 8 就转为红黑树","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"LiteFlow 复杂业务逻辑解耦","slug":"LiteFlow-复杂业务逻辑解耦","date":"2022-11-11T02:31:01.000Z","updated":"2022-11-11T02:36:14.298Z","comments":true,"path":"2022/11/11/LiteFlow-复杂业务逻辑解耦/","link":"","permalink":"http://logerjava.github.io/2022/11/11/LiteFlow-%E5%A4%8D%E6%9D%82%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91%E8%A7%A3%E8%80%A6/","excerpt":"","text":"应用场景参考官网: https://liteflow.yomahub.com/ 使用示例pom12345&lt;dependency&gt; &lt;groupId&gt;com.yomahub&lt;/groupId&gt; &lt;artifactId&gt;liteflow-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; demoyml 配置: 12liteflow: rule-source: config/flow.el.xml resource 下 config&#x2F;flow.el.xml 定义规则: 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;flow&gt; &lt;chain name=&quot;chain1&quot;&gt; THEN(a, b, c); &lt;/chain&gt;&lt;/flow&gt; 测试类: 12345678910@Component(&quot;a&quot;)public class ACmp extends NodeComponent &#123; @Override public void process() throws Exception &#123; Thread.sleep(3000); System.out.println(&quot;a : &quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;))); &#125;&#125; 12345678@Component(&quot;b&quot;)public class BCmp extends NodeComponent &#123; @Override public void process() throws Exception &#123; Thread.sleep(3000); System.out.println(&quot;b : &quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;))); &#125;&#125; 12345678@Component(&quot;c&quot;)public class CCmp extends NodeComponent &#123; @Override public void process() throws Exception &#123; Thread.sleep(3000); System.out.println(&quot;c : &quot; + LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;))); &#125;&#125; 1234567@SpringBootApplication@ComponentScan(&#123;&quot;com.loger.java.test&quot;&#125;)public class ApplicationRun &#123; public static void main(String[] args) &#123; SpringApplication.run(ApplicationRun.class, args); &#125;&#125; 测试: 1234567891011121314@SpringBootTest@RunWith(SpringRunner.class)public class TestAll &#123; @Resource private FlowExecutor flowExecutor; @Test public void test1()&#123; LiteflowResponse liteflowResponse = flowExecutor.execute2Resp(&quot;chain1&quot;, &quot;arg&quot;); System.out.println(JSON.toJSONString(liteflowResponse)); &#125;&#125; LiteFlow 可以将复杂的业务代码拆分为一个个小组件, 根据定义的规则流程进行运行","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"GroovyClassLoader 从字符串中加载解析代码","slug":"GroovyClassLoader-从字符串中加载解析代码","date":"2022-11-11T02:28:39.000Z","updated":"2022-11-11T02:30:33.338Z","comments":true,"path":"2022/11/11/GroovyClassLoader-从字符串中加载解析代码/","link":"","permalink":"http://logerjava.github.io/2022/11/11/GroovyClassLoader-%E4%BB%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%8A%A0%E8%BD%BD%E8%A7%A3%E6%9E%90%E4%BB%A3%E7%A0%81/","excerpt":"","text":"应用场景个性化较强, 复杂多变的业务场景, 可将代码持久化到数据库, 在执行时直接执行数据库脚本达到动态更新效果 使用示例pom12345&lt;dependency&gt; &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt; &lt;artifactId&gt;groovy&lt;/artifactId&gt; &lt;version&gt;$&#123;groovy.version&#125;&lt;/version&gt;&lt;/dependency&gt; 示例代码123456/** * 抽象类 */public abstract class TestHandler &#123; public abstract String testHandler(String str) throws Exception;&#125; 1234567891011121314151617181920@Testpublic void testAll() throws Exception &#123; GroovyClassLoader groovyClassLoader = new GroovyClassLoader(); Class&lt;?&gt; clazz = groovyClassLoader.parseClass( &quot;package com.loger.java.test;\\n&quot; + &quot;\\n&quot; + &quot;import com.loger.java.test.TestHandler;\\n&quot; + &quot;\\n&quot; + &quot;public class DemoTestHandler extends TestHandler &#123;\\n&quot; + &quot;\\n&quot; + &quot;\\t@Override\\n&quot; + &quot;\\tpublic String testHandler(String str) throws Exception &#123;\\n&quot; + &quot; return str;\\n&quot; + &quot;\\t&#125;\\n&quot; + &quot; \\n&quot; + &quot;\\n&quot; + &quot;&#125;\\n&quot;); TestHandler testHandler = (TestHandler) clazz.newInstance(); System.out.println(testHandler.testHandler(&quot;有参测试 !!!&quot;));&#125; 通过 GroovyClassLoader 中的 parseClass 方法, 编译加载 Groovy 脚本(可支持纯 Java 代码), 脱离 Java 的双亲委派模型 通过反射构建对象, 执行方法","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"KeyTool 工具生成密钥对","slug":"KeyTool-工具生成密钥对","date":"2022-11-09T05:56:50.000Z","updated":"2023-02-06T07:34:29.329Z","comments":true,"path":"2022/11/09/KeyTool-工具生成密钥对/","link":"","permalink":"http://logerjava.github.io/2022/11/09/KeyTool-%E5%B7%A5%E5%85%B7%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5%E5%AF%B9/","excerpt":"","text":"生成 JKS1keytool -genkeypair -alias *** -keyalg RSA -keypass *** -keystore xxx.jks -storepass *** -keystore xxx.jks 查看 JKS 生成的证书详细信息1keytool -list -v -keystore xxx.jks 导出 cer 证书1keytool -alias *** -exportcert -keystore xxx.jks -file xxx.cer 导出公钥, 此处命令需要 OpenSSL , 并配置环境变量1keytool -list -rfc --keystore xxx.jks | openssl x509 -inform pem -pubkey 拷贝出公钥","categories":[],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://logerjava.github.io/tags/Tools/"}]},{"title":"Linux 安装 JDK","slug":"Linux-安装-JDK","date":"2022-11-09T05:56:00.000Z","updated":"2022-11-09T05:56:22.285Z","comments":true,"path":"2022/11/09/Linux-安装-JDK/","link":"","permalink":"http://logerjava.github.io/2022/11/09/Linux-%E5%AE%89%E8%A3%85-JDK/","excerpt":"","text":"下载 tar.gz 包 12解压到指定位置tar -zxvf jdk.tar.gz -C /目录 12配置环境变量vim /etc/profile 12345export JAVA_HOME=/jdk/jdk1.8.0_311export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$CLASSPATHexport JAVA_PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/binexport PATH=$PATH:$&#123;JAVA_PATH&#125; 12让配置文件立即生效, 不行就重启source /etc/profile javac, java -verison 测试是否成功 echo $PATH 查看环境变量是否正确","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"}]},{"title":"Linux 集群部署 Nacos","slug":"Linux-集群部署-Nacos","date":"2022-11-09T05:55:13.000Z","updated":"2022-11-09T05:55:45.354Z","comments":true,"path":"2022/11/09/Linux-集群部署-Nacos/","link":"","permalink":"http://logerjava.github.io/2022/11/09/Linux-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-Nacos/","excerpt":"","text":"在 nacos.io 下载 tar.gz 包, 导入 linux 解压 因为 nacos 集群部署, 各个节点配置信息需要一致, 所以采取 MySQL 持久化 找到 &#x2F;conf 下的 nacos-mysql.sql 文件, 在 MySQL 执行 修改 application.properties 文件, 添加如下配置 : 1234567spring.datasource.platform=mysqldb.num=1db.url.0=jdbc:mysql://10.1.30.114:3306/nacos_config?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user.0=rootdb.password.0=1qaz@WSX 在 &#x2F;conf 下 的 cluster.conf 文件, 添加节点 ip 和 port, 例如: 12310.1.30.111:884810.1.30.112:884810.1.30.113:8848 启动 nacos 1sh startup.sh 如果出现 oom 问题, 编辑 startup.sh, 调整 jvm 内存 : 1-server -Xms512m -Xmx512m -Xmn256m -XX:MetaspaceSize=64m -XX:MaxMetaspaceSize=128m 如果出现 12Nacos Server did not start because dumpservice bean construction failure :No DataSource set 原因可能是因为 MySQL 没有给当前 ip 开放 1grant all privileges on *.* to root@&quot;xxx.xxx.xxx.xxx&quot; identified by &quot;1qaz@WSX&quot;; 如果在浏览器无法访问, 可能原因是端口未在防火墙开放 1iptables -I INPUT -p tcp --dport 8848 -j ACCEPT","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"},{"name":"Nacos","slug":"Nacos","permalink":"http://logerjava.github.io/tags/Nacos/"}]},{"title":"Linux 安装 MySQL","slug":"Linux-安装-MySQL","date":"2022-11-09T05:54:14.000Z","updated":"2022-11-09T05:54:43.740Z","comments":true,"path":"2022/11/09/Linux-安装-MySQL/","link":"","permalink":"http://logerjava.github.io/2022/11/09/Linux-%E5%AE%89%E8%A3%85-MySQL/","excerpt":"","text":"安装 Mysql执行脚本12345678910安装 MySQLcurl https://webfile.newbanker.cn/mysql/install_centos7.sh -q | bash -s记录临时密码登录mysql修改mysql密码 mysql -u root -p输入数据安装后显示的密码set password for root@&#x27;localhost&#x27;=password(&#x27;1qaz@WSX&#x27;); 可能会遇到得问题SELinux linux服务器的安全策略问题1Can&#x27;t create test file /data/mysql/test-mysql.lower-test 安全策略问题, 可临时关闭 1setenforce 0 永久关闭需要修改配置文件，重启机器： 修改&#x2F;etc&#x2F;selinux&#x2F;config 文件 将SELINUX&#x3D;enforcing改为SELINUX&#x3D;disabled MySQL 不允许远程连接可能导致的原因 : 网络不通 服务未启动 1service mysqld start; 防火墙端口未开放 12345查看网络端口信息netstat -ntp查看防火墙状态,查看3306端口iptables -vnL 如果3306如下，是drop状态，或者根本无3306端口，说明3306端口设置问题 12添加需要监听的端口/sbin/iptables -I INPUT -p tcp --dport 3306 -j ACCEPT MySQL 没有允许远程登陆 12345678root 权限登录mysql -u root -p输入use mysql;查看是否只有 localhost 主机select user,host from user; 如果只有 localhost 主机, 那么把需要远程连接的添加到这里 1grant all privileges on *.* to root@&quot;xxx.xxx.xxx.xxx&quot; identified by &quot;1qaz@WSX&quot;; 1flush privileges;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"},{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"ShardingSphere-JDBC 读写分离","slug":"ShardingSphere-JDBC-读写分离","date":"2022-11-09T05:42:40.000Z","updated":"2023-02-06T07:36:51.781Z","comments":true,"path":"2022/11/09/ShardingSphere-JDBC-读写分离/","link":"","permalink":"http://logerjava.github.io/2022/11/09/ShardingSphere-JDBC-%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/","excerpt":"","text":"为什么要读写分离 ?随着我们系统的业务量扩展, 原有的单机 MySQL 肯定会发生 I&#x2F;O 频率过高等问题, 导致损失性能, 采用主从复制, 读写分离可以提高数据库的可用性, 以及利用率 实现方式读写分离有很多种实现方式, 比如 AOP 的方式通过方法名判断是读操作还是写操作, 进而使用 master 或 slave , 但是本着不重复造轮子的原则, 以及现有框架成熟度很高我们采取 Apache 的 ShardingSphere-JDBC 框架, 该框架不仅可以实现读写分离, 还有很多其他便利功能, 这里仅对读写分离进行简单讲解 ShardingSphere-JDBC 官方文档 - https://shardingsphere.apache.org/document/current/cn/overview/ 示例项目项目配置pom 文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.12.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.76&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.velocity&lt;/groupId&gt; &lt;artifactId&gt;velocity-engine-core&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; application.yml这里采用一主一从 12345678910111213141516171819202122232425spring: shardingsphere: datasource: names: master,slave master: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://10.1.30.114:3306/test-db?useUnicode=true&amp;characterEncoding=utf8&amp;tinyInt1isBit=false&amp;useSSL=false&amp;serverTimezone=GMT username: root password: 1qaz@WSX slave: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://10.1.30.113:3306/test-db?useUnicode=true&amp;characterEncoding=utf8&amp;tinyInt1isBit=false&amp;useSSL=false&amp;serverTimezone=GMT username: root password: 1qaz@WSX props: sql.show: true masterslave: load-balance-algorithm-type: round_robin sharding: master-slave-rules: master: master-data-source-name: master slave-data-source-names: slave 启动启动看到如下提示则代表配置成功 读写接口测试我们编写两个简单的读写接口 使用 postman 请求访问, 可以看到 insert 走的是 master, select 走的是 slave","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"MySQL Explain","slug":"MySQL-Explain","date":"2022-11-09T05:37:44.000Z","updated":"2022-11-09T06:02:13.607Z","comments":true,"path":"2022/11/09/MySQL-Explain/","link":"","permalink":"http://logerjava.github.io/2022/11/09/MySQL-Explain/","excerpt":"","text":"关于 ExplainExplain 查询字段的含义 字段 含义 id 该语句的唯一标识 select_type 查询类型 table 表名 type 联接类型 possible_keys 可能的索引选择 key 实际选择的索引 key_len 索引的长度 ref 索引的哪一列被引用了 rows 估计要扫描的行 Extra 附加信息 id该语句的唯一标识, 如果 explain 的结果包括多个 id 的值, 则数字越大越先执行; 对于相同 id 的行, 则表示从上向下依次执行 select_type查询类型, 具体如下表 : 查询类型 作用 SIMPLE 简单查询(未使用 UNION 或子查询) PRIMARY 最外层查询 UNION 在 UNION 中的第二个和随后的 SELECT 被标记为 UNION DEPENDENT UNION UNION 中的第二个或后面的查询, 依赖了外面的查询 UNION RESULT UNION 的结果 SUBQUERY 子查询中的第一个 SELECT DEPENDENT SUBQUERY 子查询中的第一个 SELECT , 依赖了外面的查询 DERIVED 用来表示包含在 FROM 子句的子查询中的 SELECT , MySQL 会递归执行并将结果放到一个临时表中 (MySQL 内部将其称为 Derived table 派生表, 因为该表是从子查询中派生出来的) DEPENDENT DERIVED 派生表, 依赖了其他的表 MATERIALIZED 物化子查询 UNCACHEABLE SUBQUERY 子查询, 结果无法缓存, 必须针对外部查询的每一行重新评估 UNCACHEABLE UNION UNION 属于 UNCACHEABLE SUBQUERY 的第二个或后面的查询 table表示当前这一行正在访问哪张表, 如果 SQL 定义了别名, 则展示表的别名 type联接类型, 取值如下 (性能由好到坏排序) : system : 该表只有一行(相当于系统表), system 是 const 类型的特例 const : 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据; const 查询速度非常快, 因为仅仅读取一次即可 eq_ref : 当使用了索引的全部组成部分, 并且索引是 PRIMARY KEY 或 UNIQUE NOT NULL 才会使用该类型, 性能仅次于 system 和 const ref : 当满足索引的最左前缀规则, 或者索引不是主键也不是唯一索引时才会发生, 如果使用的索引只会匹配到少量的行, 性能也是不错的 tips : 最左前缀原则, 指索引按最左优先的方式匹配索引 fulltext : 全文索引 ref_or_null : 该类型类似 ref , 但是 MySQL 会额外搜索哪些行包含了 null, 常见于解析子查询 index_merge : 表示使用索引合并优化, 表示一个查询里面用到了多个索引 unique_subquery : 类似 eq_ref , 但是使用了 IN 查询, 且子查询是主键或者唯一索引 index_subquery : 和 unique_subquery 类似, 只是子查询使用的是唯一索引 range : 范围扫描, 表示检索了指定范围的行, 主要用于有限制的索引扫描 index : 全索引扫描, 和 ALL 类似, 只不过 index 是全盘扫描了索引的数据. 当查询仅使用索引中的一部分时, 可使用此类型, 有两种情况会触发 : 如果索引是查询的覆盖索引, 并且索引查询的数据就可以满足查询中所需的所有数据,则只扫描索引树. 此时, explain 的 Extra 列的结果是 Using index. index 通常比 ALL 快, 因为索引的大小通常小于表数据 按索引的顺序来查找数据行, 执行了全表扫描. 此时, explain 的 Extra 列的结果不会出现 Uses index ALL : 全表扫描, 性能最差 possible_keys展示当前查询可以使用那些索引, 这一列的数据是在优化过程的早期创建的, 因此有些索引可能对于后续优化过程是没用的 key表示 MySQL 实际选择的索引 key_len索引使用的字节数, 由于存储格式, 当字段允许为 NULL 时, key_len 比不允许为空时大 1 字节关于 key_len 的计算 : key_len 计算 ref表示将哪个字段或常量和 key 列所使用的字段进行比较 如果 ref 是一个函数, 则使用的值是函数的结果, 如果想查看是哪个函数, 可以在 EXPLAIN 语句后添加 SHOW WARNING 语句 rowsMySQL 估算会扫描的行数, 数值越小越好 Extra主要包括 Using filesort 、Using temporary 、Using index、Using where、Using join buffer、impossible where、select tables optimized away、distinct Using filesort : 说明 MySQL 会对数据使用一个外部的索引排序, 而不是按照表内的索引顺序进行读取; MySQL 中无法利用索引完成的排序操作称为 “文件排序” Using temporary : 使用了临时表保存中间结果, MySQL 在对查询结果排序时使用临时表; 常见于排序 order by 和分组 group by Using index : 表示相应 select 操作中使用了覆盖索引, 避免回表; 如果同时出现 Using where, 表明索引被用来执行索引键值的查找; 如果没有出现 Using where, 表明索引只是用来读取数据而非利用索引执行查找 Using where : 表明使用 where 过滤 Using join buffer : 表明使用了连接缓存 impossible where : where 的子句值总是 false select tables optimized away : 在没有 group by 子句的情况下, 基于索引优化 min&#x2F;max 操作或者对于 MyIsam 引擎, 优化 count(*) 操作, 不必等到执行阶段进行计算, 直接查询执行计划生成的阶段完成优化 distinct : 优化 distinct 操作, 在找到第一匹配的元组后即停止找同样值的动作 ……","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"MySQL 关于时间的一些思考","slug":"MySQL-时间的一些思考","date":"2022-11-09T05:36:42.000Z","updated":"2022-11-09T06:02:26.400Z","comments":true,"path":"2022/11/09/MySQL-时间的一些思考/","link":"","permalink":"http://logerjava.github.io/2022/11/09/MySQL-%E6%97%B6%E9%97%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/","excerpt":"","text":"关于 DateTime 和 Timestamp从时区方面考虑DateTime 没有时区信息, DateTime 在保存时保存的是当前会话所设置的时区对应的时间, 当时区更换会导致数据库读取时间出错Timestamp 存在时区信息, Timestamp 会跟随服务器的时区变化而变化, 自动换算成对应时间, 不同时区查询的时间是不同的 从占用空间, 时间范围方面考虑DateTime 耗费的空间更大, Timestamp 占用 4 个字节的存储空间, DateTime 占用 8 个字节的存储空间, 因此 Timestamp 表示的时间范围更小 DateTime : 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 Timestamp : 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 不要使用字符串存储时间使用字符串存储时间占用的空间更大, 效率较低(需要逐个字符对比), 无法使用相关函数进行计算和比较 不建议使用 int 和 bigint 表示时间此种存储方式拥有 Timestamp 类型具有的优点, 并且使用 int 和 bigint 进行日期排序和对比操作会更有效率, 跨系统也没有什么问题, 但是可读性很差, 无法看到具体时间 总结综上所述, 是关于 MySQL 中时间的一些思考, 可以看出关于 MySQL 的时间选择实际没有一种特定的最优解, 根据不同的业务场景应选择最适合的存储方法, 下面是各种类型的对比 : 日期类型 存储空间 日期格式 日期范围 是否存在时区问题 DateTime 8 字节 YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 是 Timestamp 4 字节 YYYY-MM-DD HH:MM:SS 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 否 时间戳 4 字节 全数字 1970-01-01 00:00:01 之后的时间 否","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"MySQL 高性能优化","slug":"MySQL-高性能优化","date":"2022-11-09T05:35:31.000Z","updated":"2022-11-09T06:02:19.564Z","comments":true,"path":"2022/11/09/MySQL-高性能优化/","link":"","permalink":"http://logerjava.github.io/2022/11/09/MySQL-%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","excerpt":"","text":"数据库命令规范 数据库对象名称使用小写字母, 下划线分割 数据库对象名称禁止使用 MySQL 关键字 数据库对象名称做到见名识意, 不要超过 32 字符 临时库表以 tmp_ 前缀, 日期为后缀; 备份表以 bak_ 为前缀, 日期为后缀 存储相同数据的列名和列类型必须一致 数据库基本设计规范1. 所有表均使用 Innodb 存储引擎在没有特殊需求的情况下(即 Innodb 无法满足的功能), 所有表必须使用 Innodb 存储引擎 2. 数据库和表的字符集统一使用 UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储emoji表情的需要，字符集需要采用utf8mb4字符集 3. 所有表和字段都需要添加注释4. 尽量控制单表数据量大小在 500 万以内单表数据量过大, 在修改表结构, 进行表备份, 恢复等操作时会出现问题, 可以通过分库分表手段控制表大小 5. 谨慎使用分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表, 谨慎选择分区键，跨分区查询效率可能更低, 建议采用物理分表的方式管理大数据 6. 条件允许尽量冷热数据分离, 减小表宽度MySQL 限制单表最多存储 4096 列, 并且每一行数据的大小不能超过 65535 字节减少磁盘 IO, 表越宽, 将表加载到内存缓冲池时所占用的内存也就越大, 会消耗更多的 IO , 保证热数据的内存缓存命中率, 更有效的利用缓存, 避免读入无用的冷数据, 经常一起使用的列可以放到一个表中(避免过多的关联操作) 7. 禁止在表中建立预留字段预留字段的命名很难做到见名识义, 预留字段无法确认存储的数据类型，所以无法选择合适的类型, 对预留字段类型的修改，会对表进行锁定 8. 禁止在数据库中存储图片, 文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时,通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在生产库做压力测试10. 禁止在开发, 测试环境直接连接生产环境数据库数据库字段设计规范1. 优先选择符合存储需要的最小的数据类型列的字段越大, 建立索引时所需要的空间也就越大, 这样一页中所能存储的索引节点的数量也就越少, 在遍历时需要的 IO 次数也就越多, 索引性能也就越差 example : 将字符串转换为数字类型存储(ip 地址转换为整型数据) MySQL 提供了两个方法来处理 ip 地址 inet_aton 把ip转为无符号整型(4-8位) inet_ntoa 把整型的ip转为地址 插入数据前，先用inet_aton把ip地址转为整型，可以节省空间, 显示数据时，使用inet_ntoa把整型的ip地址转为地址显示即可 对于非负型的数据（如自增 id、整型 ip）来说，要优先使用无符号整型来存储 因为, 无符号相对于有符号可以多出一倍的存储空间 SIGNED INT -21474836482147483647 UNSIGNED INT 04294967295 VARCHAR(N)中的N代表的是字符数，而不是字节数, 使用 UTF8 存储 255 个汉字, Varchar(255)&#x3D;765个字节, 过大的长度会消耗更多的内存 2. 避免使用 TEXT, BLOB 数据类型, 最常见的 TEXT 类型可以存储 64k 的数据建议将 BLOB 或 TEXT 列分离到单独的扩展表中 :MySQL 内存临时表不支持 TEXT, BLOB 这样的大数据类型, 如果查询中包含这样的数据, 在排序等操作时, 就不能使用内存临时表, 必须使用磁盘临时表进行, 而且对于这种数据, MySQL 还需要二次查询, 会使 SQL 性能变的很差, 如果一定要使用, 建议将 TEXT, BLOB 放到单独的扩展表, 查询时必要使用 select * , 而是查询指定列, 不需要 TEXT 时不要查询 TEXT 或 BLOB 类型只能使用前缀索引 :因为MySQL对索引字段长度是有限制的，所以TEXT类型只能使用前缀索引，并且TEXT列上是不能有默认值的 3. 避免使用 ENUM 类型修改 ENUM 类型需要使用 ALTER 语句, 并且 ENUM 类型的 ORDER BY 操作效率低, 需要额外操作 4. 尽可能将所有列定义为 NOT NULL索引 NULL 列需要额外的空间来保存, 所以要占用更多的空间, 进行比较和计算时都要对 NULL 值进行特别处理 5. 使用 Timestamp 或 DateTime 存储时间MySQL - 关于时间问题的一些思考 6. 同财务相关的金额类数据必须使用 decimal 类型float, double 为非精准浮点, decimal 是精准浮点, decimal 在计算时不会丢失精度, 占用空间由定义宽度决定, 每 4 个字节可以存储 9 位数字(小数点要占用 1 字节), 可存储比 bigint 更大的整型数据 索引设计规范1. 限制每张表的索引数量, 建议的单张表不超过 5 个索引并不是越多越好, 我们知道索引可以增加查询效率, 但是如果使用存在问题索引会降低写入的效率, 有些情况也会降低查询效率 MySQL 优化器在选择如何优化查询时, 会根据统一信息, 对每一个可以用到的索引进行评估,生成一个最好的执行计划, 如果同时有很多个索引都可以用于查询, 就会增加 MySQL 优化器生成执行计划的时间, 降低查询性能 2. 禁止给表中的每一列都建立单独的索引5.6 版本之前，一个 SQL 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好 3. Innodb 表必须存在主键Innodb是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的每个表都可以有多个索引，但是表的存储顺序只能有一种Innodb是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引）不要使用 UUID,MD5,HASH, 字符串列作为主键（无法保证数据的顺序增长）主键建议使用自增ID值 索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 多表 JOIN 的关联列 条件合适的情况下建立联合索引, 避免每个单独列都建立索引 索引顺序问题索引建立的目的是 : 通过索引进行数据查找, 减少随机 IO, 增加查询性能, 索引能过滤出越少的数据则从磁盘中读取的数据也就越少 区分度最高的放在联合索引的最左侧（区分度&#x3D;列中不同值的数量&#x2F;列的总行数） 尽量将字段长度小的列放在联合索引的最左侧（字段长度越小，一页能存储的数据量越大，IO性能也就越好） 使用最频繁的列放到联合索引最左侧（较少的建立一些索引） 避免建立冗余索引和重复索引原因 : 增加查询优化器生成执行计划的时间 重复索引示例：primary key(id)、index(id)、unique index(id)冗余索引示例：index(a,b,c)、index(a,b)、index(a) 对于频繁的查询优先考虑使用覆盖索引原因 : 避免 Innodb 表进行索引的二次查询 可以将随机 IO 变为顺序 IO 加快查询速度 详情 : MySQL - 索引机制 数据库 SQL 开发规范1. 建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题, 只传参数，比传递 SQL 语句更高效, 相同语句可以一次解析，多次使用，提高处理效率 2. 避免数据类型隐式转换隐式转换会导致索引失效, 在单次查询数据很多的情况下, 若查询列隐式转换将会降低效率 3. 充份利用已建立的索引example : 避免使用双 % 的查询条件 如 name like %loger% , 若无前置 % 只有后置 % , 是可以用到列上的索引的 一个 SQL 只能利用到复合索引中的一列进行查询 如有 a, b, c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b, c 列上的索引将不会被用到，在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧 使用 LEFT JOIN 或 NOT EXISTS 来优化 NOT IN 操作 NOT INT 会导致索引失效 4. 数据库设计时, 需考虑后续扩展情况5. 程序连接不同数据库使用不同账号, 进行跨库查询为数据库迁移和分库分表留出余地, 降低业务耦合度, 避免权限过大而产生的安全风险 6. 禁止使用 SELECT *消耗更多的 CPU 和 IO 以网络带宽资源, 无法使用覆盖索引, 可以减少表结构变更带来的影响 7. 禁止使用不含字段列表的 INSERT 语句如： insert into values (‘a’,’b’,’c’);应使用 insert into t(c1,c2,c3) values (‘a’,’b’,’c’); 8. 避免使用子查询, 可以将子查询优化为 JOIN 操作通常子查询在 IN 子句中, 且子查询为简单 SQL (不包含 union、group by、order by、limit 从句) 时, 才可以将子查询转化为关联查询进行优化 子查询性能差的原因 :子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响, 特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大, 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的CPU和IO资源，产生大量的慢查询 9. 避免使用 JOIN 关联太多表对于Mysql来说，是存在关联缓存的，缓存的大小可以由join_buffer_size参数进行设置在Mysql中，对于同一个SQL多关联（join）一个表，就会多分配一个关联缓存，如果在一个SQL中关联的表越多，所占用的内存也就越大 如果程序中大量的使用了多表关联的操作，同时join_buffer_size设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性 同时对于关联操作来说，会产生临时表操作，影响查询效率Mysql最多允许关联61个表，建议不超过5个 10. 减少同数据库交互次数数据库更适合处理批量操作, 合并多个相同的操作在一起, 可以提高处理效率 11. 对应同一列进行 OR 判断时, 使用 IN 替代 ORIN 的值不要超过 500 个IN 操作可以更有效的利用索引，OR 大多数情况下很少能利用到索引 12. 禁止使用 ORDER BY RAND() 进行随机排序会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的CPU和IO及内存资源 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式 13. WHERE 从句中禁止对列进行函数转换和计算会导致索引失效 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNIONUNION 会把两个结果集的所有数据放到临时表中后再进行去重操作UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大 SQL 为多个小 SQL大SQL:逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL, MySQL 一个 SQL 只能使用一个 CPU 进行计算, SQL 拆分后可以通过并行执行来提高处理效率 数据库操作行为规范超 100 万的批量写操作, 要分批次进行操作 大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量日志 大批量写操作会产生大量日志，特别是对于row格式二进制数据而言，由于在row格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对MySQL的性能产生非常大的影响, 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 对于大表使用 pt-online-schema-change 修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的 pt-online-schema-change它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器, 把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉, 把原来一个 DDL 操作，分解成多个小的批次进行","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"Java 泛型详解","slug":"Java-泛型详解","date":"2022-11-08T08:23:12.000Z","updated":"2022-11-09T06:06:34.290Z","comments":true,"path":"2022/11/08/Java-泛型详解/","link":"","permalink":"http://logerjava.github.io/2022/11/08/Java-%E6%B3%9B%E5%9E%8B%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"概述泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用&#x2F;调用时传入具体的类型（类型实参）。 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 举例12345678List list = new ArrayList();list.add(&quot;loger&quot;);list.add(100);for(int i = 0; i&lt; list.size();i++)&#123; String str = (String)list.get(i); log.info(&quot;泛型测试&quot;,&quot;str = &quot; + str);&#125; 运行结果 1java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String ArrayList 中可以存放任意类型, 当上述情况, 其中即存在 Integer 又存在 String, 且都以 String 的方式使用时, 程序就会报错, 泛型可以解决此类问题 1List&lt;String&gt; list = new ArrayList&lt;String&gt;(); 声明带泛型的集合, 在集合内类型不匹配时, 会直接报错 特性泛型只在编辑阶段有效 123456789List&lt;String&gt; stringArrayList = new ArrayList&lt;String&gt;();List&lt;Integer&gt; integerArrayList = new ArrayList&lt;Integer&gt;();Class classStringArrayList = stringArrayList.getClass();Class classIntegerArrayList = integerArrayList.getClass();if(classStringArrayList.equals(classIntegerArrayList))&#123; log.info(&quot;类型相同&quot;);&#125; 输出结果：类型相同 通过上面的例子可以证明，在编译之后程序会采取去泛型化的措施。也就是说 Java 中的泛型，只在编译阶段有效。在编译过程中，正确检验泛型结果后，会将泛型的相关信息擦出，并且在对象进入和离开方法的边界处添加类型检查和类型转换的方法。也就是说，泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 泛型的使用泛型类1234567891011121314// 此处 T 可以使用任意表示, T、E、K、V 均可// 实例化时必须指定 T 的具体类型public class Test&lt;T&gt; &#123; // T 的类型为外部指定 private T key; public Test(T key) &#123; this.key = key; &#125; public T getKey() &#123; return key; &#125;&#125; 传入实参的类型与泛型相同, 如不做泛型限制, 则会根据传入实参做相应限制 123456// 泛型限制Test&lt;String&gt; stringTest = new Test&lt;&gt;(&quot;loger&quot;);Test&lt;Integer&gt; integerTest = new Test&lt;&gt;(333);// 非限制Test test = new Test&lt;&gt;(33.33); 泛型接口1234// 定义一个泛型接口public interface Test&lt;T&gt; &#123; public T next(); &#125; 当实现泛型接口未传入实参时 1234567891011/** * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中 * 即：class TestImpl&lt;T&gt; implements Test&lt;T&gt;&#123; * 如果不声明泛型，如：class TestImpl implements Test&lt;T&gt;，编译器会报错：&quot;Unknown class&quot; */class TestImpl&lt;T&gt; implements Test&lt;T&gt;&#123; @Override public T next() &#123; return null; &#125; &#125; 当实现泛型接口的类，传入泛型实参时 1234567891011121314151617/** * 传入泛型实参时： * 定义一个生产器实现这个接口,虽然我们只创建了一个泛型接口 Test&lt;T&gt; * 但是我们可以为 T 传入无数个实参，形成无数种类型的 Test 接口 * 在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型 * 即：TestImpl&lt;T&gt;，public T next();中的的T都要替换成传入的 String 类型 */public class TestImpl implements Test&lt;String&gt; &#123; private String[] fruits = new String[]&#123;&quot;aaa&quot;, &quot;bbb&quot;, &quot;ccc&quot;&#125;; @Override public String next() &#123; Random rand = new Random(); return fruits[rand.nextInt(3)]; &#125;&#125; 泛型通配符试想一个问题 Integer 是 Number 的一个子类, 而 Test&lt;Integer&gt; 和 Test&lt;Number&gt; 实际上是相同的基本类型, 那么 Test&lt;Number&gt; 作为形参的方法中, 能否使用 Test&lt;Ingeter&gt; 实例传入呢 ? 123public void showValue(Test&lt;Number&gt; arg) &#123; log.info(arg.getKey()); &#125; 1234Test&lt;Integer&gt; integerTest = new Test&lt;&gt;(123);Test&lt;Number&gt; numberTest = new Test&lt;&gt;(456);showValue(integerTest); 很明显编译的时候就报错了 Test&lt;java.lang.Integer&gt; cannot be applied to Test&lt;java.lang.Number&gt; 由此可以看出: 同一种泛型可以对应多个版本（因为参数类型是不确定的），不同版本的泛型类实例是不兼容的 在处理上述问题时, 可以将泛型替换为 ? 123public void showValue(Test&lt;?&gt; arg) &#123; log.info(arg.getKey()); &#125; 注意, 此处 ? 代表的是类型实参, 而非形参, 换一种说话就是可以把 ? 看成所有类的父类, 它可以解决当具体类型不确定时, ? 即是通配符 泛型方法12345678910111213/** * 泛型方法的基本介绍 * @param clazz 传入的泛型实参 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间 &lt;T&gt; 非常重要, 可以理解为声明此方法为泛型方法 * 2）只有声明了 &lt;T&gt; 的方法才是泛型方法, 泛型类中的使用了泛型的成员方法并不是泛型方法 * 3）&lt;T&gt; 表明该方法将使用泛型类型 T , 此时才可以在方法中使用泛型类型 T * 4）与泛型类的定义一样, 此处 T 可以随便写为任意标识, 常见的如 T、E、K、V 等形式的参数常用于表示泛型 */public &lt;T&gt; T test(Class&lt;T&gt; clazz) throws Exception&#123; return clazz.newInstance();&#125; 泛型上下边界在使用泛型的时候，我们还可以为传入的泛型类型实参进行上下边界的限制，如：类型实参只准传入某种类型的父类或某种类型的子类 123public void showVale(Test&lt;? extends Number&gt; test)&#123; log.info(obj.getKey());&#125; 这样就规定了泛型的上边界, 传入的类型实参必须是指定类型的子类型 1234Test&lt;String&gt; test1 = new Test&lt;String&gt;(&quot;11111&quot;);Test&lt;Integer&gt; test2 = new Test&lt;Integer&gt;(2222);Test&lt;Float&gt; test3 = new Test&lt;Float&gt;(2.4f);Test&lt;Double&gt; test4 = new Test&lt;Double&gt;(2.56); 在编译时, test1 会报错, 因为 String 并不是 Number 的子类型","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"}]},{"title":"MySQL 主从复制","slug":"MySQL主从复制","date":"2022-11-08T05:32:44.000Z","updated":"2023-02-06T07:36:21.554Z","comments":true,"path":"2022/11/08/MySQL主从复制/","link":"","permalink":"http://logerjava.github.io/2022/11/08/MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/","excerpt":"","text":"主从复制原理 当 Master 节点进行 insert、update、delete 操作时，会按顺序写入到 binlog 中 salve 从库连接 master 主库，Master 有多少个 slave 就会创建多少个 binlog dump 线程 当 Master 节点的 binlog 发生变化时，binlog dump 线程会通知所有的 salve 节点，并将相应的 binlog 内容推送给 slave 节点 I&#x2F;O 线程接收到 binlog 内容后，将内容写入到本地的 relay-log SQL 线程读取 I&#x2F;O 线程写入的 relay-log，并且根据 relay-log 的内容对从数据库做对应的操作 这里有很重要的两个问题 : 从库同步主库数据的过程是串行化的, 也就是说主库上并行的操作, 在从库上会串行化执行, 由于从库从主库拷贝日志以及串行执行 SQL 的特点, 在高并发场景下, 从库的数据势必会比主库慢, 存在延迟, 所以经常出现刚写入主库的数据读不到的情况 如果主库突然宕机, 此时数据还没有同步到从库, 那么有些数据从库上是没有的, 会出现数据丢失情况 MySQL 存在两个机制解决上面的问题 : 半同步复制 : 主要解决主库数据丢失问题, 也叫做 semi-sync 复制, 指主库写入 binlog 日志之后, 就会强制将此时的数据立即同步到从库, 从库将日志写入自己本地的 relay log 之后, 返回一个 ack 给主库, 主库接收到至少一个从库的 ack 才会认为写操作完成了 并行复制 : 主要解决同步延时问题, 指从库开启多线程, 并行读取 relay log 中不同库的日志, 然后并行重放不同库的日志, 这是库级别的并行 如何实现主从复制这里举例是一主一从 Master登录 MySQL 1mysql -u root -p 创建用户 1234# 10.1.30.113 是 slave 从机的 IPGRANT REPLICATION SLAVE ON *.* to &#x27;root&#x27;@&#x27;10.1.30.113&#x27; identified by &#x27;1qaz@WSX&#x27;;# 刷新系统权限表的配置FLUSH PRIVILEGES; 在 etc&#x2F;my.cnf 增加以下配置 12345678# 开启binloglog-bin=mysql-binserver-id=114# 需要同步的数据库，如果不配置则同步全部数据库binlog-do-db=test_db# binlog日志保留的天数，清除超过10天的日志# 防止日志文件过大，导致磁盘空间不足expire-logs-days=10 重启 MySQL 1service mysql restart 通过下方命令查看当前 binlog 日志信息 1show master status\\G; Slave在 etc&#x2F;my.cnf 增加以下配置 12# 不要和其他mysql服务id重复即可server-id=114 登录 MySQL 1mysql -u root -p 输入以下命令 : MASTER_HOST : 主机 IP MASTER_USER: 之前创建的用户账号 MASTER_PASSWORD : 之前创建的用户密码 MASTER_LOG_FILE : master 主机的 binlog 日志名称 MASTER_LOG_POS : binlog 日志偏移量 master_port : 端口 1CHANGE MASTER TO MASTER_HOST=&#x27;10.1.30.114&#x27;,MASTER_USER=&#x27;root&#x27;,MASTER_PASSWORD=&#x27;1qaz@WSX&#x27;,MASTER_LOG_FILE=&#x27;mysql-bin.000007&#x27;,MASTER_LOG_POS=862,master_port=3306; 重新启动 1start slave; 启动下方命令校验 1show slave status\\G; 判断同步成功方式 : 首先 Master_Log_File 和 Relay_Master_Log_File 所指向的文件必须一致 其次 Relay_Log_Pos 和 Exec_Master_Log_Pos 的为止也要一致才行 测试12345678910CREATE TABLE `tb_role` ( `role_id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;角色id&#x27;, `role_name` varchar(30) DEFAULT NULL COMMENT &#x27;角色名称&#x27;, `state` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;1启用0停用,默认0&#x27;, `create_by` varchar(40) DEFAULT NULL COMMENT &#x27;创建人&#x27;, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, `remark` varchar(100) DEFAULT NULL COMMENT &#x27;备注&#x27;, PRIMARY KEY (`role_id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; 在 master 执行上述 SQL 同步到 slave , 表示配置成功 常见问题配置失败出现的问题常见于 Slave_IO_Running: No 或 Slave_SQL_Running: No , 通常是配置读取文件出现问题或事务回滚造成的主从问题, 由于问题很多不做赘述, 列举几个类似问题的博客 : https://blog.csdn.net/zzddada/article/details/113352717 https://blog.csdn.net/weixin_30657999/article/details/99613614 https://blog.csdn.net/lihuarongaini/article/details/101299375","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"}]},{"title":"为什么要使用消息队列, 优缺点, 各消息队列对比","slug":"whyMQ","date":"2022-11-07T07:53:14.000Z","updated":"2023-02-06T07:37:41.725Z","comments":true,"path":"2022/11/07/whyMQ/","link":"","permalink":"http://logerjava.github.io/2022/11/07/whyMQ/","excerpt":"","text":"为什么使用消息队列解耦 如上方场景, A 系统通过接口调用方式发送数据到 B, C, D 系统, 此时新增 E 系统也需要此数据该如何解决 ? 此时又新增了其他系统呢 ? B 系统在某个时间节点不需要 A 系统的数据了该如何解决 ? 在上方场景中 A 系统不仅和 B, C, D 等系统严重耦合在一起, 并且要时刻考虑其他系统的状态, 如果宕机是否要重新发送, 是否需要存储消息等, 负责人的讲 A 系统负责人会很痛苦 如果改用 MQ 方式处理, A 系统产生数据, 直接发送到 MQ 中, 其余需要数据的系统到 MQ 中消费, B 系统不需要则取消消费, 这种情况下 A 系统就摆脱了束缚, 无需考虑调用是否成功, 是否超时等问题, 如下图 异步 如上场景, 假设 A 系统接收到用户请求需要本地持久化数据, 过程为 3ms, 后 B, C, D 写库总计 3 + 300 + 450 + 200 &#x3D; 953ms, 总体接近 1s, 在一般的项目中我们要求基本上是请求响应基本上是对用户无感知的, 大概 200ms 以内完成, 以上情况很难接受 此时添加 MQ, A 系统发送三条消息到 MQ 中耗时 5ms, 总计 3 + 5 &#x3D; 8ms, 直接返回后续操作在后台完成 削峰考虑如下场景, 从早晨 0:00 开始到下午 13:00, 系统 A 每秒并发请求基本维持在 30 左右, 在 13:00 到 14:00 每秒请求激增到 5k+, 系统基于 MySQL 直连, 这时会有每秒 5k+ 的请求打入数据库 一般的 MySQL 很明显无法抗住这种请求级别, 2k 左右大概是极限, 很可能直接宕机, 用户也就无法继续操作系统, 但是经过高峰期后又再度恢复为每秒 30 的请求量 这个时候我们考虑接入 MQ 处理, 每秒 5k+ 的请求写入 MQ, 系统 A 每秒至多处理 2k 的请求, 那么就仅拉取 2k 的请求, 只要不超过处理极限就可以, 这样在最高峰值期间服务并不会挂掉, 每秒 5k 左右的请求进入 MQ, 2k 左右的请求被消费, 这样可能会导致几十万甚至百万的请求积压在 MQ 中, 但是短暂的积压是没有关系的, 经历过高峰期后只有每秒 30 的请求量, 但是系统还是在按照每秒 2k 左右的速度消费, 高峰期过后用不了多久就可以处理结束 消息队列的优缺点优点 : 解耦, 异步, 削峰 缺点 : 可用性降低 : 系统引入的外部依赖越多则可用性越低, 根据上面的场景, 本身是 A, B, C, D 四个系统的问题, 接入 MQ 后需要考虑 MQ 的维护问题, 如果 MQ 宕机则整套系统都将崩溃 复杂度提高 : 新增 MQ 后需要考虑消息幂等问题(是否重复), 消息丢失问题, 顺序等 一致性问题 : 在将消息发送到 MQ 后返回成功, 但是不一定真的全部成功, 有可能 B, C 写入成功而 D 却失败等问题, 会导致数据不一致 综上所属消息队列实际上并没有想象的那么简单, 引入消息队列确实可以带来好处, 但是也会衍生出另一些问题, 针对某些必要使用 MQ 的场景我们需要提前准备问题的解决方案, 难度系统直线上升, 但是关键时刻消息队列是起决定性作用的技术, 该用还是要用 ActiveMQ、RabbitMQ、RocketMQ、Kafka 对比 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百&#x2F;几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 以前很多人用 ActiveMQ , 但是现在用的很少, 并且社区不活跃, 不建议使用 RabbitMQ 社区很活跃, 但是 erlang 语言导致 RabbitMQ 处于基本不可控的状态, 也无法做到自定义 RocketMQ 来自阿里, 质量有保证, 毕竟有双 11 检验, 但是目前 RocketMQ 已经捐献给 Apache, 并且活跃度不是很高, 不过毕竟是 Java 写的可控性还是有的, 如果对公司技术自信的可以考虑 Kafka 一般适用于大数据领域, 日志采集, 实时计算","categories":[],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://logerjava.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]}],"categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://logerjava.github.io/tags/MySQL/"},{"name":"Java","slug":"Java","permalink":"http://logerjava.github.io/tags/Java/"},{"name":"Tools","slug":"Tools","permalink":"http://logerjava.github.io/tags/Tools/"},{"name":"Linux","slug":"Linux","permalink":"http://logerjava.github.io/tags/Linux/"},{"name":"Nacos","slug":"Nacos","permalink":"http://logerjava.github.io/tags/Nacos/"},{"name":"消息队列","slug":"消息队列","permalink":"http://logerjava.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]}